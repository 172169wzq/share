
ISO/TC 22/SC 32/WG 14 N220 
ISO/TC22/SC32/WG14 "Safety and ArtificialIntelligence" Convenorship: DIN Convenor: Burton Simon Mr Prof.Dr. 

ISO PAS8800 2024-01-31 for WG14 finalcheck Document type Related content Document date Expected action 
COMMENT/REPLY by 
Project / Other Project: ISO/CD PAS 8800 2024-01-31 2024-02-21 
Description 
Dear All, 
attached is the final CD document (from the OSD). 
Note for the ST leads: this is the same document that I distributed to you. 
The document is without the updated drawings. The drawings are currently being reviewed. 
As soon as we get back the updated drawings I will distriute the document again. 
Please use this document for your final review and enter any comments you may have in the ISO comment sheet (N221). 
If you have any comments, please send them to stephan.kraehnert@vda.de by 21 February. 
Deadline: 2024-02-21 
N222 contains the final WG14 comment sheet. 
Please do not hesitate to contact me if you have any questions. 
Best regards, Stephan Kraehnert 
ISO/TC 22/SC 32 
2 ISO/CD PAS 8800(en) 3 Secretariat: JISC 

4 Road Vehicles ¡ª Safety and artificial intelligence 
5 V¨¦hicules routiers ¡ª S¨¦curit¨¦ et intelligence artificielle 
6  . ISO 2024  
7  All rights reserved. Unless otherwise specified, or required in the context of its implementation, no part of this publication  
8  may be reproduced or utilized otherwise in any form or by any means, electronic or mechanical, including photocopying,  
9  or posting on the internet or an intranet, without prior written permission. Permission can be requested from either ISO  
10  at theaddressbeloworISO¡¯smemberbody inthecountry oftherequester.  
11  ISO copyright office  
12  CP 401 .Ch. deBlandonnet8  
13  CH-1214 Vernier, Geneva  
14  Phone: + 41 22 749 01 11  
15  E-mail: copyright@iso.org  
16  Website: www.iso.org  
17  Published in Switzerland  

. ISO 2024 ¨CAll rights reserved 
18 Contents 
19 Foreword ................................................................................................................................................................................................... viii 20 Introduction................................................................................................................................................................................................. ix 21 1 Scope ................................................................................................................................................................................................. 1 22 2 Normative references................................................................................................................................................................. 1 23 3 Terms and definitions ................................................................................................................................................................2 
24 3.1 General AI-related definitions ................................................................................................................................................ 2 
25 3.2 Data-related definitions ............................................................................................................................................................ 9 
26 3.3 General safety-related definitions ..................................................................................................................................... 10 
27 3.4 Safety: Root cause-, error-and failure-related definitions ....................................................................................... 13 
28 3.5 Miscellaneous definitions ...................................................................................................................................................... 15 29 4 Abbreviated terms.................................................................................................................................................................... 18 30 5 Requirements for compliance.............................................................................................................................................. 18 31 5.1 Purpose ......................................................................................................................................................................................... 18 
32 5.2 General requirements ............................................................................................................................................................. 19 
33 5.3 Interpretations of tables and figures ................................................................................................................................ 19 34 6 AI within the context of road vehicles system safety engineering and basic concepts ............................... 19 
35 6.1 Application of the ISO 26262 series for the development of AI systems........................................................... 19 
36 6.2 Interactions with encompassing system-level safety activities ............................................................................ 20 
37 6.3 Mapping of abstraction layers between ISO 26262, ISO/IEC 22989 and this document ........................... 24 
38 6.4 Example architecture for an AI system............................................................................................................................ 27 
39 6.5 Types of AI models ................................................................................................................................................................... 27 
40 6.6 AI technologies of a ML model............................................................................................................................................. 28 
41 6.7 Error concepts, fault models and causal models ......................................................................................................... 29 42 6.7.1 Cause-and-effect chain....................................................................................................................................................... 29 
43 6.7.2 Root cause classes................................................................................................................................................................ 30 
44 6.7.3 Error classification based on the safety impact....................................................................................................... 31 45 7 AI safety management............................................................................................................................................................. 32 46 7.1 Objectives..................................................................................................................................................................................... 32 
47 7.2 Prerequisites and supporting information..................................................................................................................... 32 
48 7.3 General requirements ............................................................................................................................................................. 33 
49 7.4 Reference AI safety life cycle................................................................................................................................................ 35 
50 7.5 Iterative development paradigms for AI systems ....................................................................................................... 37 
51 7.6 Work products ........................................................................................................................................................................... 38 52 8 Assurance arguments for AI systems ............................................................................................................................... 39 53 8.1 Objectives..................................................................................................................................................................................... 39 
54 8.2 Prerequisites and supporting information..................................................................................................................... 39 
55 8.3 General requirements ............................................................................................................................................................. 40 
. ISO 2024 ¨CAll rights reserved 
56 8.4 AI system-specific considerations in assurance arguments ................................................................................... 40 
57 8.5 Structuring assurance arguments for AI systems ....................................................................................................... 41 
58 8.5.1 Context of the assurance argument.............................................................................................................................. 41 
59 8.5.2 Categories of evidence ....................................................................................................................................................... 42 
60 8.6 The role of quantitative targets and qualitative arguments ................................................................................... 44 
61 8.7 Evaluation of the assurance argument............................................................................................................................. 45 
62 8.8 Work products ........................................................................................................................................................................... 46 63 9 Derivationof AI safetyrequirements ................................................................................................................................. 46 64 9.1 Objectives..................................................................................................................................................................................... 46 
65 9.2 Prerequisites and supporting information..................................................................................................................... 46 
66 9.3 General requirements ............................................................................................................................................................. 47 
67 9.4 General workflow for deriving safety requirements.................................................................................................. 48 
68 9.5 Deriving AI safety requirements on supervised machine learning...................................................................... 50 
69 9.5.1 The need for refinedAI safety requirements ............................................................................................................ 50 
70 9.5.2 Derivation of refined AI safety requirements to manage uncertainty ........................................................... 52 
71 9.5.3 Refinement of the input space definition for AI safety lifecycle ....................................................................... 55 
72 9.5.4 Restricting the occurrence of AI output insufficiencies ....................................................................................... 55 
73 9.5.5 Metrics, measurements and threshold design ......................................................................................................... 58 
74 9.5.6 Considerations for deriving safety requirements................................................................................................... 59 
75 9.6 Work products ........................................................................................................................................................................... 60 76 10 Selection of AI technologies, architectural and development measures ........................................................... 60 77 10.1 Objectives..................................................................................................................................................................................... 60 78 10.2 Prerequisites............................................................................................................................................................................... 60 79 10.3 General requirements ............................................................................................................................................................. 60 80 10.4 Architecture and development process design or refinement .............................................................................. 61 81 10.5 Examples of architectural and development measures for AI systems ............................................................. 62 82 10.6 Work products ........................................................................................................................................................................... 66 83 11 Data-related considerations ................................................................................................................................................. 66 84 11.1 Objectives..................................................................................................................................................................................... 66 85 11.2 Prerequisites and supporting information..................................................................................................................... 66 86 11.3 General requirements ............................................................................................................................................................. 66 87 11.4 Dataset life cycle........................................................................................................................................................................ 67 88 11.4.1 Datasets and the AI safety lifecycle .............................................................................................................................. 67 89 11.4.2 Reference dataset lifecycle............................................................................................................................................... 68 90 11.4.3 Dataset safety analysis....................................................................................................................................................... 70 91 11.4.4 Dataset requirements development............................................................................................................................. 76 92 11.4.5 Dataset design ....................................................................................................................................................................... 78 93 11.4.6 Dataset implementation.................................................................................................................................................... 79 94 11.4.7 Dataset verification ............................................................................................................................................................. 80 
. ISO 2024 ¨CAll rights reserved 
95 11.4.8 Dataset validation ................................................................................................................................................................ 81 96 11.4.9 Dataset maintenance .......................................................................................................................................................... 81 97 11.5 Work products ........................................................................................................................................................................... 82 98 12 Verification and validation of AI system ......................................................................................................................... 82 99 12.1 Objectives..................................................................................................................................................................................... 82 100 12.2 Prerequisites and supporting information..................................................................................................................... 83 101 12.3 General requirements ............................................................................................................................................................. 83 102 12.4 AI/ML specific challenges to verification and validation ......................................................................................... 85 103 12.5 Verification and validation of the AI system.................................................................................................................. 86 104 12.5.1 Scope of verification and validation of the AI system ........................................................................................... 86 105 12.5.2 AI component testing ......................................................................................................................................................... 88 106 12.5.3 Methods for testing the AI component........................................................................................................................ 90 107 12.5.4 AI system integration and verification........................................................................................................................ 92 108 12.5.5 Virtual testing vs physical testing ................................................................................................................................. 93 109 12.5.6 Evaluation of the safety-related performance of the AI system ....................................................................... 94 110 12.5.7 AI systemsafety validation ............................................................................................................................................... 95 111 12.6 Work products ........................................................................................................................................................................... 95 112 13 Safety analysis of AI systems................................................................................................................................................ 95 113 13.1 Objectives..................................................................................................................................................................................... 95 114 13.2 Prerequisites and supporting information..................................................................................................................... 96 115 13.3 General requirements ............................................................................................................................................................. 97 116 13.4 Safety analysis of the AI system.......................................................................................................................................... 97 117 13.4.1 Scope of the AI safety analysis........................................................................................................................................ 97 118 13.4.2 Safety analysis based on the results of testing......................................................................................................... 98 119 13.4.3 Safety analysis techniques................................................................................................................................................ 99 120 13.5 Work products .........................................................................................................................................................................101 121 14 Measures during operation.................................................................................................................................................101 122 14.1 Objectives...................................................................................................................................................................................101 123 14.2 Prerequisites and supporting information...................................................................................................................101 124 14.3 General requirements ...........................................................................................................................................................102 125 14.4 Planning for operation and continuous assurance ...................................................................................................102 126 14.4.1 Safety risk of the AI system during operation phase...........................................................................................102 127 14.4.2 Safety activities during the operation phase ..........................................................................................................103 128 14.5 Continual, periodic re-evaluation of the assurance argument.............................................................................103 129 14.6 Measures to assure safety of the AI system during operation .............................................................................104 130 14.6.1 General....................................................................................................................................................................................104 131 14.6.2 Technical safety measures .............................................................................................................................................104 132 14.6.3 Safe operation guidance and misuse prevention in the field...........................................................................106 133 14.7 Field data collection...............................................................................................................................................................106 
134 14.8 Evaluation and continuous development .....................................................................................................................108 135 14.8.1 Field risk evaluation .........................................................................................................................................................108 136 14.8.2 Countermeasures addressing field risk ....................................................................................................................109 137 14.8.3 AI re-training, re-validation, re-approval and re-deployment........................................................................109 138 14.9 Work products .........................................................................................................................................................................110 139 15 Confidence in use of AI development frameworks and software tools used for AI model development 140 110 141 15.1 Objectives...................................................................................................................................................................................110 142 15.2 Prerequisites and supporting information...................................................................................................................110 143 15.3 General Requirements ..........................................................................................................................................................110 144 15.4 Confidence in the use of AI development frameworks ...........................................................................................111 145 15.5 Confidence in the use of tools used to support the AI-safety lifecycle..............................................................114 146 15.6 Principles for data-driven AI model training and evaluation...............................................................................114 147 15.7 Work products .........................................................................................................................................................................115 148 Annex A Overview and workflow of ISO PAS 8800.................................................................................................................116 149 Annex B Example assurance argument structure for an AI-based vehicle function .................................................121 150 B.1 General ........................................................................................................................................................................................121 151 B.2 Assurance argument pattern for supervised machine learning ..........................................................................121 152 B.3 Use of assurance claim points to increase confidence in the assurance argument .....................................128 153 B.3.1 General remarks on the use of assurance claim points......................................................................................128 154 B.3.2 Example assurance claim points to support assumptions or context: ACP-A2 for assumption A2.129 155 B.3.3 Example assurance claim point to support inference: ACP-S1 for strategy S1 ........................................130 156 B.3.4 Example assurance claim point to support evidence: ACP-E5........................................................................130 157 Annex C ISO 26262:2018 Gap Analysis for ML .........................................................................................................................132 158 C.1 ISO 26262-4:2018 Tailoring and Guidance for ML ...................................................................................................132 159 C.2 ISO 26262-6:2018 Tailoring for ML ................................................................................................................................132 160 Annex D Detailed considerations on safety-related properties of AI systems.............................................................139 161 Annex E STAMP/STPA example ......................................................................................................................................................142 162 E.1 Overview.....................................................................................................................................................................................142 163 E.2 STPA Example ..........................................................................................................................................................................142 164 E.2.1 STPA Step 1: Defining the purpose and scope of the analysis .........................................................................142 165 E.2.2 STPA step 2: Modelling of the control structure ...................................................................................................142 166 E.2.3 STPA step 3: Identification of unsafe control actions .........................................................................................143 167 E.2.4 STPA step 4: Identification of causal scenarios .....................................................................................................143 168 E.2.5 Identifying safety measures to mitigate the safety-related issues ................................................................145 169 Annex F Identification of software units within NN-based systems ................................................................................147 170 Annex G Architectural and Development Measures for AI Systems ................................................................................150 171 G.1 Examples of architectural and development measures for AI systems ...........................................................150 172 G.1.1 Measures for Architectural Redundancy..................................................................................................................150 
. ISO 2024 ¨CAll rights reserved 
173 G.2 Qualitative and quantitative analysis of AI architectures ......................................................................................152 174 G.2.1 Identifying software units within AI architectures..............................................................................................153 175 G.3 Data distributions and their impacts on AI models ..................................................................................................154 176 G.3.1 Out of distribution data and its mitigation..............................................................................................................154 177 G.3.2 Distributional shift and its mitigation .......................................................................................................................154 178 G.4 Training safety measures ....................................................................................................................................................156 179 G.4.1 Hyperparameter tuning...................................................................................................................................................156 180 G.4.2 Robust Learning .................................................................................................................................................................157 181 G.4.3 Transfer learning................................................................................................................................................................157 182 G.4.4 Confidence calibration and uncertainty quantification of AI models...........................................................158 183 G.4.5 Verifying feature selection .............................................................................................................................................158 184 G.4.6 Monitoring multiple scores............................................................................................................................................159 185 G.4.7 Attention or saliency Maps.............................................................................................................................................159 186 G.4.8 Interpretable latent features.........................................................................................................................................159 187 G.4.9 Augmentation of Data.......................................................................................................................................................159 188 G.5 Monitoring and AI system modification........................................................................................................................161 189 G.5.1 Dynamic Environment Monitoring.............................................................................................................................161 190 G.5.2 AI Model Modification......................................................................................................................................................161 191 G.6 Alignment of intention..........................................................................................................................................................162 192 G.7 Considerations related to the target execution environment ..............................................................................162 193 G.7.1 Optimization of parameters and optimization of architectural entities of AI components ................163 194 G.7.2 Knowledge distillation also known as teacher-student model .......................................................................163 195 G.7.3 Analysis for differences ...................................................................................................................................................163 196 Annex H Typical performance metrics for machine learning .............................................................................................164 197 Bibliography ............................................................................................................................................................................................171 198 
. ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved 
ISO/CD PAS 8800:2024(en)  
199  Foreword  
200  ISO (the International Organization for Standardization) is a worldwide federation of national standards  
201  bodies (ISO member bodies). The work of preparing International Standards is normally carried out through  
202  ISO technical committees. Each member body interested in a subject for which a technical committee has been  
203  established has the right to be represented on that committee. International organizations, governmental and  
204  non-governmental, in liaison with ISO, also take part in the work. ISO collaborates closely with the  
205  International Electrotechnical Commission (IEC) on all matters of electrotechnical standardization.  
206  The procedures used to develop this document and those intended for its further maintenance are described  
207  in the ISO/IEC Directives, Part 1. In particular the different approval criteria needed for the different types of  
208  ISO documents should be noted. This document was drafted in accordance with the editorial rules of the  
209  ISO/IEC Directives, Part 2 (see www.iso.org/directives).  
210  Attention is drawn to the possibility that some of the elements of this document may be the subject of patent  
211  rights. ISO shall not be held responsible for identifying any or all such patent rights. Details of any patent rights  
212  identified during the development of the document will be in the Introduction and/or on the ISO list of patent  
213  declarations received (see www.iso.org/patents).  
214  Any trade name used in this document is information given for the convenience of users and does not  
215  constitute an endorsement.  
216  For an explanation on the meaning of ISO specific terms and expressions related to conformity assessment, as  
217  well as information about ISO's adherence to the World Trade Organization (WTO) principles in the Technical  
218  Barriers to Trade (TBT) see the following URL: www.iso.org/iso/foreword.html.  
219  This document was prepared by Technical Committee ISO/TC 22, Road Vehicles, Subcommittee SC 32,  
220  Electrical and electronic components and general system aspects, Working Group 14, Safety and Artificial  
221  Intelligence.  
222  Any feedback or questions on this document should be directed to the user¡¯s national standards body. A  
223  complete listing of these bodies can be found atwww.iso.org/members.html.  

224 Introduction 
225 The purpose of this document is to provide industry-specific guidance on the use of AI systems in safety-226 related functions. It is not restricted to specific AImethods or specific vehicle functions. 
227 This document defines a framework for managing AIsafety that tailors or extends existing approaches 228 currently defined within the ISO 26262 series (functional safety) and ISO 21448 (safety of the intended 229 functionality)[1]. 
230 Functional safety-related risks associated with malfunctioning behaviour of the AI system are addressed by 231 tailoring or extending relevant clauses from ISO 26262-4:2018, ISO 26262-6:2018 and ISO 26262-8:2018. 
232 Risks related to functional insufficiencies in the AI system are addressed by extending concepts and guidance 233 provided by ISO 21448. A causal model of understanding the sources of functional insufficiencies in the AI 234 system is proposed and used to derive a set of safety requirements on the AI system as well as a set of risk 235 reduction measures. 
236 NOTE 1 ISO 21448 is applicable to intended functionalities where proper situational awareness is essential to safety 237 and where such situational awareness is derived from sensors and processing algorithms, especially functionalities of 238 emergency intervention systems and systems having ISO/SAE PAS 22736[2]levels of driving automation from 1 to 5. 239 Therefore, it is possible that systems utilize AI technologies that are formally not in scope of ISO 21448. 
240 EXAMPLE 1 ISO 21448 will not apply to the development of an engine control unit that uses AI to optimize its 241 performance while this document will. 
242 This document recognizes that due to the wide range of applications of AI and associated safety requirements, 243 as well as the rapidly evolving state-of-the-art, it is not possible to provide detailed requirements on the 244 process or product characteristics required to achieve an acceptably low level of residual risk associated with 245 the use of AI systems. Therefore, in addition to providing guidance for tailoring or extension of ISO 26262 and 246 ISO 21448, this document focuses on the principles that are to be instantiated to support the creation of a 247 project-specific assurance argument for the safety of the AI system within its vehicle context. This includes 248 risk reduction measures during the design and operation phases using an iterative approach to reducing risk 249 as outlined in IEC Guide 51 [3] is proposed. 
250 The task of hazard and risk analysis is outside the scope of this document and is considered a part of vehicle 251 level systems safety engineering activities as described in ISO 26262 and ISO 21448, or in application specific 252 standards such as ISO TS 5083. 
253 ISO/IEC TR 5469:2024 [4] provides generic guidance for the application of AI technologies as part of safety 254 functions, independent of specific industry sectors. Many of the concepts outlined within ISO/IEC TR 5469 can 255 be applied in the context of road vehicle. There is therefore a close relationship to concepts described within 256 this document and ISO/IEC TR 5469. 
257 ISO/IEC TR 5469 provides classification schemes to determine the safety requirements on the AI/ML function. 258 These include the usage level and AI technology class. 
259 The usage level is related to the nature of the task being performed by the engineered AI system. 
260 NOTE 2 According to ISO/IEC TR 5469: 
261 ¡ª Usage level A1 is assigned when the AI technology is used in a safety-relevant E/E/PE system and where automated 262 decision-making of the system function using AI technology is possible; 
263 ¡ª Usage level A2 is assigned when the AI technology is used in a safety-relevant E/E/PE system and where no 264 automated decision-making of the system function using AI technology is possible (e.g. AI technology is used for 265 diagnostic functionality within the E/E/PE system); 
266  ¡ª Usage level B1 is assigned when the AI technology is used only during the development of the safety-relevant E/E/PE  
267  system (e.g. an offline support tool) and where automated decision-making of the function developed using AI  
268  technology is possible;  
269  ¡ª Usage level B2 is assigned when the AI technology is used only during the development of the safety-relevant E/E/PE  
270  system (e.g. an offline support tool) and where no automated decision-making of the function is possible;  
271  ¡ª Usage level C is assigned when the AI technology is not part of a functional safety function in the E/E/PE system, but  
272  can have an indirect impact on the function;  
273  ¡ª Usage Level D is assigned if the AI technology is not part of a safety function in the E/E/PE system and has no impact  
274  on the safety function due to sufficient segregation and behaviour control.  
275  The technology class is related to the problem complexity and the transferability of existing standards to  
276  demonstrating an adequate level of safety based on properties of the target function and the AI technology  
277  used.  
278  NOTE 3 According to ISO/IEC TR 5469:  
279  ¡ª Class I is assigned if the AI technology can be developed and reviewed using existing functional safety International  
280  Standards.  
281  ¡ª Class II is assigned if AI technology cannot be fully developed and reviewed using existing functional safety  
282  International Standards, but it is still possible to identify the desired safety properties and the means to achieve them  
283  by a set of methods and techniques.  
284  ¡ª Class III is assigned if AI technology cannot be developed and reviewed using existing functional safety International  
285  Standards and it is not possible to identify a set of properties with related methods and techniques to achieve them.  
286  This document does not explicitly call out the classes and usage levels of ISO/IEC TR 5469.  
287  EXAMPLE 2 For some AI technology, the application of ISO 26262 is deemed to be sufficient. This corresponds to the  
288  Class I of ISO/IEC TR 5469.  
289  The guidance outlined within this document is relevant for all usage of AI for which safety requirements can  
290  foreseeably be allocated either through:  
291  a) the use of AI for the functionality itself;  
292  b) the use of AI as a safety mechanism.  
293  NOTE 4 These usages correspond to the usage levels A1, A2, C of ISO/IEC TR 5469. In all cases, the applicability of the  
294  guidance provided within this document can be determined by the allocation of safety requirements to the AI technology,  
295  whereas the usage levels of ISO/IEC TR 5469 can be used to support the requirements elicitation process.  
296  This document is aligned with standards and documents developed within ISO/IEC JTC1/SC42 Artificial  
297  Intelligence. AI specific definitions are used from ISO/IEC 22989 (Artificial intelligence ¡ª Artificial  
298  intelligence concepts and terminology), unless in conflict with safety-specific definitions.  
299  Other documents developed within ISO/IEC JTC1/SC42 can be used to provide additional guidance on specific  
300  aspects of AI that are relevant to safety-related properties. Examples of such documents include ISO/IEC TR  
301  24027:2021 (Artificial Intelligence (AI) ¨CBias in AI systems and AI aided decision making) and ISO/IEC TR  
302  24029-1:2021 (Artificial Intelligence (AI) ¨C Assessment of the robustness of neural networks ¨C Part 1:  
303  Overview).  
304  This document harmonizes the concepts already described in ISO 21448:2022, Annex D.2[1]and ISO/TS  
305  5083,Annex B whilst extending these with specific guidance regarding the definition of safety requirements  
306  of ML, ML safety analyses and the creation of associated safety evidence during the development and  
307  deployment lifecycle.  

308 ISO/TS 5083, Annex B1 is an application of this document to Automated Driving Systems (ADS). 309 The relationship with the above-mentioned documents is summarized in Table 1¨C1. 310 Table 1¨C1 ¡ª How this document relates to other publications on AI safety 
Publication  Relationship with this document  
ISO/IEC 22989  AI specific definitions are used from ISO/IEC 22989, unless in conflict with safety-specific definitions. Safety-related properties are a subset of generic AI properties described in ISO/IEC 22989.  
ISO/IEC TR 5469  This document does not explicitly call out the classes and usage levels of ISO/IEC TR 5469. This document considers and adapts to road vehicles the general framework described in ISO/IEC TR 5469 on safety properties, virtual testing and physical testing, confidence in use of AI development frameworks and architectural redundancy patterns.  
ISO 26262  This document is a tailoring or extension of ISO 26262 for AI elements of the system. Refer to Clause 5 for details.  
ISO 21448  This document is a tailoring or extension of ISO 21448 for AI elements of the system. Refer to Clause 5 for details.  
ISO TS 5083  ISO TS 5083, Annex B2 is an application of this document to Automated Driving Systems (ADS).  

311 This document adds the following contents with respect to the mentioned publications: 312 ¡ª tailoring or extensions of ISO 26262 and ISO 21448 required specifically for AI elements of the system 313 (referred to as AI systems); 314 ¡ª a conceptual model for reasoning about errors and their causes specific to AI systems; 315 ¡ª a reference AI safety lifecycle; 316 ¡ª the safety assurance argument for AI systems; 317 ¡ª a method for deriving AI safety requirements for AI systems; 318 ¡ª considerations for the design of safe AI systems; 319 ¡ª considerations on data management for the AI systems; 320 ¡ª a verification and validation strategy for AI systems; 321 ¡ª a safety analysis approach for AI systems (focused on insufficiencies); 322 ¡ª activities during operation required to ensure the continuous AI safety. 
1 Under preparation. Stage of the time of publication: ISO/FDISTS 5083. 



323 Road Vehicles ¡ª Safety and artificial intelligence 
324 1 Scope 325 This document is intended to be applied to safety-related systems that include one or more electrical and/or 326 electronic (E/E) systems which use AI technology and that are installed in series production road vehicles, 327 excluding mopeds. It does not address unique E/E systems in special vehicles such as E/E systems designed 328 for drivers with disabilities. 329 This document addresses the risk of undesired safety-related behaviour at the vehicle level due to output 330 insufficiencies, systematic error and random hardware error of AI elements within the vehicle. This includes 331 the interaction with AI elements that are not part of the vehicle itself but can have a direct or indirect impact 332 on the vehicle safety. 333 EXAMPLE 1 Examples of AI elements within the vehicle include the trained AI model and AI system. 334 EXAMPLE 2 Direct impact on safety can be due to e.g. object detection by elements external to the vehicle. 335 EXAMPLE 3 Indirect impact on safety can be due to e.g. field monitoring by elements external to the vehicle. 336 NOTE The development of AI elements that are not part of the vehicle is not within the scope of this document. These 
337 elements can comply with domain-specific safety guidance. This document can be used as a reference where such 338 domain-specific guidance does not exist. 339 This document describes safety-related properties of AI systems that may be used to construct a convincing 
340 safety assurance claim for the absence of unreasonable risk. 341 This document does not provide specific guidelines for software tools that use AI methods. 342 This document focuses primarily on the subclass of AI methods defined as ML (Machine Learning): it covers 
343 the principles of established and well understood classes of ML, but is not focused on specific AI methods e.g. 344 Deep Neural Networks (DNN). 
345 2 Normative references 346 The following documents are referred to in the text in such a way that some or all of their content constitutes 347 requirements of this document. For dated references, only the edition cited applies. For undated references, 
348 the latest edition of the referenced document (including any amendments) applies. 349 ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality 350 ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality 351 ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality 352 ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality 353 ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality 354 ISO 26262-10:2018, Road vehicles ¡ª Functional safety ¡ª Part 10: Guidelines on ISO 26262 355 ISO 26262-2:2018, Road vehicles ¡ª Functional safety ¡ª Part 2: Management of functional safety 356 ISO 26262-2:2018, Road vehicles ¡ª Functional safety ¡ª Part 2: Management of functional safety 357 ISO 26262-2:2018, Road vehicles ¡ª Functional safety ¡ª Part 2: Management of functional safety 
. ISO 2024 ¨CAll rights reserved 
358  ISO 26262-6:2018, Road vehicles ¡ª Functional safety ¡ª Part 6: Product development at the software level  
359  ISO 26262-6:2018, Road vehicles ¡ª Functional safety ¡ª Part 6: Product development at the software level  
360  ISO 26262-8:2018, Road vehicles ¡ª Functional safety ¡ª Part 8: Supporting processes  
361  ISO 34502:2022, Road vehicles ¡ª Test scenarios for automated driving systems ¡ª Scenario based safety  
362  evaluation framework  
363  3 Terms and definitions  
364  For the purposes of this document, the terms and definitions given in Clauses 3.1, 3.2, 3.3, 3.4 and 3.5 apply.  
365  Also, the terms and definitions of ISO 26262-1, ISO 21448, ISO/IEC 22989, ISO/IEC TR 5469 apply unless  
366  redefined in this document.  
367  ISO and IEC maintain terminological databases for use in standardization at the following addresses:  
368  ¡ª 
IEC Electropedia: available at http://www.electropedia.org/  
369  ¡ª 
ISO Online browsing platform: available at http://www.iso.org/obp  
370  3.1 General AI-related definitions  
371  3.1.1  
372  AI component  
373  element of an AI system (3.1.17)  
374  EXAMPLE 1 AI pre-processing (3.1.11) component.  
375  EXAMPLE 2 AI post-processing (3.1.9) component.  
376  EXAMPLE 3 AI model (3.1.7).  
377  EXAMPLE 4 conventional software component inside an AI system (3.1.17).  
378  Note 1 to entry: AI components that are not AI models (3.1.7) or that do not contain AI models (3.1.7)are not developed  
379  according to this document. The integration of those components with AI components that areAI models (3.1.7)or that  
380  contain AI models (3.1.7)is performed according to this document.  
381  Note 2 to entry: See 6.3 for an elaborationof the relationship of the different abstraction layers of ISO 26262:2018,  
382  ISO/IEC 22989:2022 and this document with each other.  
383  [SOURCE: ISO/IEC 22989:2022-modified to be consistent with ISO 26262 definitions: Replaced "functional  
384  element" with "element", reworded to not use ¡°construct¡±, added examples and Notes to entry.]  
385  3.1.2  
386  AI controllability  
387  ability of an external agent to control the AI element (3.1.3), its output or the behaviour of the item influenced  
388  by the AI output in order to prevent harm  
389  EXAMPLE Before setting a PWM signal of an actor determined by an AI model (3.1.7) it is limited by a simple  
390  threshold or a substitute approximate physical model by the consumer  
391  Note 1 to entry: An external agent is a person or an element not belonging to the AI system (3.1.17).  

ISO/CD PAS 8800:2024(en)  
392  3.1.3  
393  AI element  
394  AI component (3.1.1) or AI system (3.1.17)  
395  Note 1 to entry: An AI element can refer to a subset of components (3.5.3) within an AI system (3.1.17) that provide related  
396  functionality.  
397  Note 2 to entry: See 6.3 for an elaboration of the relationship of the different abstraction layers of ISO 26262:2018,  
398  ISO/IEC 22989:2022 and this document with each other.  
399  3.1.4  
400  AI explainability  
401  property of an AI system (3.1.17) to express important factors influencing the AI system (3.1.17) results in a  
402  way that humans can understand  
403  EXAMPLE The AI system (3.1.17) can be explainable by natural language or by visualizing feature attribution  
404  methods like gradient based heat/saliency maps or SHAP.  
405  3.1.5  
406  AI generalization  
407  ability of anAI model (3.1.7) to adapt and perform well on previously unseen data during inference  
408  3.1.6  
409  AI method  
410  type of AI model (3.1.7)  
411  EXAMPLE 1 Deep neural network.  
412  EXAMPLE 2 k-nearest neighbour.  
413  EXAMPLE 3 Support vector machine.  
414  3.1.7  
415  AI model  
416  construct containing logical operations, arithmetical operations or a combination of both to generate an  
417  inference or prediction based on input data or information without being completely defined by human  
418  knowledge  
419  Note 1 to entry: Inference is using a model to understand the relation between predictors and a target. Prediction is to  
420  use a model to generate prediction (values close to the real seen or unseen targets) based on the inputs.  
421  3.1.8  
422  AI model validation  
423  evaluation of the performance of different AI model (3.1.7)candidates through testing  
424  Note 1 to entry: There are three terms, "AI model validation", "validation" and "safety validation", that are distinguished  
425  in this standard. AI model validation originates from the validation data used by the AI community, validation originates  
426  from classic system development and safety validation originates from ISO 26262.  
427  Note 2 to entry: The AI model validation is executed using the AI validation dataset.  
428  3.1.9  
429  AI post-processing  
430  any processing that is applied to the output of an AI model (3.1.7) for the purpose of mapping the raw output/s  
431  to a more contextually relevant and consumable format  

. ISO 2024 ¨CAll rights reserved 
432  EXAMPLE 1 A non-maximum suppression and thresholding for a bounding-box generation that serves to remove  
433  bounding boxes of low relevance and duplicates.  
434  EXAMPLE 2 The outputs of a Mixture Density Networks are combined with a physical model (hybrid model).  
435  Note 1 to entry: The AI post-processing would also include any data conversion that is used to bring the output into a  
436  common format for better comparability.  
437  Note 2 to entry: The AI post-processing can have a positive or a negative impact on the safety-related properties of the  
438  output of the AI system (3.1.17).  
439  3.1.10  
440  AI predictability  
441  ability of the AI system (3.1.17) to produce trusted predictions  
442  Note 1 to entry: Trusted predictions means that the predications are accurate and that this claim is supported by  
443  statistical evidence.  
444  3.1.11  
445  AI pre-processing  
446  any processing that is applied to the input of an AI model (3.1.7)  
447  3.1.12  
448  AI reliability  
449  ability of the AI element (3.1.3)to perform theAI task (3.1.18) without AI error (3.4.1) under stated conditions  
450  and for a specified period of time  
451  3.1.13  
452  AI resilience  
453  ability of the AI element (3.1.3)to recover and continue performing the AI task (3.1.18) after the occurrence of  
454  an AI error (3.4.1).  
455  3.1.14  
456  AI robustness  
457  ability to maintain an acceptable level of performance under the presence of semantically insignificant, but  
458  reasonably expected changes to the input  
459  EXAMPLE In image data these insignificant input changes might stem from naturally-induced image corruptions or  
460  sensor noise.  
461  3.1.15  
462  AI safety  
463  absence of unreasonable risk (3.3.11) due to AI errors (3.4.1)caused by faults and functional insufficiencies  
464  Note 1 to entry: This definition only applies in the context of this document. The term "AI safety" is commonly understood  
465  to have a broader meaning which includes ethics, value alignment, long-term considerations, etc.  
466  3.1.16  
467  AI safety requirement  
468  safety requirement (3.3.16) of an AI element (3.1.3)  
469  3.1.17  
470  AI system  
471  item or element that utilise one or more AI models (3.1.7)  

472  EXAMPLE An AI system consisting out of the AI component (3.1.1) ¡°Deep Neural Network for bounding box  
473  generation (AI model (3.1.7))¡± and of theAI component (3.1.1)¡°non-maximum suppression algorithm (AI post-processing  
474  (3.1.9)AI component (3.1.1).)¡±.  
475  Note 1 to entry: The AI system can use various AI methods (3.1.6) and can utilize different AI technologies (3.1.19).  
476  Note 2 to entry: The boundaries of the AI system are determined during the definition of AI system architecture.  
477  Note 3 to entry: The AI system can contain one or more AI components (3.1.1).  
478  Note 4 to entry: The term ¡°AI system¡± serves in this document as the top level of abstraction of the content to be  
479  developed in compliance with the corresponding standard. As such it is possible in a distributed development that what  
480  one party considers to be an AI component (3.1.1)., the other party considers to be an AI system, as for them it represents  
481  the top level of the content they develop.  
482  Note 5 to entry: See 6.3 for an elaboration of the relationship of the different abstraction layers of ISO 26262:2018,  
483  ISO/IEC 22989:2022 and this document with each other.  
484  3.1.18  
485  AI task  
486  action required by the AI element (3.1.3) to achieve a specific goal  
487  Note 1 to entry: Examples of AI tasks include classification, regression, ranking, clustering and dimensionalityreduction.  
488  Note 2 to entry: the AI task can be seen as a semantic description of AI model (3.1.7).  
489  [SOURCE: ISO/IEC 22989:2022 with following modifications: "task" -> "AI task" added "by the AI element",  
490  replacing "<artificial intelligence>"; modified notes to entries accordingly]  
491  3.1.19  
492  AI technology  
493  any technology used within the lifecycle of an AI system (3.1.17) to design, develop, train, test, validate and  
494  implement the AI model (3.1.7)  
495  EXAMPLE Examples of AI technologies are provided in6.6  
496  3.1.20  
497  AI testing  
498  testing the AI system (3.1.17) or AI model (3.1.7)to estimate the expected performance and generalization  
499  capability in the field  
500  Note 1 to entry: The AI testing is executed using an AI test dataset.  
501  Note 2 to entry: Refer to ISO26262-1:2018, 3.169 "testing".  
502  3.1.21  
503  AI system safety validation  
504  confirmation that an AI safety requirement (3.1.16) allocated to the AI system (3.1.17) is fulfilled  
505  Note 1 to entry: In other standards validation denotes the check that requirements are suitable for intended use. Here  
506  the term is intentionally used in the different way that is common in the ML community, i.e. to verity the requirement  
507  implementation.  

ISO/CD PAS 8800:2024(en)  
508  3.1.22  
509  bias  
510  undesired, systematic difference in the AI systems (3.1.17)' predictions with respect to particular classes of  
511  inputs in comparison to others due to potential incorrect learning process  
512  EXAMPLE The classes of inputs can refer to images of objects and people in the context of computer vision.  
513  Note 1 to entry: Bias can arise from an undesired systematic difference within the dataset, from limitations within the  
514  training process, or from limitations within the AI model (3.1.7) capability itself to accurately reflect the dataset.  
515  [SOURCE: ISO/IEC 22989:2022, 3.5.4 modified -adapted the definition to the AI context, replaced Note 1 to  
516  entry, added example.]  
517  3.1.23  
518  control element  
519  element (3.5.5) controlling the execution of the AI task (3.1.18)by the AI element (3.1.3)and other AI element  
520  (3.1.1)-related operations like updates  
521  Note 1 to entry: The control element can control non-AI elements (3.1.3)as well.  
522  3.1.24  
523  data pre-processing  
524  part of the AI workflow that transforms raw data such that it is usable as the input to create the AI model  
525  (3.1.7)  
526  Note 1 to entry: Pre-processing can include reformatting, removal of outliers and duplicates, and ensuring the  
527  completeness of the data set.  
528  3.1.25  
529  encompassing system  
530  item which contains the AI System (3.1.17)  
531  3.1.26  
532  ground truth  
533  set of dataset annotations that are taken to be correct  
534  Note 1 to entry: Individual annotations are derived from information external to the dataset.  
535  Note 2 to entry: Individual annotations may be refined as new information becomes available.  
536  [SOURCE: ISO/IEC 2382-37:2022(en), 37.09.34]  
537  3.1.27  
538  hyperparameter  
539  parameters of the used AI technologies (3.1.19) that affect both the performance of the AI model (3.1.7) and its  
540  learning process  
541  Note 1 to entry: Hyperparameters are selected prior to training and can be used in processes to help estimate model  
542  parameters (3.1.35).  
543  Note 2 to entry: Examples of hyperparameters include the number of network layers, width of each layer, type of  
544  activation function, optimization method, learning rate for neural networks, the choice of kernel function in a support  
545  vector machine, number of leaves or depth of a tree, the number of clusters in K-means clustering, the maximum number  
546  of iterations of the expectation maximization algorithm and the number of Gaussians in a Gaussian mixture.  

547  [SOURCE: ISO/IEC 22989:2022, 3.3.4 ¨Cmodified: term has been redefined to be applicable to all kinds of AI  
548  methods, not only machine learning]  
549  3.1.28  
550  inference  
551  reasoning by which conclusions are derived from known premises  
552  Note 1 to entry: In AI, a premise is either a fact, a rule, a model, a feature, or raw data.  
553  Note 2 to entry: The term "inference" refers both to the process and its result.  
554  [SOURCE: ISO/IEC 22989:2022, 3.1.17]  
555  3.1.29  
556  input space  
557  set of possible input values  
558  Note 1 to entry: See semantic input space (3.1.37)and syntactic input space (3.1.39) for ways how an input space can be  
559  specified.  
560  3.1.30  
561  machine learning (ML)  
562  process of optimizing model parameters (3.1.35)through computational techniques, such that the model's  
563  (3.1.34)behaviour aligns with data or experience and enables prediction beyond the training set  
564  EXAMPLE Learning from experience can mean trying to represent non-static data like simulation, reinforcement  
565  learning environment, etc.  
566  [SOURCE: ISO/IEC 22989:2022, 3.3.5 -modified: Replaced "reflects the data or experience" with "aligns with  
567  data or experience and enables prediction beyond the training set". Added EXAMPLE.]  
568  3.1.31  
569  ML algorithm  
570  algorithm to optimize parameters of a ML model (3.1.32) from data according to given criteria  
571  EXAMPLE Considersolving anunivariatelinearfunctiony =¦È0 + ¦È1xwhereyis anoutput orresult, xis aninput,¦È0  
572  is anintercept(thevalueofywherex=0) and ¦È1 is a weight. In machine learning, the process of determining the intercept  
573  and weights for a linear function is known as linear regression.  
574  [SOURCE: ISO/IEC 22989:2022]  
575  3.1.32  
576  ML model  
577  mathematical construct that generates an inference or prediction based on input data or information and  
578  comprises a functionality that is created by machine learning (3.1.30)  
579  EXAMPLE If anunivariatelinearfunction(y=¦È0 +¦È1x) has been trained using linear regression, the resulting model  
580  can be y = 3 + 7x.  
581  Note 1 to entry: A ML model results from training based on a ML algorithm (3.1.31).  
582  [SOURCE: ISO/IEC 22989:2022,3.3.7 modified -added "and comprises a functionality that is created by  
583  machine learning" to distinguish it from other mathematical constructs]  

ISO/CD PAS 8800:2024(en)  
584  3.1.33  
585  ML model training  
586  iterative process to optimize a ML model (3.1.32)'s input and output behaviour on a given training data set  
587  with the intention to improve its quality (e.g. AI accuracy, AI robustness (3.1.14), generalization capability, run  
588  time), based on a ML algorithm (3.1.31) that can adapt ML model (3.1.32) parameters, hyperparameter  
589  (3.1.27)s, cost function or the model structures itself  
590  [SOURCE: ISO/IEC 22989:2022,3.3.15-modified to elaborate the procedure and intention]  
591  3.1.34  
592  model  
593  physical, mathematical, or otherwise logical representation of a system, entity, phenomenon, process or data  
594  [SOURCE: ISO/IEC 22989:2022]  
595  3.1.35  
596  model parameter  
597  internal variable of a model (3.1.34)that affects how it computes its outputs  
598  Note 1 to entry: Examples of model parameters include the weights in a neural network and the transition probabilities  
599  in a Markov model.  
600  [SOURCE: ISO/IEC 22989:2022]  
601  3.1.36  
602  safety-related AI element  
603  AI element (3.1.3) that contributes to the achievement of anAI safety requirement (3.1.16) allocated to the AI  
604  system, that can contribute to the violation of anAI safety requirement (3.1.16) allocated to the AI system, or  
605  can contribute to both  
606  Note 1 to entry:  
607  3.1.37  
608  semantic input space  
609  set of possible input values on a semantic level  
610  EXAMPLE The semantic input space acquired by a camera sensor can be described as consisting of street images  
611  containing lane markers of different colours, orientation and degradations that appear in different lighting and weather  
612  conditions.  
613  Note 1 to entry: The semantic values correspond and conform to abstract semantic concepts expected within the input  
614  space.  
615  3.1.38  
616  semantic output space  
617  set of possible output values on a semantic level  
618  3.1.39  
619  syntactic input space  
620  set of possible input values on a syntactic level  
621  EXAMPLE The syntactic input space acquired by a camera sensor can be described as an RGB image array of  
622  integers.  

. ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved 
623  Note 1 to entry: The syntactic values can correspond and conform to the outputs produced by the low-level sensor output  
624  values.  
625  3.1.40  
626  syntactic output space  
627  set of possible output values on a syntactic level  
628  3.1.41  
629  trained ML model  
630  ML model (3.1.32) with a set of model parameter (3.1.35) as result of model training (3.1.33)  
631  [SOURCE: ISO/IEC 22989:2022 modified -added "ML" as part of the term]  
632  3.2 Data-related definitions  
633  3.2.1  
634  AI test dataset  
635  dataset used to estimate the performance and generalization capability of an AI model or an AI system  
636  Note 1 to entry: SeeClause 11 for more details  
637  3.2.2  
638  AI validation dataset  
639  dataset used to compare the performance of different candidate AI models (3.1.7)  
640  3.2.3  
641  dataset insufficiency  
642  insufficiency of the dataset regarding data-related safety properties under consideration  
643  Note 1 to entry: Dataset insufficiency includes data integrity errors and data distribution errors  
644  3.2.4  
645  field monitoring dataset  
646  dataset collected after the release of the AI system (3.1.17) while the product is in operation specifically used  
647  for field monitoring of the performance of the AI system (3.1.17)  
648  3.2.5  
649  hybrid dataset  
650  dataset comprising data elements that are both real-world data elements and synthetic data elements  
651  3.2.6  
652  in distribution data  
653  data whose features relevant for the AI task (3.1.18)are present and sufficiently well represented in the  
654  training dataset  
655  Note 1 to entry: In distribution input does not guarantee correctness of AI model output.  
656  3.2.7  
657  metadata  
658  data that provides additional information about the data element or dataset but is usually not directly involved  
659  in the training process  
660  Note 1 to entry: Some metadata (e.g., ground truth) is also used for training.  

661 3.2.8 662 out of distribution data 663 data containing features relevant for the AI task (3.1.18), either absent or not sufficiently well represented in 664 the training data set (3.2.12), that can result in an AI error (3.4.1) 
665 Note 1 to entry: Out of distribution (OOD) refers to data or inputs that fall outside the scope of what an AI or machine 666 learning model was trained on or is designed to handle. When an AI system encounters OOD data, it may struggle to make 667 accurate predictions or decisions because it lacks the necessary knowledge and experience to handle such inputs 668 effectively. OOD data can lead to unexpected or unreliable model behaviour. 
669 3.2.9 670 real-world dataset 671 dataset comprising data elements that have been created by real world acquisitions 
672 3.2.10 673 safety-related KPI 674 key performance indicator relevant for the achievement of AI safety (3.1.15) 
675 3.2.11 676 synthetic dataset 677 dataset comprising data elements that have been created artificially 
678 Note 1 to entry: "created artificially" implies that the data was not directly collected from something that happened in 679 the real world. Additionally, the data does not necessarily represent something that already happened in the real world. 
680 3.2.12 681 training dataset 682 dataset used to train aML model (3.1.32) 
683 [SOURCE: ISO/IEC 22989:2022 definition that has been reworked to contain dataset] 
684 3.3 General safety-related definitions 
685 3.3.1 686 assurance 687 grounds for justified confidence that a claim has been or will be achieved 
688 [SOURCE: ISO/IEC/IEEE 15026-1:2019] 
689 3.3.2 690 assurance argument 691 reasoned, auditable artefact created supporting the contention that its top-level claim (or set of claims) is 692 satisfied, including systematic arguments, its underlying evidences and explicit assumptions that support the 693 claim(s) 
694 Note 1 to entry: An assurance argument contains the following and their relationships: 
695 ¡ª one or more claims about properties; 
696 ¡ª arguments that logically link the evidence and any assumptions to the claim(s); 
697 ¡ª a body of evidence and possibly assumptions supporting these arguments for the claim(s); and 
698 ¡ª justification of the choice of top-level claim and the method of reasoning. 
699 [SOURCE: ISO/IEC/IEEE 15026-1:2019 Modified: replaced "argumentation" with "arguments"] 
ISO/CD PAS 8800:2024(en)  
700  3.3.3  
701  claim  
702  true-false statement about the limitations on the values of an unambiguously defined property ¡ªcalled the  
703  claim's property ¡ªandlimitationson the uncertaintyof the property¡¯svaluesfalling within these limitations  
704  during the claim's duration of applicability under stated conditions  
705  Note 1 to entry: Uncertainties may also be associated with the duration of applicability and the stated conditions.  
706  Note 2 to entry: A claim potentially contains the following:  
707  ¡ª property of the system-of-interest;  
708  ¡ª limitations on the value of the property associated with the claim (e.g. on its range);  
709  ¡ª limitations on the uncertainty of the property value meeting its limitations;  
710  ¡ª limitations on duration of claim's applicability;  
711  ¡ª duration-related uncertainty;  
712  ¡ª limitations on conditions associated with the claim; and  
713  ¡ª condition-related uncertainty.  
714  Note 3 to entry: Theterm ¡°limitations¡± isusedtofit themany situationsthatcanexist. Valuescanbeasinglevalueor  
715  multiple single values, a range of values or multiple ranges of values, and can be multi-dimensional. The boundaries of  
716  these limitations are sometimes not sharp, e.g. they can involve probability distributions and can be incremental.  
717  [SOURCE: ISO/IEC/IEEE 15026-1:2019]  
718  3.3.4  
719  undesired safety-related behaviour at the vehicle level  
720  hazardous behaviour, RFIM prevention issue (3.3.10) or malfunctioning behaviour that can cause a hazard  
721  3.3.5  
722  hazard  
723  potential source of harm  
724  [SOURCE: ISO 26262-1:2018, 3.75, modified ¡ª deleted "caused by malfunctioning behaviour (3.88) of the  
725  item (3.84)" and Note 1 to entry]  
726  3.3.6  
727  influencing factor  
728  factor contributing to the achievement or the absence of a safety-related property (3.3.15)  
729  3.3.7  
730  misuse  
731  usage in a way not intended by the manufacturer or the service provider  
732  [SOURCE: ISO 21448:2022, 3.17 modified -Note to entries and examples where deleted. Some Notes to entries  
733  were changed into explicit definitions]  
734  3.3.8  
735  reasonably foreseeable  
736  technically possible and with a credible or measurable rate of occurrence  

737 Note 1 to entry: Expected misuse can be understood as a sub-class of reasonably foreseeable event. 
738 [SOURCE: ISO 26262-1:2018] 
739 3.3.9 740 reasonably foreseeable indirect misuse (RFIM) 741 reasonably foreseeable (3.3.8) misuse which leads to a reduced controllability of the hazardous behaviour, to 742 a potentially increased severity of an occurring accident or a combination of both 
743 [SOURCE: ISO 21448:2022, modified ¨Cterm was introduced within Note 5 to entry of definition 3.17 misuse 744 and now is explicitly defined] 
745 3.3.10 746 RFIM prevention issue 747 inability to prevent or detect and mitigate a RFIM (3.3.9) 
748 3.3.11 749 risk 750 combination of the probability of occurrence of harm and the severity of that harm 
751 Note 1 to entry: Other forms of risk definitions exist, e.g., risk for other topics like the risk of a project to fail, etc. This 752 document focuses on the risk regarding safety. Hence this definition was chosen. 
753 Note 2 to entry: The resulting risk evaluation of an error of an AI component is typically equivalent to the evaluation of 754 the potential to lead to a violation of a safety requirement allocated to the AI system. The evaluation can be quantitative 755 as well as qualitative, depending on the safety requirement. 
756 [SOURCE: ISO 26262-1:2018,3.128 ¨C
757 3.3.12 758 safety 759 absence of unreasonable risk 
760 [SOURCE: ISO 26262-1:2018] 






761 3.3.13 762 AI safety measure 
modified: Added Note 2 to entry] 
763 activity or technical solution to avoid, detect or control AI errors (3.4.1), to mitigate their harmful effects or a 764 combination thereof 
765 EXAMPLE AI safety analysis. 
766 Note 1 to entry: Safety measures include architectural measures. 
767 Note 2 to entry: The AI safety measures include ISO 26262 safety measures of AI elements (3.1.3)as well as measures to 768 address functional insufficiencies in compliance with ISO 21448 (e.g. functional modifications addressing SOTIF-related 769 risks). 
770 3.3.14 771 safety validation 772 assurance, based on examination and tests, that the safety goals are adequate and have been achieved with a 773 sufficient level of integrity 
774 Note 1 to entry: There are three terms, "AI model validation", "validation" and "safety validation", that are distinguished 775 in this document. AI model validation originates from the validation data used by the AI community, validation originates 
. ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved 
776  from classic system development and safety validation originates from ISO 26262. The three validation meanings are not  
777  the same.  
778  [SOURCE: ISO 26262-1:2018, modified -Deleted note 1 to entry, added new Note 1 to entry]  
779  3.3.15  
780  safety-related property  
781  property impacting safety  
782  3.3.16  
783  safety requirement  
784  requirement related to safety  
785  EXAMPLE 1 SOTIF requirement.  
786  EXAMPLE 2 Functional safety requirement.  
787  EXAMPLE 3 Technical safety requirement.  
788  Note 1 to entry: This includes, but is not limited to, safety requirements motivated by functional safety as well as SOTIF.  
789  3.3.17  
790  unreasonable risk  
791  risk judged to be unacceptable in a certain context according to valid societal moral concepts  
792  [SOURCE: ISO 26262-1:2018]  
793  3.3.18  
794  work product  
795  work product of the safety lifecycle that can be used as evidence within a safety assurance argument  
796  3.4 Safety: Root cause-, error-and failure-related definitions  
797  3.4.1  
798  AI error  
799  one or more discrepancies between computed, observed or measured values or conditions of the AI element,  
800  and the true, specified or theoretically correct values or conditions of the AI element  
801  Note 1 to entry: An AI error can be a single discrepancy or a sequence of discrepancies.  
802  Note 2 to entry: An AI error can be an error caused by a fault. Faults are typically addressed by ISO 26262.  
803  Note 3 to entry: An AI error can be an output insufficiency caused by a functional insufficiency  
804  3.4.2  
805  AI triggering condition  
806  specific conditions of a scenario that serve as an initiator for a subsequent AI error (3.4.1)  
807  Note 1 to entry: Functional insufficiencies (3.4.6) or faults (3.4.5) are themselves not AI triggering conditions but are  
808  potentially activated by them thus leading to the occurrence of an AI error (3.4.1).  
809  3.4.3  
810  contributing AI error  
811  AI error (3.4.1) which can lead to a violation of anAI safety requirement (3.1.16)allocated to the AI system  
812  (3.1.17), either by itself or in combination with one or more other AI errors (3.4.1).  

ISO/CD PAS 8800:2024(en)  
813  3.4.4  
814  AI error rate  
815  probability density of AI error (3.4.1)occurrence divided by probability of no AI error (3.4.1) occurring until  
816  the measuring point  
817  Note 1 to entry: Measurement units can include errors per hour, errors per km, etc.  
818  Note 2 to entry: This is an analogue definition to the failure rate.  
819  3.4.5  
820  fault  
821  abnormal condition that can cause an element or an item to fail  
822  Note 1 to entry: Permanent, intermittent, and transient faults (especially soft errors) are considered.  
823  Note 2 to entry: When a subsystem is in an error state it could result in a fault for the system.  
824  Note 3 to entry: An intermittent fault occurs from time to time and then disappears again. This type of fault canoccur  
825  when a component is on the verge of breaking down or, for example, due to an internal malfunction in a switch. Some  
826  systematic fault (3.4.13)s (e.g. timing irregularities) could lead to intermittent faults.  
827  [SOURCE: ISO 26262-1:2018, 3.54]  
828  3.4.6  
829  functional insufficiency  
830  insufficiency of specification (3.4.7) or performance insufficiency  
831  Note 1 to entry: A functional insufficiency activated by a triggering condition leads per definition to either an output  
832  insufficiency, a hazardous behaviour, a RFIM prevention issue or a combination of these.  
833  [SOURCE: ISO 21448:2022, 3.8, modified ¨CExamples, figures and notes to entry have been removed. A new  
834  Note to entry has been added as a replacement of Note 2 to entry]  
835  3.4.7  
836  insufficiency of specification  
837  specification, possibly incomplete, contributing to either a hazardous behaviour or an RFIM prevention issue  
838  (3.3.10) when activated by one or more triggering conditions  
839  Note 1 to entry: An insufficiency of specification activated by a triggering condition leads per definition to either an  
840  output insufficiency, a hazardous behaviour, an RFIM prevention issue (3.3.10) or a combination of these.  
841  Note 2 to entry: More details can be found in Clause 6.7.1.  
842  [SOURCE: ISO 21448:2022, 3.12, modified ¨CExamples, notes to entry have been removed. A new Note to entry  
843  has been added for clarification]  
844  3.4.8  
845  output insufficiency  
846  incorrect output of an element as a result of a triggering condition activating a functional insufficiency (3.4.6)  
847  of the element, contributing to either a hazardous behaviour, a RFIM prevention issue (3.3.10) or both  
848  [SOURCE: ISO 21448:2022, modified ¨Cterm was introduced within Note to entry 6 of definition 3.8 functional  
849  insufficiency and now is explicitly defined]  

ISO/CD PAS 8800:2024(en)  
850  3.4.9  
851  random hardware fault  
852  hardware fault with a probabilistic distribution  
853  [SOURCE: ISO 26262-1:2018, 3.119]  
854  3.4.10  
855  safety-related AI error  
856  AI error (3.4.1) of a safety-related AI element (3.1.3)  
857  3.4.11  
858  safety-related fault  
859  fault of a safety-related AI element (3.1.3)  
860  3.4.12  
861  systematic error  
862  error due to a systematic fault  
863  3.4.13  
864  systematic fault  
865  fault (3.4.5) whose failure is manifested in a deterministic way that can only be prevented by applying process  
866  or design measures  
867  [SOURCE: SOURCE: ISO 26262-1:2018, 3.165]  
868  3.5 Miscellaneous definitions  
869  3.5.1  
870  architecture  
871  representation of the structure of the item or element that allows identification of building blocks, their  
872  boundaries and interfaces, and includes the allocation of requirements to these building blocks  
873  [SOURCE: ISO 26262-1:2018]  
874  3.5.2  
875  architectural measure  
876  technical solution implemented by the AI element (3.1.3) to detect and mitigate or tolerate AI errors (3.4.1) in  
877  order to uphold the ability to execute the AI task (3.1.18) in a safe manner or to achieve or maintain a dedicated  
878  operating mode in case of AI errors (3.4.1) without unreasonable risk (3.3.11)  
879  EXAMPLE 1 Addition of output layers in the AI model for classification. AI models can make incorrect predictions that  
880  can lead to hazardous behaviour. Therefore, it would be beneficial for a model to be cautious in situations where it is  
881  uncertain about its predictions. One way to accomplish this is to design AI models by adding output layer(s) to represent  
882  reject class or reject option. Such models assess their confidence in each prediction and have the option to abstain from  
883  making a prediction when they are likely to make incorrect prediction.  
884  EXAMPLE 2 Addition of redundant AI components (3.1.1).  
885  Note 1 to entry: Architectural measure has a tangible impact on the AI system (3.1.17)or AI component (3.1.1)and lead to  
886  enhancement or modification of the architecture of AI system (3.1.17) or AI component (3.1.1).  
887  Note 2 to entry: an architectural artefact that is used during development, for example using saliency map to argue the  
888  explainability of the system, but removed for the system deployment, those measures are NOT considered architectural  
889  measures.  

ISO/CD PAS 8800:2024(en)  
890  3.5.3  
891  component  
892  non-system level element that is logically or technically separable and is comprised of more than one  
893  hardware part (3.5.6) or one or more software unit (3.5.10) or a combination of hardware part(s) and software  
894  unit(s)  
895  EXAMPLE A microcontroller.  
896  Note 1 to entry: A component is a part of a system (3.5.11).  
897  [SOURCE: ISO 26262-1:2018 -modified: added "or a combination of hardware part(s) and software unit(s)"]  
898  3.5.4  
899  development measure  
900  Appropriate process step(s) (activity) for the development of an AI system (3.1.17) or an AI component (3.1.1)  
901  that facilitates fulfilling AI safety requirement (3.1.16)s and/or enhancing the AI properties.  
902  Note 1 to entry: Analysis of AI System or AI component, specific activity used during or after the training of the AI  
903  component can be a development measure. Please refer to 10.4for more details.  
904  3.5.5  
905  element  
906  system, components (system, hardware or software), hardware parts, or software units  
907  Note 1 to entry: When¡°softwareelement¡±or ¡°hardwareelement¡± isused, thisphrasedenotesanelement ofsoftware  
908  only or an element of hardware only, respectively.  
909  [SOURCE: ISO 26262-1:2018 -modified: added "system" to the components and removed Note 2 to entry]  
910  3.5.6  
911  hardware part  
912  portion of a hardware component at the first level of hierarchical decomposition  
913  EXAMPLE The CPU of a microcontroller, a resistor, flash array of a microcontroller.  
914  [SOURCE: ISO 26262-1:2018]  
915  3.5.7  
916  item  
917  system or combination of systems, to which ISO 26262 is applied, that implements a function or part of a  
918  function at the vehicle level  
919  [SOURCE: ISO 26262-1:2018 -modified: Removed Note 1 to entry]  
920  3.5.8  
921  off-board  
922  property indicating that a given task is done external to the vehicle system  
923  3.5.9  
924  on-board  
925  property indicating that a given task is done internal to the vehicle system  
926  3.5.10  
927  software unit  
928  atomic level software component of the software architecture that can be subjected to stand-alone testing  

929 [SOURCE: ISO 26262-1:2018] 930 3.5.11 
931 system 932 set of components or subsystems that relates at least a sensor, a controller and an actuator with one another 933 Note 1 to entry: The related sensor or actuator can be included in the system or can be external to the system. 934 [SOURCE: ISO 26262-1:2018] 935 3.5.12 
936 testing 937 process of planning, preparing, and operating or exercising an item or element to verify that it satisfies 938 specified requirements, to detect safety anomalies, to validate that requirements are suitable in the given 939 context and to create confidence in its behaviour 
940 Note 1 to entry: "to create confidence in its behaviour" includes also the evaluation of the performance of the element or 941 item. 
942 [SOURCE: ISO 26262-1:2018, modified -added Note to entry] 943 3.5.13 944 validation 945 confirmation, through the provision of objective evidence, that the requirements for a specific intended use or 946 application have been fulfilled 
947 Note 1 to entry: There are three terms, "AI model validation", "validation" and "safety validation", that are distinguished 948 in this standard. AI model validation originates from the validation data used by the AI community, validation originates 949 from classic system development and safety validation originates from ISO 26262. The three validation meanings are not 950 the same. 
951 [SOURCE: ISO/IEC 22929:2022] 952 3.5.14 
953 verification 954 confirmation, through the provision of objective evidence, that specified requirements have been fulfilled 955 EXAMPLE The typical verification activities can be classified as follows: 956 ¡ª verification review, walk-through, inspection; 957 ¡ª verification testing; 
958 ¡ª simulation; 959 ¡ª prototyping; and 960 ¡ª analysis (safety analysis, control flow analysis, data flow analysis, etc.) 961 Note 1 to entry: Verification only provides assurance that a product conforms to its specification. 962 [SOURCE: ISO/IEC 22989:2022] 




963 4 Abbreviated terms 
ACP assurance claim point ADS automated driving system AI artificial intelligence ASIL automotive safety integrity level DLC dataset lifecycle DNN deep neural network E/E electrical/electronic FMEA failure mode and effects analysis FN false negative FP false positive FPS frames per second GSN goal structuring notation HARA hazard analysis and risk assessment HAZOP hazard and operability study HMI human machine interface KPI key performance indicator ID in distribution ML machine learning NN neural network ODD operational design domain OOD out of distribution OTA over the air PFD probability of failure on demand RFDM reasonably foreseeable direct misuse RFIM reasonably foreseeable indirect misuse SOTIF safety of the intended functionality TN true negative TP true positive 

964 5 Requirements for compliance 965 5.1 Purpose 
966 This clause describes how: 967 a) to achieve compliance with this document; 
968 b) to interpret the applicability of each clause; and 
969 c) to interpret the tables and figures used in this document. 
970 971 5.2 General requirements 
972 When claiming compliance with this document, each normative requirement shall be met, unless one of the 
973 following applies: 974 a) tailoring of the safety activities as defined in Clause 6 or in accordance with ISO 26262-2 has been 975 performed that shows that the requirement does not apply; or 
976 b) a rationale is available that the non-compliance is acceptable and the rationale has been evaluated in 977 accordance with this document and ISO 26262-2, when applicable. 
978 Informative content, including notes and examples, is only for guidance in understanding, or for clarification 979 of the associated requirement, and shall not be interpreted as a requirement itself or as complete or 980 exhaustive. 
981 The results of safety activities are given aswork products. ¡°Prerequisites¡± are information which shall be 982 available as work products of a previous phase or from an external source. Given that certain requirements of 983 a clause are ASIL dependent or may be tailored, certain work products may not be needed as prerequisites. 
984 5.3 Interpretations of tables and figures 
985 Tables and figures can be normative or informative depending on their context. Tables and figures that are 986 referenced by normative requirements are considered normative unless it is explicitly specified otherwise. 987 Other tables and figures are only informative. 
988 In case it is possible to fulfil a requirement with a different combination of methods, a rational is provided that 989 the chosen combination of methods fulfil the requirement. 



990 6 AI within the context of road vehicles system safety engineering and basic 991 concepts 
992 6.1 Application of the ISO 26262 series for the development of AI systems 
993 This document isintended to be applied in combination with the ISO 26262 seriesto specifically address the 
994 safety of AI systems. 995 ¡ª For AI components thatare not AI models, or do not contain AI models, the ISO 26262 series can be applied 996 by itself. 
997 ¡ª For AI components that are AI models, or that contain AI models, the ISO 26262 series can be tailored and 998 applied in combination with this document (see Figure 6¨C1). 

1000  Figure 6¨C1 ¡ª Visualization of the applicability of the ISO 26262 series and this document to the item  
1001  and its elements  
1002  NOTE  See Annex C for a possible tailoring of ISO 26262-4:2018 and ISO 26262-6:2018 for ML.  
1003  6.2 Interactions with encompassing system-level safety activities  
1004  An example interaction of the AI system development with the encompassing system development based on  
1005  the ISO 26262 series and ISO 21448 can be found in Table 6¨C1 and in Table 6¨C2. These tables also include  
1006  remarks regarding the interaction and the applicability of the corresponding standards with this document  
1007  and the development of the AI elements.  
1008  NOTE 1  ISO 21448 is applicable to intended functionalities where proper situational awareness is essential to safety  
1009  and  where  such  situational  awareness  is  derived  from  complex  sensors  and  processing algorithms, especially  
1010  functionalities of emergency intervention systems and systems having SAE levels of driving automation from 1 to 5.  
1011  However, this document is applicable to all AI systems which errors can impact safety independent of the vehicle-level  
1012  functionality. It is possible that systems utilize AI technologies that are not in scope of ISO 21448, resulting in the  
1013  applicability of this document but not ISO 21448.  
1014  NOTE 2  Although this document and ISO 21448 both focus on functional insufficiencies, the compliance with one does  
1015  not automatically imply compliance with the other.  
1016  During  the  encompassing system  architecture  design phase, encompassing system requirements  are  
1017  decomposed and allocated to the AI systems as well as other elements of the encompassing system.  
1018  Table 6¨C1 ¡ª Example interaction of the AI element development in compliance with this document  
1019  with the ISO 26262 series  

ISO 26262:2018a  Interaction of the AI system with the  AI system: System component  AI component: Conventionalb  AI component: AI model  
encompassing  consisting of  hardware  implemented by  
system (the AI  hardware and  component  software  
system is not an  software  component(s)  
item)  components  
2-5 Overall safety  - Adapted to address  Directly applicable  Adapted to address  
management  also the management  also the management  
of AI safety  of AI safety  
2-6 Project dependent safety management  The management of the AI safety is part of the safety management of the encompassing  Adapted to address also the management of AI safety  Directly applicable  Adapted to address also the management of AI safety  
system  
2-7 Safety  The management of  Adapted to address  Directly applicable  Adapted to address  
management  the AI safety is part  also the management  also the management  
regarding  of the safety  of AI safety  of AI safety  
production,  management of the  
operation, service  encompassing  
and  system  
decommissioning  
3-5 Item definition  Potential source of  - - - 
input space  
specification  
3-6 Hazard analysis and risk assessment  - - - - 
3-7 Functional safety  Potential source of  - - - 
concept  safety requirements  
allocated to the AI  
system  
4-6 Technical safety concept  Potential source of safety requirements allocated to the AI  Applicable (tailoring can be necessary)  Hardware safety requirements are derived from  Software safety requirements are derived from  
system  technical safety requirements allocated to the AI  technical safety requirements allocated to the AI  
system  system  
4-7 System and item  AI system as a  Integration of the  - - 
integration and  system component  hardware and  
testing  to be integrated into the encompassing  software components of the AI  
system  system (tailoring can  
be necessary)  
4-8 Safety validation  potential source of additional validation  - - - 
strategies and requirements  
Part 5: Product  Potential source of  Applicable (tailoring  Applicable  Refinement of  
development at the  hardware safety  can be necessary)  Hardware-Software  
hardware level  requirements  Interface  
allocated to the AI  
elements  

ISO 26262:2018a  Interaction of the AI system with the encompassing system (the AI system is not an item)  AI system: System component consisting of hardware and software components  AI component: Conventionalb hardware component  AI component: AI model implemented by software component(s)  
Part 6: Product  Potential source of  Applicable (tailoring  Refinement of  Applicable (tailoring  
development at the  software safety  can be necessary)  Hardware-Software  can be necessary)  
software level  requirements allocated to the AI elements  Interface  
Part 7: Production, operation, service and decommissioning  AI elements can be part of the production process  Potential source of requirements and work products relevant for production, operation, service and decommissioning  Potential source of requirements and work products relevant for production, operation, service and decommissioning  Potential source of requirements and work products relevant for production, operation, service and decommissioning  
a n-x means ISO 26262-n:2018, Clause x b conventional hardware is hardware which is not specifically designed to implement an AI model, e.g. CPUs, GPUs or FPGAs.  

1020 Table 6¨C2 ¡ª Example interactionswith ISO 21448 
ISO 21448:2022, Clause  Interaction of the AI system development with encompassing system activities, motivated by ISO 21448:2022  
5 Specification and design  Clause 5 activities: ¡ª Provide the interfaces of the AI system with the encompassing system. ¡ª Determine the semantic input space ¡ª Provide the functionality required from the AI system ¡ª Provide safety requirements allocated to the AI system, including, but not limited to, safety-related KPIs ¡ª etc ISO PAS 8800 activities: ¡ª Provide triggering conditions and functional insufficiencies of the AI system ¡ª Provide achieved safety-related KPIs ¡ª Provide a description of deployment measures required to support the AI and data lifecycles ¡ª etc  
6 Identification and evaluation of hazards  Clause 6 activities are a potential source of safety requirements, including, but not limited to, safety-related KPIs, allocated to the AI system  

ISO 21448:2022, Clause  Interaction of the AI system development with encompassing system activities, motivated by ISO 21448:2022  
7 Identification and evaluation of potential functional insufficiencies and potential triggering conditions  Clause 7 activities: ¡ª Are a potential source of safety requirements allocated to the AI system ¡ª Are a potential source of potential triggering conditions of the AI system ISO PAS 8800 activities: ¡ª Provide potential triggering conditions of the AI system  
8 Functional modifications addressing SOTIF-related risks  ISO PAS 8800 activities: Modification request to the encompassing system in case safety requirements allocated to the AI system cannot be fulfilled  
9 Definition of the verification and validation strategy  Clause 9 activities are a potential source of safety-related KPIs, allocated to the AI system  
10 Evaluation of known scenarios  ISO PAS 8800 activities ¡ª provide triggering conditions of the AI system and the associated AI error modes, error patterns or a combination of both; ¡ª Provides achieved safety-related KPIs  
11 Evaluation of unknown scenarios  ISO PAS 8800 activities provideachieved safety-related KPIs  
12 Evaluation of the achievement of the SOTIF  ISO PAS 8800 activities support with the safety assurance case the AI system part of the evaluation of the achievement of the SOTIF  
13 Operation phase activities  ISO PAS 8800 activities provide the AI system requirements to the encompassing system regarding the operation phase  

1021 During development and as part of continuous assurance activities during operation, it can become necessary 1022 to adjust the safety requirements allocated to the AI system leading to an iterative feedback cycle to the 1023 encompassing system safety concept and safety requirements. Iterations of the requirements aretriggered for 1024 example if: 
1025 ¡ª an AI system capable of fulfilling its assigned safety requirements and associated safety-related properties 1026 cannot be feasibly developed (e.g. due to inherent performance limitations in the machine learning 1027 algorithm used); 
1028 ¡ª suitable training data and test data cannot be found; or 
1029 ¡ª evidence to demonstrate that the safety requirements and associated safety-related properties are fulfilled 1030 cannot be collected with sufficient confidence. 
1031 In each of these cases, changes to the encompassing system safety concept can be defined, leading to a set of 1032 updated, realisable requirements on the AI system. 
1033 NOTE 3 Measures on the encompassing system safety concept to reduce the safety load of an AI system towards better 1034 feasibility can be (see also 10.5): 
1035 ¡ª restrictions in the ODD; 
1036 ¡ª implementation of diversity such as different processing algorithms or sensing modalities; 
1037 ¡ª implementation of redundancy such as multiple hardware components in parallel; or 
1038 ¡ª a combination of the aforementioned measures 
1039 Once an adequate level of performance of the AI system has been achieved (in relation to the safety 1040 requirements), then the AI system can be provided for integration into the encompassing system including 1041 evidence to support the achievement of the safety requirements. This can result in the need for further 1042 iterations of the AI safety life cycle, for example, due to the following conditions: 
1043 ¡ª Integration tests of the encompassing system reveal previously undiscovered faults or functional 1044 insufficiencies in the AI system that require additional development cycles. 
1045 ¡ª The encompassing system assurance case requires additional evidence to support safety claims related to 1046 the AI system that require additional effort to collect the required evidence. 
1047 Collected field data and observations made during operation related to the performance of the AI system (e.g. 1048 increased number of false positive errors under certain traffic conditions or an increased rate of out-of-1049 distribution inputs) can indicate changes in the input space. Those changes might not be able to be addressed 1050 by a refinement of the safety requirements on the AI system or through additional development activities but 1051 require changes to be made at the encompassing system level. In turn, this can lead to changes in the safety 1052 requirements assigned to the AI system and a repetition of the safety life cycle. 
1053 6.3 Mapping of abstraction layers between ISO 26262, ISO/IEC 22989 and this document 
1054 ISO 26262:2018 uses the following levels of abstraction: item, system, component, software unit and 1055 hardware part. The relationship of these are visualized in Figure 6¨C2 which corresponds to ISO 26262-1056 10:2018, Figure 3. Theterm¡°element¡±can mean ¡°system¡±, ¡°component¡±, ¡°software unit¡±or¡°hardware part¡±, 1057 depending on the context. It is typically used when a given requirement can be applied on different levels of 1058 abstraction, e.g. on hardware component as well as on hardware part level. An example item composition is 1059 shown in Figure 6¨C3 which corresponds to ISO 26262-10:2018, Figure 4. 

1061  NOTE 1  Depending on the context, the term ¡°element¡± canapply totheentities¡°system¡±,¡°component¡±, ¡°hardware  
1062  part¡± and ¡°softwareunit¡± inthischart, according to ISO26262-1:2018, 3.41.  
1063  NOTE 2  "*" sign means N elements are possible, where N is a positive integer number.  
1064  Figure 6¨C2 ¡ª Relationship of item, system, component, hardware part and software unit  


1066 Figure 6¨C3 ¡ª Example item composition 
1067 ISO/IEC 22989 uses the following abstraction layers: AI system and AI components, where the AI system 1068 consists of AI components. ISO/IEC 22989 does not explicitly state if a given AI component can itself consist 1069 of AI components. In this document this is possible. The AI component can be an AI model, a conventional 1070 element, i.e. an element not considered to be an AI model, or a combination of both. The AI system contains at 1071 least one AI model and realizes the AI task. Figure 6¨C4 uses the same notation as Figure 6¨C2 to visualise the 1072 relationship between AI system and AI components. 

1074 NOTE 1 Dependingonthecontext, theterm ¡°AIelement¡± canapply to ¡°AIsystem¡± and ¡°AIcomponent¡± 1075 NOTE 2 "*" sign means N elements are possible, where N is a positive integer number. 
ISO/CD PAS 8800:2024(en)  
1076  Figure 6¨C4 ¡ª Relationship of AI system and AI component  
1077  This document uses terms from both ISO/IEC 22989:2022 and ISO 26262-1:2018. The terms from the  
1078  different standards do not map one-to-one. So, depending on the context, multiple mappings are possible as  
1079  shown in Table 6¨C3  
1080  Table 6¨C3 ¡ª Possible mappings between ISO/IEC 22989 and ISO 26262 terms, depending on the  
1081  context  

ISO/IEC 22989:2022 terminology  ISO 26262:2018 terminology  
AI system  Item, system, component, software unit or hardware part Element  
AI component  System, component, software unit or hardware part Element  

1082  Theterm¡°AI system¡±servesinthisdocumentasthe top level of abstraction of the contentto be developed. As  
1083  such it is possible in a distributed development that what one party considers to be an AI component, the other  
1084  party considers to be an AI system, as for them it represents the top level of the content they develop.  
1085  Figure 6¨C5 provides an example of an item decomposed into its elements. In this example the item consists of  
1086  two systems, system A and system B. For the sake of simplicity system B is not further decomposed. System A  
1087  is composed out of the system components A.1 and A.2. System component A.1 does not contain an AI model.  
1088  As such it cannot be an AI system. System component A.2 contains an AI model and is declared to be the AI  
1089  system in this example.  
1090  NOTE  It would have also been possible to declare system A as the AI system, as it too contains at least one AI model.  
1091  The decision regarding the scope of the AI system is made as part of negotiations between the development organisations  
1092  responsible for the encompassing system.  
1093  The system components themselves consist of hardware and software components. In  case  of system  
1094  component A.2 these components are considered to be AI components as they compose the AI system. A  
1095  further breakdown into software units and hardware parts of the components has been omitted for sake of  
1096  simplicity.  


1098 Figure 6¨C5 ¡ª Example of a hierarchical decomposition of an item into its elements down to the 1099 component level -decomposition tree view 
ISO/CD PAS 8800:2024(en)  
1100  6.4 Example architecture for an AI system  
1101  This document uses the architecture shown in Figure 6¨C6 as an example architecture. The AI system receives  
1102  its input from the source, executes its task based on the input and the control signals and then provides its  
1103  output to the consumer. The AI system itself consists of the AI components AI pre-processing, AI model and  
1104  AI post-processing. The AI post-processing hereby uses data provided from the previous process steps (i.e. AI  
1105  pre-processing and the AI model) in combination with the original input data for monitoring purposes.  
1106  NOTE 1  This architecture is just an example and has no claim of representing all possible architectures of AI systems.  
1107  NOTE 2  If the AI system is implemented for a real-time task, the execution of the AI system could be triggered  
1108  synchronously or asynchronously depending on the behaviour of the control element, sources of the input streams and  
1109  the consumers of the outputs. These considerations can be relevant to the definition of the safety requirements on the AI  
1110  system.  


1112 Figure 6¨C6 ¡ª Example architecture of an AI system 
1113 




1114 6.5 Types of AI models 
1115 Examples for types of AI models include, but are not limited to, deep neural networks, k-nearest neighbours, 1116 support vector machines, decision trees, symbolic AI and fuzzy logic. These can be clustered in different 1117 categories. For example, deep neural networks, k-nearest neighbours, support vector machines and decision 1118 trees can be categorized as ML models as shown in Figure 6¨C7 

1120 Figure 6¨C7 ¡ª Example of different types of AI models 
. ISO 2024 ¨CAll rights reserved 
1121 


1122 6.6 AI technologies of a ML model 
1123 Figure 6¨C8 and Figure 6¨C9 show an example of possible AI technologies utilized for a ML model that is 1124 implemented in hardware and software. Next to the AI method itself the AI technology also contains the tools 1125 and procedures to generate the AI model. 

1127 Figure 6¨C8 ¡ª AI technologies to create an executable ML model (application of ISO 26262) 

1128 
. ISO 2024 ¨CAll rights reserved 
1129  Figure 6¨C9 ¡ª AI technologies to create a trained ML model (application of this document)  
1130  The AI technologies listed in Figure 6¨C8 are not considered to be relevant sources for functional insufficiencies,  
1131  e.g.  compiler  and  linker  are  well  known  technologies already  utilized  by  non-AI  systems.  For  these  
1132  technologies the application of the ISO 26262 series is considered to be sufficient in order to achieve AI safety.  
1133  The AI technologies listed in Figure 6¨C9 are considered to be relevant sources of functional insufficiencies for  
1134  which the application of the ISO 26262 series alone is not considered to be sufficient to achieve AI safety. For  
1135  these technologies the remaining clauses of this document are applied.  
1136  6.7 Error concepts, fault models and causal models  
1137  6.7.1  Cause-and-effect chain  
1138  This document utilizes the concept of AI triggering conditions, faults, functional insufficiencies, AI errors and  
1139  the undesired safety-related behaviour at the vehicle level. The mapping of the terms of this document to the  
1140  cause-and-effect chain used by ISO 21448:2022 can be found in Figure 6¨C10. The mapping of the terms of this  
1141  document to the cause-and-effect chain used by ISO 21448:2022 can be found in Figure 6¨C11. The AI triggering  
1142  condition activates a fault or a functional insufficiency, resulting in an AI error in case of a fault and a  
1143  contributing AI error in case of a functional insufficiency.  


1145  Key  
a  An output insufficiency, either by itself or in combination with one or more output insufficiencies of other  
elements, contributes to either a hazardous behaviour at the vehicle level or an inability to prevent or detect  
and mitigate a reasonably foreseeable indirect misuse.  
b  Since a triggering condition of ISO 21448 results in a contributing AI error in the context of this document, they  
represent a subset of all AI triggering conditions  
1146  Figure 6¨C10 ¡ª Mapping of the cause-and-effect chain of ISO 21448 to the terms of this document  
1147  EXAMPLE  An insufficiency of specification could be a missing object in the AI training, AI validation and AI test  
1148  dataset of an AI system utilizing an ML model for object classification. In this case encountering this object during  
1149  operation in the field is the AI triggering condition activating this insufficiency of specification, resulting in the occurrence  
1150  of a contributing error. The contributing AI error would be the incorrect classification of the object by the ML model and  
1151  consequently by the AI system.  
1152  An AI error of an AI component can propagate through the AI system and can result in an AI error of the AI  
1153  system. The AI error of the AI system can propagate through the encompassing system and can contribute  
1154  either by itself or in combination with one or more other errors or output insufficiencies of the elements of  
1155  the encompassing system to an undesired safety-related behaviour at the vehicle level.  


ISO/CD PAS 8800:2024(en)  
1157  Key  
a  In ISO 26262 there is neither a dedicated term for the condition which activates a fault nor is this concept  
explicitly utilized.  
b  A failure, either by itself or in combination with one or more failures of other elements, contributes to a  
malfunctioning behaviour.  
1158  Figure 6¨C11 ¡ª Mapping of the cause-and-effect chain of ISO 26262 to the terms of this document  
1159  The undesired safety-related behaviour at the vehicle level is used as an umbrella term for the corresponding  
1160  terms of ISO 26262:2018 (i.e. the malfunctioning behaviour at the vehicle level which can cause hazards) and  
1161  ISO 21448:2022 (i.e. the hazardous behaviour at the vehicle level and RFIM prevention issue).  
1162  6.7.2  Root cause classes  
1163  Different error classes can be distinguished depending on the root cause. The correlation of contributing AI  
1164  errors and their different root causes is shown in Figure 6¨C12.  
1165  NOTE 1  A contributing AI error of the AI element can lead to the undesired safety-related behaviour at the vehicle  
1166  level by itself or in combination with one or more other AI errors.  


1168  Figure 6¨C12 ¡ª Correlation of safety-related errors with their different classes of root causes  
1169  The root causes for the different kinds of AI errors are:  
1170  ¡ª Insufficiency of the specification  
1171  EXAMPLE 1  An insufficiency of the specification could be missing datasets in the AI training, AI validation or AI test  
1172  dataset. The resulting output insufficiency could be a misclassification when exposed to the missing datasets.  
1173  EXAMPLE 2  Specification of a neural network model with insufficient complexity.  
1174  EXAMPLE 3  An inadequate training loss function.  
1175  EXAMPLE 4  Inadequate labelling specification  
1176  ¡ª Performance insufficiency  
1177  EXAMPLE 5  A performance insufficiency could be an insufficient range of a sensor in case of certain environmental  
1178  conditions. The resulting output insufficiency could be a false negative detection of an obstacle in the trajectory.  
1179  NOTE 2  In the case of ML models, performance insufficiencies can be specifically caused by training and test  
1180  dataset related issues, e.g. insufficiencies in the coverage of the respective input space. These data-related issues are  
1181  in turn considered to be insufficiencies of specification, or more precisely as insufficiency of specification of the data.  
1182  ¡ª Contributing systematic fault  

1183 EXAMPLE 6 A contributing systematic fault could be to divide by zero in software or to use incorrect variable names. 
1184 EXAMPLE 7 Overfitting the DNN resulting in wrong high-confidence classification outputs of corner cases can be 1185 regarded to be a contributing systematic fault in the training procedure. 1186 NOTE 3 Sometimes the classification of a given issue in either a systematic fault or a functional insufficiency can 
1187 be ambiguous. Independent of the classification a safety assurance argument is provided to argue that this issue 1188 does not represent an unreasonable risk. As long as this safety assurance argument is available, the exact 1189 classification is not relevant. 
1190 ¡ª Contributing random hardware fault 
1191 EXAMPLE 8 Physical defect causing a short to ground. 
1192 When evaluating the effectiveness of safety mechanisms, it can be necessary to distinguish the different classes 
1193 of errors. 1194 EXAMPLE 9 Homogenous redundancy can be effective in detecting random hardware errors in one of the redundant 1195 elements, but it is not effective in detecting systematic errors. 
1196 The different fault classes can also be used within the safety assurance argument by addressing each root 
1197 cause class with a dedicated set of safety measures. 1198 EXAMPLE 10 Coding guidelines are a measure to avoid systematic faults in SW. In combination with other fault 1199 avoidance measures, e.g. as specified in ISO 26262-6:2018, the absence of unreasonable risk due to systematic faults 1200 could be argued. 
1201 It can also be useful to classify the AI triggering conditions in different categories. 1202 EXAMPLE 11 For ML based AI models the AI triggering conditions can be distinguished as: 1203 ¡ª cases that are similar to known training data samples ("in-distribution" cases), such as: cases at decision boundaries 1204 ("hard" cases), and undecided cases (high aleatoric uncertainty e.g. due to label noise); 1205 ¡ª unseen cases ("out-of-distribution" cases), such as: novel objects (semantic shift), or novel image styles (covariate 1206 shift); 1207 ¡ª addition of small changes to a sample which causes no error (e.g. addition of adversarially crafted perturbation) 1208 ¡ª applying a maliciously inserted trigger pattern (e.g. inserted via data or model poisoning) 
1209 Statistically obtained ML models like deep neural networks exhibit for any output an inherent uncertainty 1210 about their correctness. Therefore, besides classification of AI errors, root causes, and AI triggering conditions, 1211 also a classification of uncertainty types and their associated sources can be helpful in practice. 
1212 EXAMPLE 12 The two major types used to model the uncertainty of the ML model are epistemic and aleatoric 1213 uncertainty. While epistemic uncertainty stems from uncertainty of having the right model for the given sample and can 1214 often be fixed with more data, aleatoric uncertainty stems from intrinsic noise in the training data. 
1215 6.7.3 Error classification based on the safety impact 
1216 Compared to the criticality classification of random hardware faults as described in ISO 26262-5:2018 this 1217 document uses a simplified scheme as shown in Figure 6¨C13. 

1219  Key  
(1)  Is the element a safety-related element?  
(2)  Can the error significantly contribute to the occurrence of a safety-related undesired behaviour at the vehicle  
level?  
1220  Figure 6¨C13 ¡ª Error classification scheme based on the potential to lead to an undesired safety- 
1221  related behaviour at the vehicle level  
1222  7  AI safety management  
1223  7.1 Objectives  
1224  The objectives of this clause are:  
1225  a)  to define an AI safety lifecycle and its activities to ensure that contributing errors of the AI system do not  
1226  lead to unreasonable risk of undesired safety-related behaviour at the vehicle-level. The AI safety lifecycle  
1227  includes:  
1228  1)  a definition of activities necessary to develop the AI system, to provide the assurance and the evidence  
1229  that the AI system is safe and to ensure the AI safety during operation;  
1230  2)  in case of the utilization of ML based AI techniques: a data-driven, iterative approach for the  
1231  development, evaluation and continuous assurance of AI-based functions within the context of a  
1232  system-level safety lifecycle;  
1233  b)  to ensure that overall and project specific safety management processes and activities are appropriate to  
1234  ensure the safety of the AI system;  
1235  NOTE  ISO 26262-2:2018 provides suitable guidance on overall safety management and project specific safety  
1236  management. This guidance can require extensions based on recommendations in this document.  
1237  c)  to plan, initiate and conduct the AI safety activities.  
1238  
1239  7.2 Prerequisites and supporting information  
1240  The following information shall be available (from external sources):  
1241  a)  the AI system definition (from external sources), including:  
1242  1)  the AI system functionality;  
1243  2)  the interfaces of the AI system with the encompassing system, including if applicable, the ASIL  
1244  capability of the inputs to the AI system.  

1245  3) the safety requirements allocated to the AI system, including if applicable:  
1246  i) the ASIL rating of the safety requirements;  
1247  ii) the acceptance criteria or validation targets derived in compliance with ISO 21448:2022, Clause  
1248  6 or 9.  
1249  NOTE 1 The safetyrequirements allocated to the AI system from external sources are typically requirements regarding  
1250  the avoidance or control of safety-related faults, the allowed maximum error occurrence rate of contributing AI errors,  
1251  the identification of AI triggering conditions, and the robustness against certain environmental conditions.  
1252  NOTE 2 For an elaboration of the fault model, the causal model, and the error concepts used by this document, see 6.7.  
1253  7.3 General requirements  
1254  7.3.1 An AI safety lifecycle shall be defined that specifies the activities necessary to develop the AI system,  
1255  to provide the assurance and the evidence that the AI system is safe and to ensure the maintenance of AI safety  
1256  during operation. It can be based on the reference lifecycle (Figure 7¨C2) and can be tailored according to  
1257  project specific needs. The tailoring is supported by a rational why the tailored AI safety lifecycle is sufficient  
1258  and appropriate to achieve AI safety.  
1259  7.3.2 At each phase within the AI safety life cycle, work products shall be defined to support the safety  
1260  assurance claims of the AI system.  
1261  7.3.3 The activities of the AI safety lifecycle shall be coordinated with the safety lifecycle activities of the  
1262  encompassing system as defined by ISO 26262-2 and, if applicable, ISO 21448.  
1263  NOTE The AI system can be developed as a safety element out ofcontext. The concept of a safety element out of  
1264  context is described in ISO26262-10:2018, Clause 9 "Safety Element out of Context".  
1265  7.3.4 The activities described in ISO 26262-2:2018 shall be adapted in order to address the management of  
1266  AI safety, including:  
1267  a) theintegration of the AI safety lifecycle into the ISO 26262 safety lifecycle (see Figure.2 of ISO26262- 
1268  2:2018);  
1269  b) the enhancement of ¡°functional safety¡± to ¡°AI safety¡±;  
1270  c) measures to ensure that a sufficient level of cross domain competences regarding safety and AI are  
1271  available, in compliance with ISO 26262-2:2018, 5.4.4.1;  
1272  d) adding this document as a relevant standard of ISO 26262-2:2018;  
1273  e) the use of a safety assurance argument as part of the safety case of ISO 26262-2:2018;  
1274  f) extending the safety plan of ISO 26262-2:2018 to includethe safety activities of this document;  
1275  g) Tailoring of Table 1 of ISO 26262-2:2018 to address the workproducts of this document (see Table 7¨C1).  
1276  Table 7¨C1 ¡ª Required confirmation measures, including the required level of independence  

. ISO 2024 ¨CAll rights reserved 
Confirmation  Level of independencea applies to  Scope  
measure  QM  ASIL A  ASIL B  ASIL C  ASIL D  
Confirmation review of the safety plan Independence with regard to the developers of the itemb, project management and the authors of the work product  - I1  I1  I2  I3  Applies to the highest ASIL among the safety requirements  
Confirmation review of the AI system validation report Independence with regard to the developers of the itemb, project management and the authors of the work product  - I0  I1  I2  I2  Applies to the highest ASIL among the safety requirements  
Confirmation review of the AI safety analyses Independence with regard to the developers of the itemb, project management and the authors of the work product  - I1  I1  I2  I3  Applies to the highest ASIL among the safety requirements  
Confirmation review of the safety assurance argument Independence with regard to the authors of the safety  - I1  I1  I2  I3  Applies to the highest ASIL among the safety requirements  

Confirmation measure  Level of independencea applies to  Scope  
QM  ASIL A  ASIL B  ASIL C  ASIL D  
assurance argument  
AI safety audit Independence with regard to the developers of the itemb and project management  - - I0  I2  I3  Applies to the highest ASIL among the safety requirements  
AI safety assessment Independence with regard to the developers of the itemb and project management  - - I0  I2  I3  Applies to the highest ASIL among the safety requirements  
a The notations are defined as follows: ¡ª -: no requirement and no recommendation for or against regarding this confirmation measure; ¡ª I0: the confirmation measure should be performed; if the confirmation measure is performed, it shall be performed by a different person in relation to the person(s) responsible for the creation of the considered work product(s); ¡ª I1: the confirmation measure shall be performed, by a different person in relation to the person(s) responsible for the creation of the considered work product(s); ¡ª I2: the confirmation measure shall be performed, by a person who is independent from the team that is responsible for the creation of the considered work product(s), i.e. by a person not reporting to the same direct superior; ¡ª I3: the confirmation measure shall be performed by a person who is independent, regarding management, resources and release authority, from the department responsible for the creation of the considered work product(s). b The developers of the item include the developers of the AI system  



1277 7.4 Reference AI safety life cycle 
1278 The reference AI safety life cycle described in this clause covers the activities at the different phases of the AI 1279 system development, deployment and operation: safety-related requirements derivation, AI system design, 1280 verification and validation, deployment and operation. The AI safety life cycle is summarised in Figure 7¨C1and 1281 is used to structure the remainder of this document. A detailedview of the AI system design and V&V phase is 1282 shown in Figure 7¨C2. Clause 8 through Clause 15(as indicated by the numbered black dots in Figure 7¨C1 and 1283 Figure 7¨C2) are used to describe the activities within the safety life cycle in more detail. 
. ISO 2024 ¨CAll rights reserved 

1284 
1285 Key black circle represent clause number(s) with number s arrow show the flow and iterative nature in particular of the AI component design, implementation and verification and the whole AI system design and verification 
1286 Figure 7¨C1 ¡ª Reference AI safety lifecycle 

1288  Key  
black circle represent clause number(s)  
with  
number  
s  
arrow show the flow and iterative nature in particular of the AI component design, implementation and verification and  
the whole AI system design and verification  
1289  Figure 7¨C2 ¡ª Detailed view of the AI system design and V&V phase of the reference AI safety lifecycle  
1290  7.5 Iterative development paradigms for AI systems  
1291  The AI system development phase of the AI safety life cycle covers all activities required to design, implement,  
1292  verify and validate the AI system.  
1293  The AI system development activities described in Clause 8through Clause 13 are iteratively performed until  
1294  a sufficient level of performance with respect to the safety requirements and the associated safety properties  
1295  can be demonstrated and associated workproducts are generated.  
1296  An essential characteristic of this development process is the analysis to identify potential functional  
1297  insufficiencies, their root causes,and their impact on safety. This analysis is used to derive appropriate  
1298  measures to reduce these functional insufficiencies during design (including the selection of training data in  
1299  case of ML) as well as to reduce the impact of contributing errors (through architectural measures).  
1300  NOTE 1 DuringtheAI system development, thepropertyof ¡°independence¡± is considered whereverappropriate(e.g.  
1301  independence between training datasets and test datasets). An analysis similar to the Dependent Failure Analysis (DFA)  
1302  described in ISO 26262-9:2018, Clause 7 can be used.  
1303  The development model described here reflects the iterative development model typically used in the area of  
1304  machine learning with a focus on activities to identify, analyse, reduce and mitigate functional insufficiencies  
1305  in the trained model and the cumulative collection of evidences to support claims regarding safety  
1306  requirements allocated to the function. Furthermore, for machine learning, the specification and collection of  
1307  suitable training and test data is one of the most influential factors for the performance of the function.  
1308  Therefore, the specification, planning, collection, acquisition, preparation and the labelling of data related to  

1309 AI component implementation and verification process is treated in this document as asafety-related 1310 development activity with specific objectives and associated safety artefacts (see Clause 11). 
1311 The iterative development of the AI system is guided by a set of performance indicators, including safety-1312 related properties associated with the safety requirements allocated to the AI system. 
1313 NOTE 2 ¡°Iteration¡± inthecontext ofAI system developmentcanbedefined asasinglerepetitionofoneormoreAI 1314 system safety lifecycle phases, that is applied until a set of conditions defined by a set of KPIs or other target parameters 1315 are either fulfilled or demonstrated to be unachievable. 
1316 NOTE 3 Considering that there can be multiple, potentially conflicting target KPIs, the presence of contributing errors 1317 in the AI system can be inevitable. 
1318 Despite the inherently iterative nature of the AI system development, different iteration cycles of the 1319 development can focus on different sets of performance indicators, depending on the maturity of the 1320 development. 
1321 Examples of such distinct iteration cycles can include: 
1322 ¡ª Proof of concept: In this phase of development, AI models aredesigned, implemented and tested against 1323 a set of initially defined safety requirements. The objective of this phase is to evaluate the potential of the 1324 chosen AI technologies to fulfil the safety requirements, as well as defining a set of measures required to 1325 minimise the number or the impact of functional insufficiencies in the function (e.g. optimisation of model 1326 parameters and defining adata collection strategy). 
1327 ¡ª Series development of the AI system: In this phase of development, the AI system is iteratively 1328 developed against all safety requirements. Errors of the AI system or the AI model are analysed with 1329 respect to their potential root causes and measures are defined to reducethe functional insufficiencies 1330 through design and data selection or for minimising their impact through architectural measures. This 1331 phase of development can use a host platform for implementing and testing the AI model and will continue 1332 until an adequate level of performance for all safety-related properties has been met. 
1333 ¡ª Deployment to the target hardware platform: In this phase of development, the AI system is transferred 1334 to the target hardware platform and target software platform. This can include, for example a change in 1335 numerical precision used to calculate the results as well as the consideration of constraints such as timing, 1336 memory limitations (e.g. resulting in the necessity to prune the computational graph) and robustness 1337 against potential random hardware faults. The focus of this development phase is to ensure that the safety 1338 requirements are nevertheless met, despiteany limitations of the target hardware platform and the target 1339 software platform. 
1340 ¡ª Furtherimprovement after deployment: In this phase of development, the AI system is iteratively 1341 updated based on observations made during operation in the field and new requirements mandated by 1342 the developer. This can include compensating for previously unknown triggering conditions (e.g. concept 1343 drift) and distributional shift in the environment (e.g. domain shift). Performance indicators within this 1344 phase of development are monitored to ensure a monotonic safety improvement with respect to previous 1345 iterations of the AI model. This phase of development can also include development versions of the 1346 function runningin¡°shadow¡±mode withinanoperational environmentinorder tocollect suitable data 1347 and evaluate the potential performance under realistic conditions. 
1348 7.6 Work products 
1349 7.6.1 AI safety lifecycle resulting from 7.3.1 to7.3.4. 
1350 7.6.2 Work products of ISO 26262-2:2018, 5.5, resulting from 7.3.4. 
1351 7.6.3 Work products of ISO 26262-2:2018, 6.5,resulting from 7.3.3and 7.3.4, in particular the safety plan. 
ISO/CD PAS 8800:2024(en)  
1352  7.6.4 Work products of ISO 26262-2:2018, 7.5, resulting from 7.3.4.  
1353  8 Assurance arguments for AI systems  
1354  8.1 Objectives  
1355  The objectives of this clauseare:  
1356  a) to develop an assurance argument demonstrating that the safety requirements allocated to the AI system  
1357  are fulfilled;  
1358  NOTE 1 The assurance argument for the AI system contributes to the safety assurance argument of the  
1359  encompassing system.  
1360  NOTE 2 The assurance argument can be developed independently from the encompassing system as a safety  
1361  element out of context (SEooC) development activity (see ISO 26262-10:2018, Clause 9[5]). In such cases, the  
1362  assurance argument documents all necessary assumptions on the encompassing system for arguing that the safety  
1363  requirements allocated to the AI system are fulfilled.  
1364  b) to evaluate whether the assurance argument reflects the actual residual risk of the AI system violating its  
1365  safety requirements.  
1366  8.2 Prerequisites and supporting information  
1367  The following information shall be available at the initiation of these activities:  
1368  a) the AI system definition (from external sources), including:  
1369  1) a specification of the safety requirements allocated to the AI system;  
1370  2) a definition of the technical context within the encompassing system (e.g. definition of interfaces to  
1371  and from the encompassing system and, if applicable, the environment, conditions under which the  
1372  AI system functionality is triggered, etc.);  
1373  NOTE 1 This includes the ASIL capability and noise to signal ratio of the input signals provided by the source, if  
1374  applicable.  
1375  3) a specification of the input space;  
1376  b) requirements on the assurance argument and work products for the AI system (from external sources).  
1377  These requirements can be derived from the assurance argument of the encompassing system as well as  
1378  safety management procedures from Clause 7;  
1379  The following information shall be available for the finalization of these activities:  
1380  c) the work products of the AI safety lifecycle;  
1381  NOTE 2 This body of evidence can be cumulatively collected as part of the iterative development process phases or  
1382  produced within dedicated development process cycles  
1383  The following information can be considered for the finalization of this phase:  
1384  d) required properties of the encompassing system to achieve AI safety;  
1385  e) relevant properties of the input space (e.g. distribution of critical events, physical constraints on changes  
1386  of input values over time);  

1387 f) evidence of organization-specific rules and processes for AI safety, evidence of competence management 
1388 and evidence of a qualitymanagement system,.from 6.6.2; 1389 g) evidence that the organization-specific rules and processes have been followed and that the work 1390 products have the required maturity and quality, from 6.6.3 and 6.6.4. 
1391 8.3 General requirements 
1392 8.3.1 An assurance argument for the fulfilment of the safety requirements allocated to the AI system shall 1393 be provided. 1394 NOTE 1 The assurance argument can be part of the encompassing system's safety case in accordance with ISO 26262-
1395 2:2018, 6.4.8. 1396 NOTE 2 The assurance argument can be constructed at the level of the encompassing system. In this case, the 
1397 development of the AI system-specific contributions and supporting evidence are considered part of the AI safety life 1398 cycle and therefore within the scope of this document. 1399 8.3.2 The assurance argument shall usethe relevant work products generated during the AI safety lifecycle 
1400 to support the assurance claims. 
1401 NOTE Changes to the work products and their impact on the assurance argument are considered as part of change 1402 management throughout the AI safety life cycle. 1403 8.3.3 Confirmation measures of 7.3.4 shall be applied to the assurance argument. 1404 NOTE The evaluation of the validity of the assurance argument can be performed as part of confirmation measures 
1405 of the encompassing system (see ISO 26262-2:2018, 6.4.9 and ISO 21448:2022, 12.3), or as an independent activity, for 1406 example in the case of a SEooC (see ISO 26262-10:2018, clause 9) or as part of a distributed development. 1407 8.4 AI system-specific considerations in assurance arguments 
1408 A number of AI system-specific considerations impact the creation of the assurance argument. 1409 a) The formulation of the AI safety requirements (see Clause 9): 1410 ¡ª These include quantitative properties expressed in the form of probabilities (e.g. proportion of 1411 false positive classifications). 1412 ¡ª Arguments are expressed that demonstrate that these properties have been achieved with a level 1413 of statistical confidence appropriate to the quantitative targetsassociated with the requirement¡¯s 1414 acceptance criteria. 1415 ¡ª This can lead to additional requirements on the nature of evidence to support the claim and how 1416 the validity of this evidence is evaluated. 1417 b) Statistical arguments related to aggregated performance metrics: 1418 ¡ª These might not be sufficient to argue a suitable level of safety in rare, but critical situations (e.g. 1419 edge cases, sensor defects or adversarial perturbations). 1420 ¡ª Arguments can be required to demonstrate that such input conditions nevertheless lead to a 1421 suitable level of AI safety. 1422 ¡ª The probability of unknown triggering conditions leading to a violation of safety requirements can 1423 depend on: 
1424 ¡ª features of the input space not directly related to the function (e.g. due to spurious 1425 correlations in the training data) and 
1426 ¡ª predictions based on past inputs (e.g. the accuracy of previous detections of dynamic objects 1427 could impact the future behaviour of a planning task). 
1428 c) Verification of the AI system: 
1429 ¡ª Direct introspective approaches of AI models might not be effective. 
1430 ¡ª Alternative means of arguing the correct behaviour of the AI model or an increased reliance on 1431 indirect verification (e.g. test) can be required. 
1432 EXAMPLE Due to the lack of transparency with respect to the individual contributions of the large number of 1433 parameters used in some machine learning models, introspective approaches to verification might have limited 1434 applicability. 
1435 d) Reliance on training and test data: 
1436 ¡ª In machine learning-based AI methods, the behaviour of the AI system as well as its verification 1437 and validation are predominantly reliant on the selection of suitable training and test datasets as 1438 well as the training procedures themselves. 
1439 ¡ª Dedicated assurance arguments for demonstrating how the data selection process supports the 1440 achievement of AI safety might be required (see Clause 11). 
1441 ¡ª These arguments consider the training process and associated tools (see Clause 15). 
1442 e) Conditions during operation: 
1443 ¡ª Conditions can occur during operation that invalidate the assurance argument due to the complex 1444 nature of the environment in which vehicles containing AI systems are deployed. 
1445 ¡ª These conditions might include distributional shift of the input space (e.g. new types of road 1446 vehicles, changes in road infrastructure), changes to the technical system (e.g. replacement or 1447 upgrade of sensors) or previously undiscovered unknown triggering conditions. 
1448 ¡ª A continual, periodic re-evaluation and adaptation of the assurance argument is therefore 1449 performed, including an impact analysis of which parts of the assurance argument and associated 1450 evidence are to be re-evaluated (see Clause 14). 
1451 NOTE The degree to which the continual, periodic re-evaluation is required depends on properties of the input 1452 space and operating environment. If the input space, its distribution, and change of distribution over time (e.g. due 1453 to ageing) are well known, re-evaluation can be performed within regular software update activities. 
1454 8.5 Structuring assurance arguments for AI systems 
1455 8.5.1 Context of the assurance argument 
1456 Within the scope of this document, assurance relates to the claim that the AI system achieves AI safety. An 1457 assurance argument communicates the relationship between evidence and the AI safety requirements. 
1458 The level of confidence in the assurance argument should be appropriate to the required level of integrity 1459 (functional safety) and acceptance criteria assigned to the AI system within the context of the encompassing 1460 system. 
1461 NOTE A model-based graphical representation of the assurance argument can aid the communication and 1462 evaluation of the assurance argument. Examples of graphical notations for assurance arguments include the Goal 1463 Structuring Notation (GSN)[6] and Claims Argument Evidence (CAE)[7] based on the Structured Assurance Case 1464 Metamodel (SACM)[8]. 
1465 The structure of the assurance argument can appeal to: 1466 ¡ª features of the implemented item (product argument); or 1467 ¡ª features of the development measures and assessment process (process argument); or 1468 ¡ª factors impacting the residual risk associated with the AI system (e.g. potential causes of insufficiencies 
1469 and failure modes). 1470 The assurance argument can also include a combination of the above perspectives. 1471 EXAMPLE 1 Process focused aspects of the assurance argument for the AI system can include an argument for the 
1472 appropriate tailoring of the AI safety life cycle and the effectiveness with which each activity has been performed, based 
1473 on an evaluation of the work products developed in each phase. 1474 EXAMPLE 2 A risk-oriented assurance structure can include an argument that all possible causes of contributing AI 1475 errors are identified (e.g. via safety analyses), and suitable countermeasures for each cause have been identified and 1476 implemented, either through specific development measures or dedicated architectural measures. 
1477 The assurance argument for the AI system begins with a claim that the safety requirements allocated to the AI 1478 system are achieved. This can include statements related to a reasonable level of residual AI errors with 1479 respect to the AI safety requirements and the target functionality of the AI system. 
1480 An explicit definition of the context, as well as relevant assumptions, increases the transparency of the 1481 assurance argument and limits the scope of the argument to the specific AI system, its technical context within 1482 the encompassing system and its operating conditions. 
1483 EXAMPLE 3 Examples of context information that can be referenced by the assurance argument include: 1484 ¡ª definition of the technical system context of the encompassing system; 1485 ¡ª definition of the set of environmental conditions and operating context for which the assurance argument is valid; 1486 ¡ª potential causes of contributing AI errors considered as part of the assurance argument. 1487 EXAMPLE 4 Examples of assumptions that might be discharged as separate arguments can include: 1488 ¡ª assumptions on the usage and operational profile of the AI system; 1489 ¡ª assumptions on the reliability of inputs to the AI system; 1490 ¡ª assumptions on the fundamental performance potential of the chosen AI technology. 1491 An example of an assurance argument for an AI-based vehicle function structured according to a strategy that 
1492 addresses possible sources of insufficiencies can be found in Annex B. 1493 8.5.2 Categories of evidence 
1494 The following categories of evidence in the form of work products created during the AI safety life cycle can 1495 be considered for use within the assurance argument. 1496 a) Addressing insufficiencies in the specification of the AI safety requirements: 
1497 1498 1499  ¡ª 
Evaluation of the completeness of the definition of the environmental conditions and operating context (input space).This is used to confirm completeness requirements on training and test datasets (see Clause 9).  
1500 1501 1502 1503  ¡ª 
Evaluation of the validity of the AI safety requirements derived from the safety requirements allocated to the AI system (from external sources). This includes traceability tosafety requirements allocated to the AI system and a review of the completeness and consistency of the safety-related properties used to define the AI safety requirements (see Clause 9).  
1504  b)  Addressing performance insufficiencies in the design of the AI system:  
1505 1506 1507 1508  ¡ª 
Justification for the selection of the chosen AI methods, AI technologies and AI system architecture. This can include references to performance benchmarks and analysis indicative of the fundamental potential of the chosen technology and AI system architecture to meet the safety requirements (see Clause 10).  
1509 1510 1511  ¡ª 
Evaluation of the effectiveness of architectural and development measures. This can include an evaluation of the ability of architectural and development measures (see Clause 10) to limit the impact of contributing AI errors in the AI model.  
1512 1513 1514 1515 1516  NOTE 1 These measures can include hyperparameter optimization (development measure) as well as monitoring components (architectural measure) that detect inconsistencies in the outputs and trigger a dedicated AI error reaction to ensure AI safety. This can include components that ensure a continuous availability of the functionality through redundancy and voting, and dynamic adaptation of vehicle behaviour based on the evaluated performance of the AI system.  
1517 1518 1519  ¡ª 
Evaluation of robustness against hardware and software faults during execution. This can include an evaluation of the impact of random hardware faults and systematic design faults (including software) on AI safety (see ISO 26262-5:2018[9], ISO 26262-6:2018 [10]).  
1520 1521  ¡ª Evaluation of the impact of differences between the development and the target execution environment.  
1522 1523 1524  NOTE 2 This supports the argument that AI safety is achieved (see Clause 10), under the condition that some parts of the evaluation were performed within a development environment, e.g. software-in-the-loop tests, or using synthetic data (see Clause 12).  
1525 1526 1527  c)  Suitability of AI training and AI test datasets. This can include an evaluation of the suitability of AI training and AI test datasets to achieve and demonstrate that the safety requirements have been fulfilled (see Clause 11).  
1528  NOTE 3  This includes evidence of the independence of the AI test datasets from AI training datasets.  
1529 1530 1531 1532 1533 1534  d)  Evaluation of the fulfilment of the safety requirements. This demonstrates the extent to which the safety requirements are fulfilled and, where necessary, provides rationale (e.g. risk analysis) on the requirements that are not fulfilled, or where such fulfilment cannot be demonstrated. This can include a quantitative evaluation of functional insufficiencies in the AI system with respect to target metrics and safety-related properties used to define the requirements (see Clause 9). Approaches to collect this category of evidence can make use of real, synthetic and/or hybrid datasets (see Clause 12).  
1535 1536 1537 1538  e)  Evaluation of the impact of AI errors. This can include an evaluation of specific properties of the AI system that can lead to AI errors and consequently hazardous behaviour of the system. This can be based on targeted testing and analysis approaches to evaluate the presence and magnitude of known causes of AI errors such as insufficient generalization capability and insufficient robustness. This evaluation is made  

1539 based on an analysis of potential causes of insufficiencies and AI errors in the AI system (see Clause 13) 1540 and includes a definition of a set of suitable measures to address the AI errors. 
1541 f) Addressing AI errors during operation: 
1542 ¡ª Identification and analysis of previously undiscovered AI errors. This includes the continual 1543 evaluation of the behaviour of the AI system during operation (see Clause 14). 
1544 ¡ª AI errors discovered during operation are analysed to understand their criticality, and a set of 1545 mitigation measures are identified, including a repetition of relevant phases of the AI safety life 1546 cycle. 
1547 ¡ª Re-evaluation of robustness against changes in the operating conditions over time (distributional 1548 shift). This supports the argument that the AI system maintains its safety-related properties 1549 despite reasonably expected changes in its deployment environment. 
1550 EXAMPLE An analysis of the resilience of the AI system to shifts in the distribution of its inputs or the effectiveness 1551 of architectural measures to detect out of training/test distribution conditions (see Clause 14). 




1552 8.6 The role of quantitative targets and qualitative arguments 
1553 Safety requirements allocated to the AI system (from external sources) can include quantitative risk 1554 acceptance criteria and validation targets (see ISO 21448:2022, clause 6[1]). 
1555 These quantitative targets are considered during the derivation of AI safety requirements and are used to 1556 define target metrics for the safety-related properties (see Clause 9). 
1557 A direct mapping between quantitative targets (e.g. accident rates) of the safety requirements allocated to the 1558 AI system and the safety-related properties of the AI system (e.g. robustness to small changes in inputs) might 1559 not be possible. 
1560 Safety analyses (see Clause 13) that evaluate the impact and potential causes of AI errors in the AI system can 1561 provide a qualitative argument that the residual risk of violation of quantitative targets defined in safety 1562 requirements allocated to the AI system is acceptably low. 
1563 A demonstration of the correlation between causes of AI errors, the safety-related properties and the 1564 fulfilment of the AI safety requirements increases the confidence in the effectiveness of the safety analysis and 1565 thereby the associated assurance arguments and evidence. 
1566 EXAMPLE 1 The safety analysis hypothesises that an inability to generalise on inputs outside of the training 1567 distribution leads to an unacceptably high rate of AI errors under certain conditions. An out-of-distribution detection as 1568 a post-processing function is therefore proposed as an architectural measure. To argue the effectiveness of this measure, 1569 the assurance argument demonstrates both the achieved coverage of out-of-distribution inputs as well as the actual 1570 contribution of out-of-distribution inputs to the overall contributing AI error rate of the AI system. Thus, both the 1571 effectiveness and the appropriateness of the out-of-distribution detection as a safety measure are argued. 
1572 To ensure that evidence referenced by the assurance argument provides sufficient confidence in the fulfilment 1573 of safety requirements, the following assumptions can be supported with dedicated assurance arguments and 1574 additional evidence: 
1575 a) The measurement targets are an adequate proxy for measuring the achievement of the safety 1576 requirements. There is a demonstrable correlation between the collected evidence, measurement targets 1577 of safety-related properties, and risk acceptance criteria associated with the safety requirements 1578 allocated to the AI system. 
1579 b) The approach to measuring the achievement of the target values of the AI safety requirements is 1580 appropriate. This includes assurance arguments for the applicability of methods used; and how 1581 representative and indicative the datasets are that are used to collect evidence. In particular: 
ISO/CD PAS 8800:2024(en)  
1582  1) The datasets (e.g. test inputs) are representative of the input space.  
1583  2) The datasets used to collect evidence are sufficient to detect critical classes of AI errors in the AI  
1584  system, e.g. by covering known edge cases and triggering conditions.  
1585  3) The datasets used to collect evidence provide a representative indication of the actual AI error rate  
1586  for all inputs satisfying the system assumption, including in the presence of unknown triggering  
1587  conditions. This includes an evaluation of the statistical confidence of performance evaluations and  
1588  overall coverage of the input space.  
1589  EXAMPLE 2 A method for obtaining a reliable target measurement for computer vision classification tasks based on  
1590  a single image might not necessarily be applicable to object detection tasks involving the processing of real-time video  
1591  streams.  
1592  
1593  8.7 Evaluation of the assurance argument  
1594  Confirmation measures according to 7.3.4 as well as methods and criteria for evaluating SOTIF according to  
1595  ISO 21448:2022, 12.3 [1] can be used to evaluate the achievement of AI safety on the basis of the assurance  
1596  argument and associated evidence. The confirmation of the assurance argument for the AI system can be used  
1597  as a precondition for the recommendation for SOTIF release at the level of the encompassing system (see ISO  
1598  21448:2022, 12.4[1]).  
1599  In case of "conditional acceptance", the conditions required for a final release of the system can be documented  
1600  in the assurance argument (see ISO 21448:2022, 12.4 [1]).  
1601  EXAMPLE 1 Restricted usage within the operational environment can be required to confirm assumptions regarding  
1602  the distribution of triggering conditions. Once sufficient evidence has been gathered to support these assumptions, a final  
1603  release can be accepted.  
1604  Sources of potential uncertainty in the assurance argument can be used to structure the evaluation procedure  
1605  and to identify potential for strengthening the argument. This includes the identification of defeaters which  
1606  might contradict assertions within the argument [11]. The following types of assertions can be identified for  
1607  scrutiny within the assurance argument:  
1608  ¡ª Asserted context: These assertions are associated with the contextual information and assumptions that  
1609  are used to scope the claims within the argument. If these assertions cannot be demonstrated to be valid,  
1610  then the conditions under which the assurance argument is valid will be restricted.  
1611  EXAMPLE 2 Changes in the operational environment in which the AI system is deployed can undermine the  
1612  assumptions made on the input space of the AI system, thus undermining the validity of the assurance argument.  
1613  EXAMPLE 3 An incomplete documentation or understanding of safety requirements allocated to the AI system will  
1614  undermine the validity of the statements made within the assurance argument.  
1615  ¡ª Asserted evidence: These assertions relate to the evidence used to support claims in the assurance  
1616  argument being both appropriate and trustworthy. This relates to individual pieces of evidence as well as  
1617  combinations of evidence that are used to support a specific claim. If these assertions related to evidence  
1618  cannot be argued with sufficient confidence, then the veracity of the claim can no longer be asserted.  
1619  EXAMPLE 4 Tests based on samples taken from within a test dataset are used to provide evidence for the robustness  
1620  of the AI system against rare events (edge cases). However, a sufficient number of data points representing such events  
1621  is not available, resulting in test results which are insufficient to demonstrate that the robustness claim has been fulfilled  
1622  within a given statistical confidence interval. Therefore, additional or alternative forms of evidence are required.  
1623  EXAMPLE 5 Synthetic test data can be used to generate a sufficiently large and diverse number of edge cases that are  
1624  not commonly found in samples taken directly from the operating environment. Additional assurance arguments can  

1625  ensure the appropriateness of the testing approach to support the claim based on a validation of the fidelity of the  
1626  synthetically generated data in comparison to the target environment and the inclusion of real-world samples in the test  
1627  set.  
1628  EXAMPLE 6 When tools are used to produce evidence, the level of confidence in the evidence is directly linked to the  
1629  confidence in the usage of the software tools. This is achieved by applying the requirements from Clause 15. Work  
1630  products from Clause 15 can be used in the assertion of the validity and integrity of evidence in the assurance argument.  
1631  ¡ª Asserted inference: These assertions relate to the reasoning behind the structuring of the assurance  
1632  argument itself. In particular, how top-level claims are iteratively refined into detailed sub-claims that can  
1633  be directly supported by evidence.  
1634  EXAMPLE 7 The set of causes of AI errors in the AI system used to structure a risk-based assurance argument  
1635  overlook critical exacerbating factors (e.g. variation in sensor positioning and calibration) resulting in an assurance  
1636  argument that demonstrates a set of properties that are not sufficient to ensure all safety requirements allocated to the  
1637  AI system are met.  
1638  NOTE The use of assurance claim points [6],[11] can be used to elaborate those assertions within a GSN  
1639  argument that require additional confidence arguments.  
1640  8.8 Work products  
1641  8.8.1 Safety assurance argument, resulting from8.3.1and 8.3.2.  
1642  8.8.2 Confirmation measure reports, resulting from8.3.3.  
1643  9 Derivationof AI safetyrequirements  
1644  9.1 Objectives  
1645  The objectives of this clause are:  
1646  a) to specify a complete and consistent set of AI safety requirements, that are sufficient to ensure AI safety;  
1647  b) to refine AI safety requirements based on learnings from development, verification and validation;  
1648  c) to specify the limitations of an AI system over its inputspace to be escalated to itsencompassing system  
1649  development process.  
1650  9.2 Prerequisites and supporting information  
1651  The following information shall be available at the initiation of this activity:  
1652  a) AI system definition (from external sources), including:  
1653  1) safety requirements allocated to the AI system;  
1654  2) input space definition;  
1655  3) functional requirements;  
1656  4) impacted stakeholders;  
1657  5) the interfaces of the AI system with the encompassing system, including if applicable, the ASIL  
1658  capability of the inputs to the AI system;  
1659  6) interfaces to the environment, if applicable.  

1660  NOTE Safety requirements allocated to the AI system are allocated from the encompassing system development  
1661  process. They can be motivated by different aspects, e.g. functional safety, SOTIF, or indirectly due to security (e.g.  
1662  robustness against adversarial attacks and data poisoning). In this document, safety requirements are not explicitly  
1663  distinguished by these aspects.  
1664  The following information can be considered during further iterations of this activity:  
1665  b) safety analysis report, from Clause 13;  
1666  c) evaluation report of functional insufficiencies detected during operation, from Clause 14;  
1667  d) measures for ensuring AI safety during operation, from Clause 14.  
1668  9.3 General requirements  
1669  9.3.1 The input spacedefinition of the AI system shall be refined to the degree suitable for initiating the AI  
1670  safety lifecycle.  
1671  9.3.2 To provide a connection between each AI safety requirement and the addressed problem, the refined  
1672  AI safety requirements shall either:  
1673  a) trace to the safety requirements allocated to the AI system (from external sources), assumptions or critical  
1674  scenarios; or  
1675  b) address and trace to the potential influencing factors or root causes of functional insufficiencies and  
1676  triggering conditions.  
1677  9.3.3 A justification shall be provided that the refined AI safety requirements are reasonable to either ensure  
1678  the achievement of the safety requirements allocated to the AI system (from external sources), or prevent or  
1679  mitigate the functional insufficiencies at the AI system level.  
1680  NOTE Refined AI safety requirements to address functional insufficiencies at the AI system level are identified by  
1681  safety analysis as necessary to fulfil the safety requirements allocated to the AI system (from external sources).  
1682  9.3.4 To argue for the absence of unreasonable risk due to random hardware faults and classic systematic  
1683  faults, the requirements of the ISO 26262 series shall be fulfilled.  
1684  NOTE 1 Combining 9.3.1 to 9.3.4, all causes defined in Figure 6¨C12, i.e. functional insufficiency (9.3.1to 9.3.3) and  
1685  contributing systematic faults and contributing random hardware faults (9.3.4), can be addressed.  
1686  NOTE 2 Some adaptions can be necessary, in particular for safety requirements motivated by ISO 21448 activities  
1687  with no ASIL rating and since the target is to achieve AI safety and not only functional safety.  
1688  
1689  9.3.5 The following cases shall be identified and reported to the encompassing system development  
1690  process:  
1691  a) the AI system does not fully comply with the AI safety requirements;  
1692  b) the AI safety requirements are only fulfilled for a limited part of the input space.  
1693  9.3.6 AI safety requirements shall be identified to support the measures ensuring AI safety during operation.  
1694  EXAMPLE Different goals of field observation can be addressed with requirement 9.3.6:  

1695  a) monitoring of the uncertainty in the current situation to indicate this to the encompassing systems allowing for  
1696  adjustments of the driving mission tactical control of the vehicle;  
1697  b) monitoring of performance and detection of failure to support mitigation measures internal to the AI system (also  
1698  referring to Clause 10.5);  
1699  c) supporting the continuous improvement of the AI system to ensure AI safety (e.g.recording devices toidentify  
1700  inconsistency among different sensor modalities and collect data for training and updating AI models);  
1701  9.4 General workflow for deriving safety requirements  
1702  Figure 9¨C1 explains general requirements in Clause 9.3 for deriving AI safety requirements and establishes  
1703  their connections to the objectives.  
1704  ¡ª The safety requirements allocated to the AI system (from external sources)are part of the AI system  
1705  definition provided by the encompassing system development process. These safety requirements have  
1706  SOTIF and functional safety aspects and are refinedinto an initial set of AI safety requirements. SOTIF  
1707  requirements are typically identified as allowed maximum error rates while being exposed to the input  
1708  space.  
1709  NOTE 1 "Safety requirements allocated to the AI system (from external sources)" are not work products in this  
1710  document. "AI safety requirements" refersto all requirements derived within the scope of this document.  
1711  ¡ª Refined AI safety requirements (quantitative or qualitative) will be derived either by referencing  
1712  requirements from past products, or by utilising safety-related properties of AI systems that can be  
1713  relevant to the application. These requirements will control the uncertainty in the development process  
1714  of the AI system in order to achieve the development quality of AI models and in addressing the safety- 
1715  related issues in the AI system in order to achieve SOTIF. As the work product 9.6.2AI safety requirements,  
1716  these requirements are distributed to further development tasks in different phases of AI safety lifecycle  
1717  as described in other clauses (see Clauses 9.5.1 and 9.5.2).  
1718  NOTE 2 Organizations define criteria to evaluate the uncertainty in the AI development process, i.e. rigour in training,  
1719  evaluation, data collection, labelling, etc., to achieve the integrity of AI models, the safety-related errors in the AI system,  
1720  and their overall impact to the encompassing system's safety. These criteria capture the organizational acceptance of the  
1721  uncertainty in the AI development process and the safety-related errors in the AI system. Thus, it guides requirements  
1722  elicitation, development tasks and decisions.  
1723  NOTE 3 Derived AI safety requirements include SOTIF and functional safety aspects and are allocated to AI  
1724  components (See Clause 10), AI systems, encompassing systems, systems, vehicles, mobility services, etc. SOTIF  
1725  requirements do not have ASIL values. Only functional safety requirements have ASIL values, including QM, and comply  
1726  with the applicable parts of the ISO 26262 series.  
1727  NOTE 4 Derived AI safety requirements can be allocated to the AI system or the AI system and further to AI  
1728  components and tested at the respective level. In particular, the requirements allocated to the AI components, with  
1729  appropriate safety metrics and test targets, are derived from the requirements allocated to the AI system. The derivation  
1730  of the requirements allocated to AI components considers the complexity of the involved AI models, their task, and the  
1731  environment they operate in.  
1732  EXAMPLE 1 The AI safety requirements allocated to an AI system for automatic braking, including test targets, can be  
1733  derived from the vehicle-level behaviour related to vehicle speed, distance to an obstacle, etc. If the AI system is  
1734  composed of multiple AI components, the AI system decomposition needs to be accounted for in the derivation of the  
1735  requirements allocated to the AI components. Examples of such decompositions are parallel AI model ensembles, parallel  
1736  heads in multi-task architectures, and sequential components in multi-stage object detectors.  
1737  ¡ª The input space definition, e.g. ODD in the automated driving context,in the prerequisite 9.2, list item a)AI  
1738  system definition (from external sources) is provided by the encompassing system development process.  

ISO/CD PAS 8800:2024(en)  
1739  The definition can require further refinement to be used in AI safety lifecycle, as described in Clause 9.5.3  
1740  and distributed as the work product 9.6.1input space definition (refined).  
1741  ¡ª For functional safety requirements with ASILs (including QM), compliance to the ISO 26262 series can be  
1742  demonstrated. For SOTIF requirements, the occurrence of particular error (sequence) patterns might be  
1743  too high with respect to the criteria used to evaluate the uncertainty of the AI system during the  
1744  engineering of the AI system or during operation. Refined AI safety requirements can be derived to inhibit  
1745  the error (sequence) patterns produced by the AI system by understanding the triggering conditions or  
1746  the safety-related errors. When further performance improvements are unlikely, performance limitations  
1747  and relevant triggering conditions can be reported to the encompassing system development process. This  
1748  is described in Clause9.5.4.  
1749  ¡ª 
Safety analysis (from Clause 13) or observations from the field (from Clause 14) can be used to determine  
1750  the required thresholds for particular error (sequence) patterns.  
1751  ¡ª 
The activities described in Clause 13 either evaluate the residual risk of the AI system with respect to the  
1752  AI safety requirements, or identify the safety-related errors in the AI system which can lead to the violation  
1753  of the safety requirements allocated to the AI system (from external sources). The work products resulting  
1754  from Clause 13 activities will be used as an input for a subsequent iteration of Clause 9 activities.  
1755  EXAMPLE 2 A lane detection function produces AI errors during night-time and heavy snow. Continued operation  
1756  under these conditions case could lead to the violation of AI safety requirements. This information can be communicated  
1757  to the encompassing system development, so that measures can be taken at the system or vehicle level to restrict the  
1758  conditions of operation or otherwise mitigate against AI errors under these conditions.  
1759  ¡ª During the operation phase, field monitoring can be used to detect unknown triggering conditions and  
1760  violations of assumptions.  


1762  NOTE  For ease of understanding, the following activities are omitted in the diagram:  
1763  a)  reviewing the selection ofsafety-related properties of AI systems based on safety analysis in an iterative manner;  
1764  b)  refinement of input space definition based upon e.g. safety-related errors or component-level triggering conditions;  
1765  c)  reporting to the encompassing systemdevelopment process forthe technical limitations of the AI system to ensure  
1766  the AI safety requirements and the safety requirements on the encompassing systems and the vehicle reach  
1767  consistency.  
1768  Figure 9¨C1 ¡ª Conceptual diagram reflecting major activities in derivation of AI safety requirements  
1769  9.5 Deriving AI safety requirements on supervised machine learning  
1770  9.5.1  The need for refinedAI safety requirements  
1771  Based  on  the ISO 21448 (SOTIF) activities at the vehicle level to identify and evaluate risks, potential  
1772  functional insufficiencies, and potential triggering conditions, safety requirements allocated to the AI system  
1773  (from external sources) are refined into AI safety requirements.  
1774  For AI systems operating within a specific input space, AI safety requirements can contain acceptance criteria  
1775  similar to the concept of probability of failure on demand (PFD)[12],e.g. ¡°Probability(occurrence of anerror 
 
1776  pattern) < ¦Á¡±.  

1777  EXAMPLE 1 AnAI error(sequence)patternofaDNN¡°Consecutivemisdetection(FalseNegatives, FNs)of anearby  
1778  pedestrian formore than 0. 1 seconds¡± canbe refined into ¡°The occurrence of more thanthree consecutive FNs ofa  
1779  pedestrian@30FPS¡± ifthe DNNistriggeredat arateof 30 FPS. Such anAI error(sequence)patternwith FNscanbe  
1780  verifiable only if the ground truth is available.  
1781  Methods such as Systems-Theoretic Process Analysis (STPA) can be used to refine AI safety requirements  
1782  where a detailed design is available (see Annex E), i.e. low-level control structure in STPA terms. However,  
1783  specific types of AI models, e.g. those implemented by supervised learning, cannot be decomposed further.  
1784  This clause focuses on the case that the AI model cannot be decomposed.  
1785  EXAMPLE 2 An example detailed design in an AI system can be:  
1786  ¡ª redundant DNNs for perception task;  
1787  ¡ª a majority voting on the individual results;  
1788  ¡ª an out-of-distribution (OOD) detector to abort the validity of the majority voting upon detection of an out-of- 
1789  distribution input.  
1790  Based on such a detailed design, refinedAI safety requirements can be derived to address unsafe conditions, e.g. OOD  
1791  detector provides an incorrect output in rainy situations.  
1792  For machine learning applications, the probability of an error in an AI system may not be computable due to  
1793  the complexity of its input space, e.g. pedestrian detection for a wide variety of pedestrians in automated  
1794  driving, and the inherent nature of machine learning, i.e. in-distribution and out-of-distribution error gap. The  
1795  relative frequency of an error in an AI system often can be estimated only if enough samples are observed in  
1796  its input space. The inaccuracy of relative frequency, e.g. PFD, is the uncertainty of an AI system that refined  
1797  AI safety requirements can control in the AI development process. Requirements on a sufficiently low level of  
1798  probability of AI errors are refined into quantitative requirements and qualitative requirements. Figure 9¨C2  
1799  illustrates the underlying approach.  
1800  EXAMPLE 3 Asafety requirement allocated toanAI system(from externalsources)¡°Probability (occurrenceoftwo  
1801  consecutive FNsof a nearby pedestrian per hour of operation) < 10.6¡±canberefined intoanAI safety requirement¡°The  
1802  DNN does not produce two consecutive FNs of a nearby pedestrian in 108 hours of driving data with sufficient diversity  
1803  demonstrated¡± undertheconditionthevalidity of therefinementis provided.Itisassumed that moredriving hourscan  
1804  be used to prove the hypothesis that the requirement holds.  
1805  NOTE 1 Relative frequency, empirical probability or experimental probability of an event is the ratio of the number of  
1806  outcomes in which a specified event occurs in the total number of trials [13].  
1807  NOTE 2 For estimating very low probability using relative frequency, the denominator, i.e., the number of samples to  
1808  be tested, can be large. Sample size determination for estimating the probability of occurrence of a particular error  
1809  pattern can be based on estimation principles and the statistical confidence of experimental results.  


1811 Figure 9¨C2 ¡ª Understanding refinement of AI safety requirements for supervised learning 
ISO/CD PAS 8800:2024(en)  
1812  9.5.2  Derivation of refined AI safety requirements to manage uncertainty  
1813  For engineering machine learning components, risks can be reduced by managing sources of uncertainty in  
1814  the  AI  engineering  process.  Uncertainties  exist  for  each  phase  of  the  AI  engineering  process,  which  
1815  arereferredto here  as  potential influencing factors.  Influencing factors fall into the following categories:  
1816  Observation, Label, Model, and Operation. Table 9¨C1describes typical approaches to managing influencing  
1817  factors in the AI engineering process, which constructs an abstracted framework to reduce the uncertainty  
1818  regarding limited knowledge about the true state of the world.  
1819  NOTE 1  Such limited knowledge is expected in any data-driven AI developments. Perceptual uncertainty refers to  
1820  uncertainty associated with the performance of perceptual components[14].  
1821  Table 9¨C1 ¡ª Managing influencing factors in the AI engineering process  

Influencing factor class  Uncertainty management approach  
Observation  (1) Enumerate patterns influencing AI outputs both in deductive (top-down) and in inductive (bottom-up) ways (2) Define data coverage policy for comprehensive AI performance and safety (harm avoidance) trade-off  
Label  Reduce variation of labelling policy and enhance/ensure labelling work quality  
Model  Use appropriate model selection and de facto standard approaches for tuning and evaluation  
Operation  Detect deviation in model performance between development and deployment  

1822  Addressing the above influencing factors, Table 9¨C2describes a non-exhaustive set of example generic AI  
1823  safety requirements that may be considered for specific applications.  
1824  Table 9¨C2 ¡ª Example generic AI safety requirements to manage influencing factors  

Influencing factor class  Example generic AI safety requirements  
Observation  ¡ª Derive scenes relevant to attributes of inference targets, environmental conditions, and system configurations (relevance) ¡ª Include scenes which would potentially lead to errors (e.g., false negative, false positive, substantially incorrect regression) ¡ª Consider the effects of system configuration change during operation (e.g., sensor mounting position shift, sensor hardware degradation caused by ageing, and suitable sensor parameter change due to it ) ¡ª Include specific scenes which empirically have led to errors during development of the current product and development and operation of legacy products (safety) ¡ª Specify data quantity for each scene or combination of scenes (diversity) ¡ª Explain the rationale for the scenes for which one cannot define target data quantity  
Label  ¡ª Specify inference targets ¡ª Describe the necessary quality of labels, e.g. false positive rate, bounding box size precision, etc.  

Influencing factor class  Example generic AI safety requirements  
¡ª Specify labelling procedure ¡ª Define the appropriate type of label e.g. class, numerical etc. ¡ª Publish and circulate a labelling policy guideline, e.g. treatment of occluded objects, number of annotators annotating the same data etc., to all relevant stakeholders ¡ª Provide clear criteria and lines of accountability about the labelling of data involving protected characteristics or special category data (or both) ¡ª Specify label evaluation policy, i.e., evaluation timing, evaluation procedure, evaluation measures, and acceptance criteria ¡ª Make sure appropriate processes are in place if crowdsourced labellers are used ¡ª Assess the risk of incorrect labelling through sample checks of submitted labels ¡ª Specify label incident reporting and management process to handle labelling errors and ambiguity in labelling policy  
Model  ¡ª 
Specify metrics and acceptance criteria for model performance ¡ª 
Specify model selection policy and ensure an appropriate model is selected according to the policy ¡ª 
Specify hyperparameter search policy and conditions, e.g., bounds, and document the determined hyperparameter values ¡ª 
Perform safety analysis of the AI model ¡ª 
Introduce the technical capability of checking the plausibility of the output ¡ª 
Introduce the technical capability of de-noising/quantizing the input ¡ª 
Introduce the technical capability of robust training techniques (e.g., training against noise, quantized neural networks) ¡ª 
Introduce the technical capability of providing interpretation (explainability) over the decision being made ¡ª 
For DNN, pre-hoc global explainability (i.e., extracting the decision-making mechanisms into human-comprehensible rules) can be difficult. Post-hoc local explainability (i.e., the reason why a particular decision is made for a specific input) is likely, via techniques such as local linearization or heatmaps [15] ¡ª 
Introduce the technical capability to measure epistemic uncertainty for the AI model outputs  

Influencing factor class  Example generic AI safety requirements  
Operation  ¡ª Introduce the technical capability of identifying situations matching a known triggering condition or measuring model uncertainty ¡ª Introduce the technical capability of identifying concept/domain drift ¡ª Introduce the technical capability of identifying implausible or otherwise non-trustworthy AI system behaviour  

1825  While AI safety requirements derived using influencing factors are largely qualitative, how safety-related  
1826  properties of AI systems can be used to assist in derivingrefinedquantitativeAI safety requirements is  
1827  considered in the following.  
1828  In this document, we establish a differentiation between safety-related properties and AI safety requirements.  
1829  ¡ª Safety-related properties are inherent characteristics of engineering AI systems which may either lead to  
1830  the insufficiency of the generalization or merely make it difficult to argue the safety of the AI system.  
1831  Safety-related properties are a subset of generic AI properties; they are application-independent and may  
1832  include aspects such as robustness or domain shift, incomplete specification as suggested by ISO/IEC  
1833  22989 and related guidance on safety in ISO/IEC DTR 5469. See Annex Dfor a list of AI properties that may  
1834  be safety-related.  
1835  ¡ª AI safety requirements are application-dependent and can be mapped to one or more safety-related  
1836  properties.  
1837  NOTE 2 The term safety-related property used in this document is stated differently in various project contexts with  
1838  similar meanings. Within the French national project DEEL, it is referred to as high-level properties[16], while in the  
1839  German national project KI-Absicherung (EN: AI-assurance), it is referred as safety concerns[17].  
1840  The list of safety-related properties of AI systems from Annex D can be used to refine AI safety requirements.  
1841  The safety-related properties of AI systems considered relevant for the AI system under consideration are  
1842  identified and instantiatedinto refined AI safety requirements. These requirements can be associated with the  
1843  safety-related properties of AI systems to support the selection of AI technologies, and architectural and  
1844  development measures, as detailed in Clause 10.  
1845  Deductive safety analysis approaches such as FTA might not directly lead to the identification of safety-related  
1846  properties and associated AI safety requirements. This is because a causal analysis of AI errors might have  
1847  limited effectiveness due to the complexity of the model and interaction of numerous, potentially unknown  
1848  causes. Instead, inductive approaches such as hypothesis testing are used to derive the safety requirements.  
1849  First, an initial set of AI safety requirements is identified, and then AI safety requirements are updated and  
1850  validated through safety analysis.  
1851  NOTE 3 These safety-related properties of AI systems are currently described conceptually rather than using formal  
1852  definitions in mathematics. The list of safety-related properties on AI systems, e.g., Annex D, is not exhaustive. The list is  
1853  based on discussions in AI safety research, etc.  
1854  Refined AI safety requirements can be either qualitative or quantitative, where for quantitative AI safety  
1855  requirements, the design of the acceptance thresholds is application dependent. The validity of these  
1856  thresholds requires justificationwith respect to the evaluation criteria and the overall residual risk acceptable  
1857  for the AI system. For the example of robustness, the validation means ¡°Cansettingsuch a threshold positively  
1858  increase the robustness?¡±, which is an activity to be evaluated via experiments within Clause 12.5.7.  
1859  EXAMPLE Appropriate safety-related properties are hypothesized at the early stage of development, e.g., based on  
1860  past product experiences and identified by safety analysis in an iterative manner to ensure their contribution to the  
1861  system¡®ssafety. Safetyanalysis canincludetheimpact ofchangingsafety-related properties, specific noise robustness for  
1862  theequipped hardwarecharacteristics, etc., onthesystem¡¯ssafety. Forexample, asafety-related property of AI systems  
1863  AI robustness is identified by such safety analysis and further refined to different kinds of robustness, then the following  

1864  quantitativeAI safety requirement canbederived along with justificationforthevalidity of thesethresholds:¡°Forall  
1865  clear images in the input space, if noise perturbations characterized by ..1 norm < 0.001 are added on the image, the AI  
1866  system should at most introduce 0.01% of new errors¡±. This requirement, derived from ¡°AI robustness,¡± will lead to  
1867  additional engineering efforts in Clause 10, such as using robust training techniques.  
1868  Using safety-related properties of AI systems to derive refined AI requirements is an inductive approach, and  
1869  exhaustiveness is not ensured. Therefore, it can be only a complementary part of the verification and  
1870  validation strategy elaborated in ISO 21448 (SOTIF) Clause 9.  
1871  9.5.3 Refinement of the input space definition for AI safety lifecycle  
1872  The input space defines what workspace, what conditions, and around what dynamic elements a system will  
1873  operate with the purpose to ensure its safe design and safe operation. If a system depends on AI systems using  
1874  data-driven methods to build machine learning models, the input space leads to the definition of the dataset  
1875  requirements, which form the basis of the data used throughout the lifecycle of the system.  
1876  EXAMPLE For autonomous driving, the workspace can be the road or area where the system operates. The  
1877  conditions can be the weather, visibility conditions, illumination, and connectivity. The dynamic elements can be  
1878  surrounding traffic and moving debris.  
1879  The refinement of the input space is an iterative process. The initial definition provided as a prerequisite may  
1880  not be suitable to be used for ensuring the intended functionality of the AI system. This is also reflected in the  
1881  ISO 21448 document, emphasizing that environmental factors are essential issues, while systems and their  
1882  elements have different concerns depending on the hierarchical layers.  
1883  The refinement of the input space definition aims at complying with the capabilities of the available systems  
1884  (e.g., sensors). Its refinement may also consider newly discovered or known triggering conditions that may  
1885  lead to performance insufficiencies. Regarding where and what kind of refinement of the definition of the input  
1886  space is needed, the information can be derived from the results of the safety analyses as described in Clause  
1887  13.  
1888  The refined input spacedefinition forms the basis for the verification and testing of the target system  
1889  performance. That is, the refined input space bounds the performance expectations of the AI system, reducing  
1890  the uncertainty associated to its operation and consequently ensuring the bounds of its safe operation.  
1891  9.5.4 Restricting the occurrence of AI output insufficiencies  
1892  Overall, during the engineering of an AI system, a particular scenario (or a set of scenarios) where the error  
1893  rate of the AI system is too high may be observed, violating the initially derived AI safety requirements. The  
1894  predefined list of safety-related properties of AI systems can be used again as a checklist to examine the  
1895  potential root causes and subsequently, derive a new set of AI safety requirements with associated metrics. In  
1896  the iterations beyond the initial iteration, the safety analysis described in Clause 13 provides additional input  
1897  to this activity. Based on observations from the field or from V&V activities, the safety analyses identify the  
1898  safety-related properties which maximally correlate with the safety-related issue. The insights into the  
1899  correlation (or if applicable causality) between output insufficiencies and safety-related issue can be used as  
1900  a basis for the refinement of the AI safety requirements after the initial iteration.  
1901  NOTE 1 Currently these safety-related properties are described conceptually rather than using formal definitions. The  
1902  consequence is that the derivation of AI safety requirements is not viewed as a "(logical) causal derivation" but rather a  
1903  hypothesis to be validated.  
1904  NOTE 2 The vast majority of ML-type statistical models estimate the likelihood of events, which is inferred from the  
1905  correlations in the data. In such cases clear causal relationships between the inputs and the outputs of these models (and  
1906  ultimately the actions taken by the system) cannot be established. Thus, the classic causal fault analysis tools are not  
1907  necessarily applicable in this context.  

ISO/CD PAS 8800:2024(en)  
1908  In addition to the analysis results from Clause 13, which are driven by V&V issues or issues observed in the  
1909  field, analytic methods can also support the creation and refinement of AI safetyrequirements, which may  
1910  include plausibility checks on the inputs to the AI system and bounding on the outputs from the AI system.  
1911  As illustrated in Figure 9¨C3, an issue associated with performance insufficiencies may be identified from a too-
 
1912  high error rate during operation. By conducting appropriatelydesigned experiments (e.g., counterfactual  
1913  analysis or hypothesis testing), it may be discovered that the issue is strongly correlated with one or more  
1914  intrinsic AI properties that are safety-related (e.g., robustness), with evidences of correlation empirically  
1915  manifested in some measurable metrics. The activity can be continued to further refine the influencing factors  
1916  as described in Table 9¨C3, thereby deriving reasonable AI safety requirements with the aim to prevent or  
1917  mitigate the performance insufficiencies. The effectiveness of the refinedAI safety requirements as counter- 
1918  measures should still be validated (see Clause 12.5.7).  
1919  NOTE 3  The use of correlation-driven analysis techniques is driven by the practical need as demonstrated in the field  
1920  of supervised machine learning. It does not inhibit the establishment of an assuranceargument for AI, provided that  
1921  additional supporting evidence such as experiments can be provided. This document also does not inhibit logical causal  
1922  analysis for AI as used in establishing the safety case, provided that causality can be rigorously demonstrated.  
1923  NOTE 4  Regarding systematic errors, technical safety requirements allocated to the AI system are achieved by safety  
1924  mechanisms as the countermeasures for systematic errors.  


1926 Figure 9¨C3 ¡ª Conceptual diagram illustrating safety-related properties 1927 Table 9¨C3 ¡ª Influencing factor classes 
Influencing factor class  Description  Examples  
Observation  Influencing factors related to data representing the input space  ¡ª Diversity of training and test data  

Influencing factor class  Description  Examples  
¡ª Coverage of the inference target data domain ¡ª Distribution according to features of the inference target domain  
Label  Influencing factors related to the data labels  ¡ª Incorrect labels ¡ª Inconsistent labels  
Model  Influencing factors related to the ML model itself  ¡ª Choice of ML model ¡ª Choice of hyperparameters ¡ª Training procedure ¡ª Limited computing power/timing/memory, or precision change/constraint during deploying AI systems into target environment, e.g., hardware NOTE The impact of differences due to deployment constraints between the AI model developed in the off-board environment and its on-board implementation cannot be analytically determined due to the complexity of AI models and is left as uncertainty in the AI context.  
Operation  Influencing factors related to changes within the input space during operation  ¡ª For a ML system classifying traffic signs: Introduction of new traffic signs during its operational phase  

1928  Table 9¨C4 illustrates an example of connecting insufficiencies, safety-related properties, and concrete AI safety  
1929  requirements.  
1930  Table 9¨C4 ¡ª Examples for utilizing safety-related properties to derive new safety requirements  

Insufficiencies observed from the field  Associated safety-related properties  Concrete AI safety requirement derived from insufficiencies  Metrics as KPIs  Original error pattern being considered  
Under slight input variations, the object being detected is missing (false negative)  Robustness (inherent in model training, decision boundary)  For any training data, if a Gaussian noise bounded by L-infinity norm < 0.01 is added, the predicted output class should remain  For each image in the training/test data, evaluate the robustness by creating 1000 variations of each image with additional Gaussian noise, and derive  The safety requirements allocated to the AI system (from external  

Insufficiencies  Associated safety- Concrete AI safety  Metrics as KPIs  Original error  
observed from  related properties  requirement  pattern being  
the field  derived from insufficiencies  considered  
the same with a  the number of incorrect  sources)related  
high probability  predictions.  to false negatives  
(e.g., 99.5%)  KPI returns¡°violation/ false¡± if thereexistsan image where the number of incorrect predictions subject to noise is larger than 5.  
When HW faults  Robustness (against  For any training  For each image in the  The safety  
occur where a  random HW faults)  data, if 1% of the  training / test data, evaluate  requirements  
small area of  pixels is arbitrary  the robustness by randomly  allocated to the  
pixels has turned  changed to be  setting 1% of the pixels to  AI system (from  
black, the  completely black,  black, repeat 1000 times,  external  
prediction can be  the predicted  and derive the number of  sources)related  
constantly wrong  output class should  incorrect predictions.  to false negatives  
(false negative)  remain the same with a high probability (e.g., 99.5%)  KPI returns¡°violation/ false¡± if thereexistsan image where the number of incorrect prediction subject to noise is larger than 5.  
NOTE L-infinity norm quantifies noise by taking the maximum absolute value of the noise on every input dimension. For example, if there are three inputs to the DNN and the noise on each input is 0.1, -0.2, and 0.15 respectively, then L-infinity norm = max {|0.1|, |-0.2|, |0.15|} = 0.2. The use of L-infinity norm is only an option; there exists other norms (e.g., L1 or L2) for quantifying the noise.  

1931 9.5.5 Metrics, measurements and threshold design 1932 Quantitative evidence requires the definition of a set of metrics and target thresholds. The metrics are used to 1933 define, characterise and theoretically analyse specific properties of AI systems. The selection and design of 1934 these metrics and associated measurements and target thresholds involves the following considerations: 
1935 ¡ª Metrics and measurement methods for specific properties of AI technologies referenced in the safety 1936 requirements allocated to the AI system (from external sources). 1937 ¡ª Measurements and evaluation of the AI systems with respect to these properties. 1938 ¡ª Analysis of how the AI methods used to develop the AI system impacts themetrics/measurement methods 
1939 chosen for the AI safety requirements. 1940 ¡ª Definition and creation of suitable datasets used to measure each property (see Clause 11). The datasets 
1941 are designed to provide a statistically relevant analysis of the property and be representative of the input 1942 space. 1943 Selection of suitable target thresholds for each metric requires justification. The justification can include: 
1944 ¡ª past product experiences; 1945 ¡ª commonly agreed industry consensus; 1946 ¡ª system analysis; 1947 ¡ª expert judgements; 
1948 ¡ª experiments. 
1949 The decision may also involve multiple factors where trade-offs (e.g., bias-variance trade-offand robustness-1950 accuracy trade-off phenomena in machine learning) are taken into consideration. 
1951 EXAMPLE As an example, when one considers robustness against noise demonstrated by the minimum 1952 perturbation to change to incorrect prediction, the fundamental question is why one uses ..1 (e.g., 0.5) rather than ..2 (e.g., 1953 0.25) as the acceptance criteria. There can be multiple ways of justifying the threshold by providing convincing 1954 arguments, e.g., the robustness should be larger than ..1, as 
1955 ¡ª ..1 is the maximum possible noise communicated from the complete input pipeline, or 
1956 ¡ª ..1 is derived from experiments where experts agree that noise exceeding ..1 is also hard for human to make 1957 reasonable prediction, or 
1958 ¡ª ..1 is the value being used in similarstandards or products. 
1959 9.5.6 Considerations for deriving safety requirements 
1960 While previous sections offer the general principles in deriving refined AI safety requirements via the 1961 assistance of safety-related properties of AI systems, the following is a summary of some practical example 1962 considerations for the derivation of AI safety requirements. 
1963 NOTE This list of considerations is not exhaustive. Nevertheless, together, they address complementary topics that 1964 can be relevant to achieve safety. 
1965 a) Given the lack of guarantees on the generalization ability of ML models, it is important to specify the input 1966 space defining the workspace, conditions, and dynamic elements, thereby limiting the performance 1967 requirements and also the necessary training data to that space. 
1968 b) Results from statistical learning theories are commonly based on idealistic assumptions. These theories 1969 (e.g., Vapnik¨CChervonenkis theory[18]) can nevertheless be used to derive a lower bound on the number 1970 of samples needed to ensure a tight generalization error for a given type of ML model (e.g., support vector 1971 machine). The derived number of samples for model training commonly reflects the best-case scenario 1972 where the data used in training (in-sample) has the same distribution as the data in operation (out-of-1973 sample). 
1974 c) The possibility of relevant foreseeable adversarial attacks, i.e., foreseeable attacks that are judged to be 1975 realistic, and their impact on the overall AI system can be considered. However, cybersecurity has not 1976 been considered in this document. 
1977 d) The side effects of region-specific privacy considerations (e.g., GDPR in EU) can be considered, as they can 1978 indirectly influence the quality of the collected data and impact the resulting model performance. 
1979 e) A practical purpose of improving AI explainability is to ease the engineering of AI systems. For validation 1980 and performance improvement purposes, interfaces to AI components can be specific to improve the 1981 understanding of the AI component response. Such interfaces can allow for introspective approaches for 1982 validation and performance improvement, particularly isolating errors of the black-box machine learning 1983 based AI components. 
1984 f) Toreduce gaps or non-conformities to a particular AI safety requirement(e.g., by improving the related 1985 performance),different performance aspects, which are inherent to the selected AI methods, can become 1986 entangled. If overall efforts cannot be increased to meet either of the conflicting safety requirements, a 1987 trade-off (e.g., between robustness and accuracy) is found. As shown in Figure 9¨C1, such limitationscan 1988 be forwarded to the encompassing system development process so that safety requirements on the AI 1989 system and the encompassing system design are updated, and the entire safety requirements are finally 
1990 satisfied. 1991 g) Results from the Neyman-Pearson approach (Hypothesis testing) can be used to minimize the missed 1992 detection (number of false negatives) under a fixed number of false alarms (number of false positives) by 1993 considering inputs like sample size and signal-to-noise ratio (SNR).[19] 
1994 9.6 Work products 1995 9.6.1 Input space definition (refined), resulting from 9.3.1. 1996 9.6.2 AI safety requirements, resulting from9.3.2, 9.3.3, 9.3.4, and9.3.6. 1997 9.6.3 Known insufficiencies of the AI system and the corresponding subdomains of the input space, 
1998 resulting from 9.3.5. 1999 10 Selection of AI technologies, architectural and development measures 2000 10.1 Objectives 2001 The objectives of this clause are: 2002 a) to select and justify appropriate AI technologies for use in the AI system; 2003 b) to identify appropriate architectural anddevelopment measures tofulfil the safety requirements prior to 2004 deployment; 2005 c) to identify appropriate architectural measures to mitigate residual functional insufficiencies of the AI 2006 system revealed after deployment; 2007 d) to identify measures for ensuring the safety requirements of the AI system are fulfilled within its target 2008 execution environment. 2009 10.2 Prerequisites 2010 The following information shall be available: 2011 a) safety requirements on the AI system, from Clause 9: 2012 b) training and validation datasets, fromClause 11; 2013 c) AI component or AI system architecture, if already existing; 2014 d) AI component or AI system development process, if already existing. 2015 10.3 General requirements 2016 10.3.1 A justification shall be provided that the selected AI technologies and AI methods are capable of 2017 fulfilling the AI safety requirements. 2018 NOTE AI technology, its application to road vehicle functionality, as well as methods for assuring the AI safety 2019 requirements, are rapidly evolving. However, the most advanced technologies might not be the most suitable for safety-2020 related applications due to the lack of an appropriate set of methods for assuring the safety of such technologies (see 2021 Technology class III of ISO/IEC TR 5469 -[4]). 
. ISO 2024 ¨CAll rights reserved 
2022  EXAMPLE After analysing the benefits and limitations of alternate technologies, an argument is made to justify why  
2023  DNNs are selected and used in combination with a set of redundancy and monitoring measures for a potentially safety- 
2024  related functionality despite the challenges of demonstrating safety requirements for such approaches.  
2025  10.3.2 AI safety requirements shall be allocated to AI components.  
2026  NOTE 1 In some exceptions, AI components might not have allocated AI safety  
2027  requirements, e.g. diagnostics components.  
2028  NOTE 2 The AI safety requirements allocated to the AI component depend on the functionality of the AI component  
2029  and on the AI safety requirements of the system.  
2030  10.3.3 Sufficient measures, such as architectural, development, or a combination of them, shall be defined to  
2031  ensure the AI safety requirements are fulfilled by the AI components.  
2032  NOTE The architectural and development measures contribute to preventing, by design, AIerrors of the AI  
2033  components.  
2034  10.3.4 Sufficient measures, such as architectural, development or a combination of them, shall be defined to  
2035  reduce the risk resulting from contributing AI errors of the AI components.  
2036  EXAMPLE For out-of-distribution error detection, implementation of reject classes implies development measures  
2037  as well as architectural measures for ML systems.  
2038  10.3.5 The effectiveness of the chosen combination of architectural and development measures resulting  
2039  from 10.3.3 and 10.3.4 shall be supported by an argument.  
2040  10.3.6 Safety analysis of the AI system outputs and, where reasonably practicable, of its architectural  
2041  elements shall be performed to determine whether the safety requirements allocated to the AI system can be  
2042  met.  
2043  NOTE For DNNs, safety analysis of the architectural entities of the AI model might not be reasonably practicable.  
2044  EXAMPLE Such safety analysis can include the analysis of the computational graph of AI components to identify if  
2045  the intermediate(latent space) or final outputs fulfils the relevant AI safety requirement allocated to the AI components.  
2046  10.3.11 The differences between the development environment and the target execution environment  
2047  shall be identified and evaluated regarding their potential impact on the safety requirements and, if necessary,  
2048  appropriate AI architectural and development measures shall be defined.  
2049  10.3.12 AI components that are AI models or contain AI models shall be trained using the training  
2050  dataset and evaluated using the validation dataset.  
2051  10.4 Architecture and development process design or refinement  
2052  The architecture of an AI component or AI system isupdated, if available, or designed, to fulfil the safety  
2053  requirements provided as input to this clause. Similarly, an AI system or AI component development process  
2054  is tailored or designed.  
2055  The AI safety requirements are satisfied by two types of measures, architectural and development; there are  
2056  two categories of AI safety requirements, some derived from safety requirements allocated to the AI system  
2057  (from external sources) and some derived from safety related properties of AI system. For the latter ones,  
2058  Table 10¨C1 provides some examples of measures that can help fulfil them.  
2059  It is possible that, during the design process, there is no architecture and/or development process that can  
2060  fulfil all AI safety requirements. In such a case, the challenges will be discussed with the requirement  

2061 stakeholders and the AI safety requirements will be updated (see Clause 9 for guidance on updating the AI 2062 safety requirements). 
2063 Similarly, there can be more than one architecture and/or development process that fulfils all AI safety 2064 requirements during the design process. In such a case, the benefits and the cost of each option will be 2065 discussed with the system integrator to choose the most appropriate option for the AI component or AI system 2066 application. 
2067 Once a candidate architecture and development process are identified, the AI component or AI system is 2068 trained using the training dataset and its potential to achieve its allocated safety requirements is evaluated 2069 using the validation dataset. 
2070 AI model training is a critical step in the development process where the model learns from data in an iterative 2071 manner by using a preferred optimization algorithm. In ¡°supervised learning¡±, for example, the learning 2072 process involves learning the weightsandbiasesthat minimize theerror betweenthe model¡¯sprediction and 2073 the ground truth. The training process relies on several tunable parameters known as hyperparameters (e.g. 2074 learning rate, regularization strength, etc.). Hyperparameter tuning helps optimize the model's 2075 performance. The training process can also involve one or more of the steps such as feature engineering, 2076 regularization, dropout, error analysis etc. 


2077 10.5 Examples of architectural and development measures for AI systems 
2078 Table 10¨C1 provides guidance on which measures can support the achievement of the AI-property-specific 2079 KPIs and targets associated with AI safety requirements and safety-related properties of AI systems. 
2080 NOTE 1 Table D¨C1 in Annex D provides a definition of the safety-related properties of AI systems listed in Table 10¨C 
2081 1. 
2082 NOTE 2 Annex G provides a short description of each architectural and development measure listed in Table 10¨C1. 
2083 NOTE 3 The applicability of a safety-related property of an AI system depends on the use case, encompassing systems 2084 AI models, etc. For example, while a self-drivingvehicle¡¯sactions, such asaccelerationand steering, could becontrollable, 2085 the property of AI controllability might not apply to the outputs of a DNN model for object detection in the perception 2086 pipeline. 
2087 Acknowledging that there is an overlap between safety-related properties of AI systems, rationales used for 2088 the allocation of measures to the appropriate AI property(ies) can include: 
2089 a) AI Resilience supports AI Robustness. The following rationales were considered in the selection of 2090 recommended measures to distinguish AI Robustness and AI Resilience: 
2091 ¡ª Measures that can guarantee that the system maintains its nominal performance under bounded input 2092 perturbations are classified under AI Robustness; 
2093 ¡ª Measures that mitigate a failure for which the system impact is unclear are classified under AI Resilience; 
2094 ¡ª Some measures fall under both categories, AI Robustness and AI Resilience. 
2095 b) AI Controllability can support AI Resilience, e.g. a supervisory system detects an error and switches 2096 between redundant systems. The same supervisory system could also detect an error and trigger a risk 2097 mitigation measure that stops the AI system operation. The following rationales were considered in the 2098 selection of recommended measures to distinguish AI Controllability and AI Resilience: 
2099 ¡ª the aim of AI Resilience is to keep the system running; 
2100 ¡ª the aim of AI Controllability is to keep the system safe. 
2101 c) AI Alignment and AI Predictability share the same objective, confidence in the correctness of the AI 2102 system¡¯s prediction, but achieve it with different means: 
2103 ¡ª Measures focusing on demonstrating that the AI system's behaviour is aligned with the user's 2104 expectations and values are classified under AI Alignment; 
2105 ¡ª Measures providing supporting evidence that the system behaves as expected, i.e. the system¡¯s behaviour 2106 can be reasonably predicted based on its inputs, are classified under AI Predictability. 
2107 Table 10¨C1 ¡ª Example of measures fostering "AI Robustness" 
Type of Measure  Measure  Remarks  
Architectural Measure  Architectural measures that foster AI robustness against Out-of-Distribution (OOD) inputs (G.3.1.1)  Architectural artefacts such as reject classes can be required to detect OOD inputs.  
Diverse redundant models (G.1.1.1) Model ensembles (G.1.1.2) N-version diverse programming (G.1.1.3) Selection techniques for architectural redundancy (voting and switching) (G.1.1.5)  Architectural redundancy combined with a voting system provides confidence in the AI system generating a correct output despite some ML elements providing wrong predictions.  
Development Measure  Fault-aware training (G.4.2)  Training the AI system to recognize faults helps to handle those faults whilst maintaining a nominal or a degraded mode.  
Adversarial training (G.3.1.2, G.4.2)  Adversarial training reduces the AI system¡¯ssensitivity to external malevolent perturbations and increases its reliability.  
Transfer learning (G.4.3)  Transfer learning relies on the robustness a given AI model has shown within its source input domain to leverage this robustness within the target input domain.  
Augmentation of data (G.4.9)  Using data augmentation to increase the diversity of data the model is exposed to helps itgeneralizebetter.. 

2108 Table 10¨C2 ¡ª Example of measures fostering "AI Generalization capability" 2109 Table 10¨C3 ¡ª Example of measures fostering "AI Reliability" 
Type of Measure  Measure  Remarks  
Architectural and Development Measure  Transfer learning (G.4.3)  Transfer learning can leverage the generalization capability of the foundation model to the target application.  
Development Measure  Regularization (G.4.2)  Regularization methods help the model adapt better to small shifts in the input domain and reject OOD inputs.  
Hyperparameter tuning (G.4.1)  Hyperparameter tuning can help reduce the underfitting or  

Type of Measure  Measure  Remarks  
overfitting, thereby improving generalization.  

Type of Measure  Measure  Remarks  
Architectural and Development Measure  All measures supporting AI Robustness, AI Resilience and AI Generalization support Reliability  None  

2110 Table 10¨C4 ¡ª Example of measures fostering "AI Resilience" 
Type of Measure  Measure  Remarks  
Architectural and Development Measure  OOD data and its mitigation (G.3.1) Distributional shifts and its mitigation (G.3.2)  Mechanisms detecting OOD samples and distributional shifts trigger the need for update and containing actionsto ensurethesystem¡¯ssafety until the OOD or distributional shift is addressed.  
Qualitative and Quantitative Analysis of AI architectures (Clause G.2)  Safety analyses on the architecture help identify the need for redundant systems that maintain the AI system to an initial or degraded performance in case of AI error.  
Usage of AI-model based and conventional software (G.1.1.6)  Non-AI components can be used to detect errors and switch to redundant or fallback systems. Redundant systems provide a means to continue the AI system operation with the initial performance.  
Development Measure  Targeted and controlled model update (G.5.2.2)  Partial and targeted updates can allow simplified degradation of safety performance testing and speed up an issue resolution.  

2111 Table 10¨C5 ¡ª Example of measures fostering "AI Controllability" 2112 Table 10¨C6 ¡ª Example of measures fostering "AI Explainability" 
Type of Measure  Measure  Remarks  
Architectural Measure  Usage of AI components and non-AI components (G.1.1.6)  Non-AI components are used to detect errors and switch to redundant or fallback systems. Fallback systems guarantee the AI system's controllability in all situations.  
Supervisory, limiting logic and non-AI backup system (G.1.1.4)  The implementation of safety monitors provides a means to detect errors and take control over one element of the AI system to ensure theAI system¡¯ssafety.  
Architectural and Development Measure  Qualitative and Quantitative Analysis of AI architectures (Clause G.2)  Safety analyses on the architecture help identify the need for monitoring system that can that can detect and control AI errors.  

Type of Measure  Measure  Remarks  
Development Measure  Attention or Saliency Maps (G.4.7)  Attention/Saliency maps provide information that helps understand the characteristics of the input that strongly influence the prediction of the ML system.  
Structural Coverage of AI component (G.4.9.1)  This is a white (or open) box method; it can build confidence by identifying which features of inputs are important for the decision / prediction of AI models.  
Identification of SW units (G.2.1)  Breaking down the architecture into SW units aims to understand the function and performance of each unit, fostering some explainability of the AI component output outcome.  

2113 Table 10¨C7 ¡ª Example of measures fostering "AI Predictability" 
Type of Measure  Measure  Remarks  
Architectural Measure  Model ensembles (G.1.1.2)Techniques for selection of architectural redundancy (G.1.1.5)  Ensembles typically increases the accuracy of the prediction, therefore fostering predictability.  
Architectural and Development Measure  Criteria for Retraining (G.5.2.1)  Monitoring criteria for retraining, e.g. distributional shift, helps identify a decrease in the performance of the AI model to predict the correct output. Consequently, this monitoring also helps to detect a reduction in robustness, resilience and generalisation capability.  
Development Measure  Confidence Calibration and Uncertainty Quantification of AI models (G.4.4)  CalibratingtheAI model¡¯s uncertainty fosters confidence in the correctness of the model's predictions.  
Structural Coverage of AI component (G.4.9.1)  Structural coverage provides confidence in the comprehensiveness of the testing strategy.  
Monitoring multiple scores (G.4.6)  Monitoring model performance metrics such as precision, recall, F1-score during training provides insight intothemodel¡¯sability to predict correct outputs consistently.  

2114 Table 10¨C8 ¡ª Example of measures fostering "AI Alignment" 
Type of Measure  Measure  Remarks  
Development Measure  Alignment of intention (Clause G.6)  None  


2115 Table 10¨C9 ¡ª Example of measures fostering "Bias and Fairness" 
Type of Measure  Measure  Remarks  

Development Measure  Data coverage techniques for test data 
augmentation(G.4.9.2)  This method can help identify and reduce bias within the datasets by  
measuring the data distribution  
across equivalence classes.  

2116 10.6 Work products 2117 10.6.1 AI component or AI system architecture (refined), resulting from 10.3.1 to10.3.11. 2118 10.6.2 AI component or AI system development process (refined), resulting from 10.3.1 to 10.3.11. 2119 10.6.3 Implemented AI component, resulting from 10.3.12. 
2120 11 Data-related considerations 2121 11.1 Objectives 2122 The objectives of this clause are: 
2123 a) to define the dataset lifecycle of activities related to the gathering, creation, analysis, verification and 2124 validation, management, and maintenance of the datasets used in the development of the AI system; 
2125 b) to identify the dataset insufficiencies that may impact the safety of the AI system; 2126 c) to identify the data-related safety properties that have a bearing on the safety of the AI system and that 2127 support dataset safety analysis; 
2128 d) to define the countermeasures to prevent or mitigate dataset insufficiencies using dataset safety analysis 2129 methods at different steps in the dataset lifecycle; 
2130 e) to define the data-related work products that support providing evidence of the safety of the AI system. 2131 This clause applies to AI systems whose development and/or testing relies on data, including AI systems based 2132 on supervised, semi-supervised, and unsupervised learning techniques. 
2133 
2134 11.2 Prerequisites and supporting information 2135 The following information shall be used at the initiation of this phase of activities: 2136 a) AI system definition, including: 
2137 1) AI safety requirements, from Clause 9; 2138 2) input space definition(refined), from Clause 9; 2139 b) field data and functional insufficiencies detected during operation, from Clause 14; 2140 c) safety analysis report, fromClause 13. 2141 11.3 General requirements 2142 11.3.1 A dataset lifecycle shall be defined for the datasets used in the development of the AI system. 
. ISO 2024 ¨CAll rights reserved 
2143 11.3.2 The dataset lifecycle shall be defined such that it supports iterative development of the dataset taking 2144 into account changes in the AI safety requirements and any insufficiencies observed during the AI system 2145 deployment phase. 
2146 11.3.3 The dataset lifecycle shall include activities that relate to the gathering, creation, safety analysis, 
2147 verification, validation, management, and maintenance of the datasets used to develop the AI system. 2148 NOTE An example dataset lifecycle covers requirements development, design, implementation, verification, 2149 validation, safety analysis, and maintenance of the dataset. The dataset verification activity can ensure traceability from 2150 the dataset requirements to the dataset design and implementation. The dataset validation can involve integration of the 2151 AI system and be performed as part of the AI system verification. 
2152 11.3.4 Data-related safety properties of the dataset shall be identified and be used as inputs at different 
2153 phases of the dataset lifecycle. 2154 11.3.5 The dataset lifecycle activities shall include safety analyses to identify potential dataset insufficiencies, 2155 their root causes, and their potential to cause a violation of AI safety requirements. 
2156 11.3.6 Dataset requirements of the dataset shall 2157 ¡ª address the dataset insufficiencies that can lead to violation of the AI safety requirements; 2158 ¡ª specify countermeasures to prevent the dataset insufficiencies, to mitigate them, or both. 2159 11.3.7 Traceability shall be ensured between the dataset requirements and the AI safety requirements. 2160 11.4 Dataset life cycle 2161 11.4.1 Datasets and the AI safety lifecycle 2162 Datasets play a crucial role in AI system development and testing. Machine learning, in particular, typically 
2163 involves an off-line training process whose purpose is to determine values for the parameters of an AI model, 2164 and three different types of datasets enable this training and its usage afterwards: AI training datasets, AI 2165 validation datasets, and AI test datasets. 
2166 Example flows for dataset creation and supervised learning are shown in Figure 11¨C1 and Figure 11¨C2. 

2168 Figure 11¨C1 ¡ª An example dataset creation flow 

2170 Figure 11¨C2 ¡ª An example supervised learning flow 
2171 The AI training dataset and AI validation dataset are used in the iterative AI training process of an AI model. 2172 The AI training dataset is input into the AI model while optimization is performed on its parameters and 2173 hyperparametersbased ontheAI model¡¯sperformance. Thisproceedsuntilthe predetermined AI training 2174 pass/fail criteria are reached. The AI validation dataset is then input into the AI model, and the AI model is 2175 evaluated against the AI validation pass/fail criteria. If the results are not satisfactory, hyperparameters of the 2176 AI model are refined and the AI training process is repeated. 
2177 Once the AI training is completed, the AI model is evaluated with the AI test dataset using the AI test pass/fail 2178 criteria as part of the verification and validation activities. If the verification or the validation fail, the process 2179 is continued after more data are collected and/or training is modified. 
2180 
2181 11.4.2 Reference dataset lifecycle 
2182 A typical dataset lifecycle describes the set of data-related activities carried out during the entire lifecycle of 2183 AI system development, including after deployment. The lifecycle serves as a means to manage the datasets 2184 and supports the realization of the AI safety requirements (and ultimately the safety requirements of the 2185 encompassing system). 
2186 A dataset lifecycle can consist of the following phases: 
2187 ¡ª dataset safety analysis; 
2188 ¡ª dataset requirements development; 
2189 ¡ª dataset design; 
2190 ¡ª dataset implementation; 2191 ¡ª dataset verification; 2192 ¡ª dataset validation; 2193 ¡ª dataset maintenance. 2194 A dataset lifecycle is created for the AI training, AI validation, and AI test datasets used in the AI system 
2195 development (an individual dataset lifecycle can be created for each dataset role, if appropriate). 
2196 A dataset lifecycle can be aligned with or defined as part of the dataset creation and management activities at 2197 the level of the encompassing system, since system-level validation typically also relies on datasets. 2198 Figure 11¨C3 provides an example dataset lifecycle based on the traditional V-model of development. Some of 
2199 the salient features of the lifecycle are traceability of AI safety requirements to the dataset requirements 2200 (which impact the dataset¡¯s design and implementation) and an iterative workflow that extends into 2201 operation, where new data can influence dataset revision. 

2203 Figure 11¨C3 ¡ª Dataset lifecycle model 
2204 Clauses 11.4.3 through 11.4.9 discuss each dataset lifecycle phase in more detail. 2205 
2206 11.4.3 Dataset safety analysis 
2207 11.4.3.1 General considerations 
2208 Dataset safety analyses focus on identifying safety-relevant dataset insufficiencies. When these dataset 2209 insufficiencies have been examined and the causes and consequences of such dataset insufficiencies have been 2210 identified (including the risks at the AI system and encompassing system), that information is fed as inputs to 2211 the dataset requirements development, dataset design, and dataset implementation to realize: 
2212 ¡ª countermeasures to prevent or mitigate dataset insufficiencies; 
2213 ¡ª metrics to judge achievement of the dataset insufficiency avoidance. 
2214 Depending on the dataset lifecycle phase, different approaches to dataset safety analyses can be used as 2215 outlined below. 
2216 Dataset requirements development phase: 
2217 ¡ª A guideword-based approach (such as that applied in HAZOPs) can be used to identify how dataset 2218 insufficiencies impact the safety of the AI system. Using this approach, one can determine what 2219 characteristics of the dataset(s) lead to the AI system performing a function incorrectly, seldom, too often, 2220 too little, too early, or too late. 
2221 ¡ª A qualitative risk analysis approach can be used to define the rigor applied in the dataset lifecycle to avoid 2222 dataset insufficiencies. Such an approach is analogous to the application of HARAs to determine ASIL in 2223 ISO 26262, and it considers: 
2224 ¡ª the severity of the outcome associated with dataset insufficiencies (including the risks at the AI 2225 system and encompassing system); 
2226 ¡ª the likelihood of the outcome associated with dataset insufficiencies; 
2227 ¡ª existing countermeasures that can prevent or mitigate dataset insufficiencies; 
2228 ¡ª additional countermeasures that can be applied to prevent or mitigate the dataset insufficiencies, 2229 and the degree of rigor with which such countermeasures can be applied. 
2230 Generally, more significant risks at the AI system and encompassing system can be associated with a 2231 qualitatively higher assurance level and hence a greater rigor in the robustness of the countermeasures that 2232 can be applied. 
2233 Last, a residual risk analysis approach can be applied after the impact of the countermeasures in risk reduction 2234 has been considered. 
2235 Dataset design phase:Various deductive and inductive analysis approaches can be conducted considering the 2236 proposed dataset design and can generate additional countermeasures that were not originally introduced at 2237 the dataset requirements development phase. 
2238 Dataset implementation phase:A process failure mode and effects analysis(PFMEA) approach can be 2239 employed to identify potential issues in the processes, methods, and tools of the data preparation and labelling 2240 and link these issues with the dataset insufficiencies and/or violations of the AI safety requirements (see 2241 Clause 9 of this document and ISO 26262-9). 
2242 EXAMPLE An AI system classifies in-path objects for alert and trajectory planning purposes. The AI system uses a 2243 colour image from a camera mounted on the windshield, and the AI training data, in particular, is hand-labelled. A PFMEA 2244 raises issues that can generate safety-relevant dataset insufficiencies such as: 
2245 ¡ª using images that were collected from the same camera on a different vehicle with different mounting and 2246 calibrations (data reuse impact); 
2247 ¡ª using images that were collected 10 years ago, even though there has been a considerable change in the types of 2248 vehicles that are on the road in the domain of interest (data ageing impact); 
2249 ¡ª using images that were collected from one region even though the system is targeted to operate in multiple regions 2250 with varying types of in-path object behaviour (data bias impact); 
2251 ¡ª using images that have not been labelled in a standard and correct manner (labelling inconsistency impact). 
2252 A summary of outputs of the dataset analyses serves as evidence to demonstrate that safety-relevant dataset 2253 insufficiencies are prevented or sufficiently mitigated. This summary can motivate or reference further 2254 artefacts (e.g., dataset tool qualification plan). 
2255 11.4.3.2 Dataset-related safety properties 
2256 Dataset insufficiencies are insufficiencies of the dataset regarding data-related safety properties under 2257 consideration. Examples of these data-related safety properties are given in Table 11¨C1, and they encompass 2258 both general properties and properties specific to AI applications. 
2259 Table 11¨C1 ¡ª Examples of data-related safety properties 
Property  Definition  
Accuracy  The data correspond to their source with respect to semantical representation and interpretation.  
Completeness  The data elements (including metadata) are populated and the data have defined coverage of the input space, safety-relevant cases, and plausible data perturbations.  
Correctness (or fidelity)  The data correspond to the phenomenon they intend to capture and include features and metadata which help to characterize the phenomenon.  
Independence of datasets  The datasets sufficiently avoid leakage of information amongst themselves with respect to data sources and the methods used to capture, gather, generate, and process the data.  
Integrity  The data are not altered by natural phenomenon (e.g., noise) or intentional action (e.g., usage of lossy data compression without consideration of impact to model, poisoning).  
Representativeness  The distribution of data corresponds to the information in the environment of the phenomenon to be captured; it is free of biases.  
Temporality  The data gives sufficient consideration to time-based characteristics (e.g., timeliness, ageing, lifetime, time contributing to distribution shift).  
Traceability  The derivation of the data from their origin (including information on how they were captured, gathered, generated, and processed) is demonstrated.  
Verifiability  The data include sufficient features to be amenable for verification as prescribed by their requirements and properties.  

2260 NOTE 1 ISO/IEC CD 5259-1 and the SCSC Data Safety Guidance Version 3.2 detail additional data-related safety 2261 properties (e.g., portability, understandability, auditability). 
2262 
. ISO 2024 ¨CAll rights reserved 71 
2263  NOTE 2  Properties might not necessarily be mutually exclusive (e.g., a well-known property that is not listed is  
2264  independence and identical distribution, or IID, which is covered by the correctness, completeness, and independence  
2265  properties).  
2266  Regarding a dataset insufficiency due to lack of independence of datasets, independence between the AI training  
2267  and AI validation datasets supports detecting overfitting. Though not required, the K-fold cross validation  
2268  technique can support this as the AI training and AI validation datasets are independent in each of the folds.  
2269  Independence between the AI training and AI test datasets, on the other hand, supports providing a reliable  
2270  statistical estimation of the residual risk of the trained AI component.  
2271  Regarding a dataset insufficiency due to lack of representativeness, biases can manifest in different forms.  
2272  Human cognitive biases impact how engineering decisions are made and how datasets are sampled. Non- 
2273  human  cognitive  biases  (such  as  sensors  failing during  data  collection)  result  in  systematic  dataset  
2274  insufficiencies. More information on these forms can be found in Clause 6 of ISO/IEC TR 24027.  
2275  Generally, a bias in the AI training dataset impacts the performance of the AI system and is unwanted.  
2276  However, intended biases can be used as a design measure to put the AI training focus on some important but  
2277  rare features critical to the safety of the AI system (e.g., an AI training dataset with a higher occurrence rate of  
2278  corner cases can be used to complement an AI test dataset based on the real-world distribution). This design  
2279  measure can still be insufficient to capture the true variability of rare safety-relevant cases, however.  
2280  Specific examples of dataset insufficiencies and the potential actions to avoid such dataset insufficiencies can  
2281  be found in Table 11¨C2.  
2282  Table 11¨C2 ¡ª Examples of dataset insufficiencies  

Property  Dataset Insufficiency Example(s)  Potential Actions to Avoid Insufficiency  
Accuracy  The resolution of camera images is not sufficient according to expected AI model inputs for object detection. An AI system operates with sensors detecting certain type of obstacles at a given distance range and high  ¡ª Selection of source sensors appropriate for the input space and application; ¡ª Inspection of manually labelled data.  
speed, but the camera used is not adapted for the range and speed, yielding blurry images. The mesh used in LiDAR imaging of objects is not fine enough (number of points, spacing) to properly detect target obstacles.  
Completeness  Few images have obstacles close to the camera in a dataset for obstacle detection. No night images are in the dataset even though the input space includes nighttime.  ¡ª Investigation of general use cases; ¡ª Calculation of distribution of the data and verification that the data cover the input space;  
Perturbations like noise, brightening, darkening, vibration, rotation, turbulence, blurring, blooming, smear, and interference are not reflected in the dataset. An AI system for traffic signal identification is trained with a dataset does not contain data elements that have all of the possible variations of traffic signal shape,  ¡ª Collection of data from different geographical experts' perturbations on data that represent realities within the input space; ¡ª Addition of data through selection, generation, augmentation, or synthesis if there are gaps identified (e.g., by  

Property  Dataset Insufficiency Example(s)  Potential Actions to Avoid Insufficiency  
height, positions, etc. outputted by  analysis such as examination of  
the AI system.  saliency maps, training, and  
Missing information on the location  testing);  
of captured data does not allow one  
to analyse the geographical  ¡ª Monitoring and collection of new  
distribution of data and can cause  or changing items within input  
undetected bias.  space; ¡ª Corner and edge case collection.  
Correctness (or fidelity)  Annotators manually create bounding boxes around objects inconsistently, which leads to object size per scenario to be calculated differently. No distinction has been made between a motorcycle and its rider in  ¡ª Characterization of the essential features of the target phenomenon; ¡ª Determination of the adequacy of the sensors to detect, observe, and capture the phenomenon;  
an image label, though this is relevant for the driving task that the AI system performs. A scene is marked as rain by an annotator although it is in snow. LiDAR provides ground truth for a camera outputting distance to an object. The vehicle collecting data drives through rain, which causes the ground truth to be noisier than in nominal conditions. Adhesive body markers provide ground truth for a driver monitoring system outputting head position, and the markers shift during the data collections.  ¡ª Redundancy of sensors.  
Independence of datasets  One frame in a sequence is in the AI test dataset and the next frame is in the AI training dataset. Due to frame rate, the frames are nearly identical. One frame of a certain geo-position is in the AI test dataset and another taken later at the same geo-position is in the AI training dataset. For object detection, this can compromise the independence of the datasets. All datasets used for an AI model development come from the same exact environment (e.g., same city street, time intervals, weather conditions, and traffic load). All datasets are collected relying upon the same means (e.g., only one sensor or database).  ¡ª Use of data management system; ¡ª Use of different sources of data; ¡ª Separation of the teams preparing the different datasets; ¡ª Use of different technical means for data capturing, e.g. different sensors, vendors, and brands; ¡ª Deployment of different processes/methods for dataset creation, e.g. applying two algorithms for data sample generation.  

Property  Dataset Insufficiency Example(s)  Potential Actions to Avoid Insufficiency  
The task for dataset creation is always based upon the same technique, algorithm, or parameters and is conducted by the same person.  
Integrity  Corruption of hardware storage introduces error(s) in the dataset. Failure of database memory introduces error(s) in the dataset.  ¡ª General inspection; ¡ª Analysis of robustness to adversarial attacks on the dataset (e.g., random erasing,  
Untrained/careless user inadvertently introduces inconsistent data element (e.g., altered label). Error is introduced during dataset processing/manipulation due to transfer over lossy channel.  corruption); ¡ª Standard access controls (e.g., authorized users with passwords, denial of service protection techniques); ¡ª Built-in features for integrity checks in databases and other data storage; ¡ª Inclusion of integrity check codes (e.g., CRC, checksum, hash) for storage and transfer over lossy channels.  
Representativeness  An AI system for driver monitoring is going to be used in a region where drivers have an even distribution from 20 years old to 100 years old, but the dataset does not contain drivers above 80 years old. AI datasets collected by a heavy truck fleet can have geospatial bias for an AI system intended to perceive aspects of roadways with weight limits. Synthetic data do not capture have differences with real-world data to which the AI system is sensitive, like shadows in images. Real-world data have been captured with wrong sensor parameters, resulting in variances to which the AI system is sensitive. Sensor data have disturbances due to a bug on the camera.  ¡ª Analysis and comparison of the theoretical and experimental distributions of the phenomenon; ¡ª Distributional drift analysis.  
Temporality  COVID-19 induced change in distribution of people wearing face masks, but the dataset for a driver monitoring system does not consider this.  ¡ª Inclusion of metadata containing details like time of creation and validity; ¡ª Usage of version control for the dataset.  

Property  Dataset Insufficiency Example(s)  Potential Actions to Avoid Insufficiency  
Traceability  An image lacking information on its source appears to be complete under simple visual inspection and is integrated into a dataset. Two datasets containing the same category of data are integrated into a single dataset without their metadata and attributes. The original datasets and their metadata are deleted. Data samples that were randomly selected for a training dataset decrease performance of the AI model due to those samples being corner cases. The samples are subsequently removed from the training dataset, but their metadata do not have an attribute to label them as corner cases. A new optimization algorithm is applied to datasets without properly tracing its application in the data management process. The optimized datasets are still used as replacement of the older ones.  ¡ª Use of data management system; ¡ª Updating of data management process to account for all tasks impacting datasets; ¡ª Creation and inclusion of appropriate and sufficient metadata to the data element collected and synthesized.  
Verifiability  A framework that relies upon a random algorithm generates data samples that violate a safety indicator in a simulation run. Without sufficient mechanisms for reproducing the same run, the violation is likely unverifiable. No checksum, CRC, or hash mechanisms were activated at any time in the data management process, and the datasets cannot be integrity-checked. Dataset images cover a certain period of the year (e.g., autumn and winter), but the camera was not correctly configured with the actual date and the respective metadata is unreliable. ¡ª Different camera vendors and brands were used to ensure independence of data. However, all images were mixed and processed together, and the technical means were not recorded.  ¡ª Ensuring reproducibility of data generation; ¡ª Mechanisms to allow verification of data properties, e.g. built-in features in databases for integrity checks; ¡ª Discarding of datasets with unreliable or insufficient metadata; ¡ª Manual analysis; ¡ª Use of statistical sampling methods.  

2283 11.4.4 Dataset requirements development 2284 The dataset requirements development follows the activity flow below, assuming that the method specified in 2285 ISO 26262-3 regarding item definition and the method specified in ISO 21448 Clause 7 regarding triggering 
2286 conditions have been followed: 2287 a) comprehension of the AI system; 2288 b) dataset safety analysis; 2289 c) dataset requirements formulation; 2290 d) dataset requirements quality assurance. 2291 The comprehension of the AI system activity of the dataset requirements development focuses on 
2292 understanding the intended functionality of the AI system, including: 2293 ¡ª the AI safety requirements, from Clause 9; 2294 ¡ª the input space definition, from Clause 9. 
2295 The dataset safety analysis activity is performed in alignment with the guidance in Clause 11.4.3, and the 2296 outputs are fed into the dataset requirements formulation. 2297 The dataset requirements formulation activity focuses on formulating the dataset requirements that mitigate 
2298 the risks associated with the output of the AI system. It specifies: 2299 ¡ª the logistical aspects, addressing the following items at minimum: 2300 ¡ª where the dataset is stored; 2301 ¡ª who has access to the dataset, what type of access they have, and when they have access, including 2302 consideration given to ensure that this dataset is safe from unintended editing; 2303 ¡ª how the dataset is version controlled and how changes are tracked; 2304 ¡ª requirements on the verification and validation processes to be employed to ensure that the data 2305 within the dataset is correct and appropriate for usage; 2306 ¡ª how stakeholders can report known vulnerabilities, risks, or biases in the data and/or dataset 2307 during any of the dataset life cycle phases. 2308 ¡ª the technical aspects, addressing the following items at minimum: 2309 ¡ª size of the dataset; 2310 ¡ª format of the data within the dataset, including what syntactic and semantic parameters describe 2311 the data and what the format for labelling is; 2312 ¡ª boundaries of the data within the dataset (driven by both ground truth and design decisions); 2313 ¡ª dataset¡¯srole (AI training, AI validation, orAI test) andwhat ensuresthat itissufficientfor its 2314 given role, including limitations on how many times it can be used (to avoid overfitting); 2315 ¡ª constraints affecting creation of the dataset (e.g., region-specific data privacy regulations); 2316 ¡ª mitigations for the different manifestations of dataset insufficiencies detailed in Clause 11.4.3; 
2317 ¡ª methods to prevent undetected data failures. 2318 Finally, the dataset requirements quality assurance activity focuses on ensuring that the dataset requirements 
2319 follow the criteria given in ISO 26262-8, Clause 6. Requirements are: 2320 ¡ª traceable to the AI safety requirements; 2321 ¡ª updatable and maintainable upon a change to the encompassing system, the AI system, or input space; 2322 ¡ª updatable upon exposure of an insufficiency in the AI system due to the discovery of new safety-relevant 
2323 scenarios or other triggers. 2324 Table 11¨C3 ¡ª Further considerations for requirements development 
Requirements Topic  Considerations  
Input space  ForanADS, ¡°input space¡± isequivalent to ¡°operational designdomain¡±. An input space can also include the user demographic and user-driven parameters (e.g., a driver monitoring system can have an input space that includes drivers with face paint).  
Dataset's role  Regarding the AI test dataset role, the AI test dataset is used specifically in the AI test verification part of the process, which can be performed as part of the vehicle-level verification and validation. Ideally, a large AI test dataset is used to uncover overfitting. In case of repeated performance measurements on the same AI test dataset, the statistical validity of the test results can be threatened by implicitly optimizing towards the AI test dataset, and the dataset requirements can include countermeasures to address this. Example countermeasures include the following: ¡ª 
Employing multiple independent AI test datasets for use across different iterations of AI system development; ¡ª 
Restricting access to the AI test dataset such that KPIs measured on a random subset of the dataset are returned instead of detailed results. Datasets can support another role ¨Cproviding a means to monitor the input space while the AI system is in operation. Clause 11.4.9covers this use case in more detail.  
Boundaries of data  An example of setting boundaries is a situation in which a data element for a particular dataset has the following format: <Time>, <Day>, <Traffic heaviness>. Time is bounded between 00:00 and 23:59, inclusive, day is bounded within the [Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday] enumerations, and traffic heaviness is bounded within 0 and 100%, inclusive.  
Constraints affecting creation  An example of a constraint is a situation where a region in which a vehicle is going to operate has restrictions disallowing capturing images of individuals. This can  

Requirements Topic  Considerations  
introduceanunwanted biasif theAI system¡¯sfunctionis to classify pedestrians based on the images.  
Traceability  Traceability from dataset requirements to AI safety requirements (including those which have been generated from consideration of the input space (refined)) can be evidence that the AI safety requirements have been sufficiently considered. An example of this is as follows: ¡ª An AI system has an AI safety requirement that specifies the acceptable false positive rates for the function that the AI system performs in two different weather conditions: sunny and rainy. ¡ª The AI training dataset has dataset requirements that specify how much of its data are in sunny conditions and how much of its data are in rainy conditions. Similar dataset requirements are created for the other datasets. ¡ª These dataset requirements link to the AI safety requirement.  

2325 11.4.5 Dataset design 
2326 The dataset design outlines details on: 
2327 ¡ª data elements that are collected physically, created synthetically, and/or created through augmentation 2328 (and how augmentation is applied in dataset generation and on-the-fly in the AI training pipeline, if 2329 applicable); 
2330 ¡ª which aspects of the data elements comprise the core data; 
2331 ¡ª the metadata, including any ground truth data associated with objects of interest within the data elements 2332 (also known as the labelling specification for supervised learning); 
2333 ¡ª the operations to be performed on the dataset (e.g., filtering of irrelevant or invalid data, dimensionality 2334 reduction, de-identification of data for data privacy purposes, normalization of the data with respect to 2335 appropriate metadata parameters, etc.); 
2336 ¡ª any mechanisms to be realized for monitoring the distribution shift in the input data during operation 2337 and collecting additional data items for subsequent revision of the dataset. 
2338 The details outlined in the dataset design are to be documented and subjected to analysis to ensure that they 2339 preserve the AI safety requirements and the dataset requirements. 
2340 11.4.5.1 Creation of data elements and identification of core data 
2341 There are three main approaches used for creating data elements in a dataset: 
2342 1) Physical collection of data elements: Data elements in the dataset are directly obtained using either the 2343 sensors used in the encompassing system or their surrogates. In the case of surrogate sensors, a gap 2344 analysis is done to assess the effects of the difference with the native sensors, and countermeasures are 2345 taken to contain the effects. 
2346 2) Synthetic creation of data elements: Certain aspects of the input space might not have been captured 2347 during data collection, and various simulation tools and specialized machine learning methods like 2348 generative adversarial networks can be employed to create additional data elements that capture those 2349 aspects. The adequacy of the synthetic data elements is considered. 
2350 3) Augmentation of physically-created data elements: A physically-created set of data elements are 2351 augmented to create a new set of data elements that have parameters altered to include perturbations 2352 such as noise, brightening, darkening, vibration, rotation, turbulence, blurring, blooming, smear, and 2353 interference. If augmented data are used for testing and if the data even partially replace real-world tests, 2354 the adequacy of the augmentation is considered. 
2355 EXAMPLE For computer-vision based data, the means of altering can include color space transformation, 2356 background modification, superposition of multiple images, flipping, scaling, translation, and random cropping. 
2357 The aspects of these data elements that serve as the inputs to the AI function during runtime operation of the 2358 AI function (e.g.. the raw image data from a camera sensor, a 3-dimensional 128 x 128 x 3 array of RGB pixel 2359 values, for an AI system built on computer vision) are the core data. 
2360 11.4.5.2 Design of metadata for data elements and datasets 
2361 The metadata associated with a data element provides valuable information about the data element that can 2362 be used during training, analysis, and verification. For a supervisory ML system, the metadata includes the 2363 ground truth or the label used during the training process. The data type, structure, and range of values that 2364 the labels assume are also identified during the design phase, and they conform to the functionality of the AI 2365 system and meet the dataset requirements. 
2366 EXAMPLE 1 An AI system used to classify objects for an automatic emergency braking system depends on an AI 2367 training dataset for which the data elements have metadata containing the class of the object (e.g. car, truck, person), the 2368 height and width of the object (the bounding box), and the distance of the object from the subject vehicle. 
2369 Metadata can also be associated with a dataset as a whole. The metadata associated with a dataset serves to 2370 support analysis, verification, and validation of the dataset, and can contain the following: 
2371 ¡ª details on how the dataset was created, e.g., physical collection, details of sensor devices, synthetic 2372 methods and tools that were used for generation, augmentation, etc.; 
2373 ¡ª statistics of syntactic and semantic parameters of data elements in the dataset; 
2374 ¡ª information that relates the data elements in the dataset to the AI system and the encompassing system, 2375 input space, object/event detection and response, dynamic driving tasks, edge cases, etc. 
2376 EXAMPLE 2 An image dataset can be defined to be such that 10% of its data elements contain traffic signals in the top 2377 left corner. In the assessment of completeness, the range of values of various parameters and their combinations can be 2378 useful. 
2379 
2380 11.4.6 Dataset implementation 
2381 The activities in the dataset implementation phase realize a concrete dataset based on the dataset 2382 requirements and dataset design, and they include: 
2383 ¡ª defining the processes, methods, and tools to prepare a given dataset (e.g., physical, synthetic, and/or 2384 augmented data generation, cleaning as covered in Clause 10.4.5, etc.); 
2385 ¡ª preparing the dataset; 
2386 ¡ª defining the processes, methods, and tools for labelling the dataset; 2387 ¡ª labelling the dataset. 
2388 NOTE ISO/IEC 23053 identifies the data preparation step as a tool in the development of an AI system. ISO/IEC CD 2389 5259 defines a data quality framework that can be used as guidance during this phase, and ISO 24368 provides guidance 2390 on having processes in place for stakeholders to disclose/report known vulnerabilities, risks, or biases associated with 2391 the dataset preparation and labelling, which can then be fed into a dataset safety analysis. 
2392 Regarding labelling in particular, it is often a human-labour intensive process involving a label supplier, and 2393 label quality tests and audits are applied. The nature and extent of these tests and audits is commensurate 2394 with the complexity of the inputs and outputs being labelled. 
2395 EXAMPLE 1 If the data to be used for an AI system development is LiDAR data involving point clouds, the labelling 2396 to identify objects in the inputs could be complex and error-prone. To address this, the labelling process can employ 2397 multiple levels of human involvement, e.g., labellers, reviewers, and auditors, and possibly also semi-automated and 2398 automated label quality tests and plausibility checks. 
2399 Additionally, for labelling, consideration is given to how any ground truth is obtained to ensure that any 2400 instrumentation used to obtain the ground truth does not interfere with how the data are represented. 
2401 EXAMPLE 2 Body markers being used to provide ground truth positioning of a person for a camera-enabled driver 2402 monitoring system can cause interference since the body markers will likely not be a part of the real-world operation 2403 distribution set. 
2404 As part of the dataset implementation activities, the details of records created during the preparation and 2405 labelling are documented as inputs for dataset safety analysis and dataset verification activities. The coverage 2406 of the input and output spaces and the statistical distribution of the datasets are also recorded. 
2407 11.4.7 Dataset verification 
2408 Dataset verification applies to the dataset under evaluation, and its purpose is to confirm that the dataset has 2409 been developed correctly. It comprises product verification complemented by process verification: 
2410 ¡ª product verification: 
2411 ¡ª determining the consistency and correctness of information in a data element; 
2412 ¡ª determining the consistency and correctness of information at the dataset and metadata levels, 2413 e.g., lack of outliers, missing data elements, duplicates, wrong data types, etc.; 
2414 ¡ª verifying the conformance of the dataset against dataset requirements, e.g., metrics on 2415 distribution of parameters of the dataset, extreme values and edge cases of parameters and their 2416 combinations, noise characteristics, independence between datasets, etc.; 
2417 ¡ª process verification: 
2418 ¡ª checking that the design and implementation phases are performed correctly; 
2419 ¡ª checking the correctness of the processes, methods, and tools used to create the dataset and its 2420 metadata (including any other AI systems involved in ground truth labelling). 
2421 NOTE Product verification can be done either manually or using automated tools, depending upon the type of 2422 checking that is involved. For instance, verification of the information about the sensing device used for data collection 2423 can require manual inspection, while that of the ground truth label can employ running automated software. Checking 2424 the correctness of all of the data elements in a dataset can be impractical, so this can be done using statistical sampling 2425 approaches. 
2426 EXAMPLE For an AI system that is expected to learn certain high-level concepts (e.g., pedestrians, vulnerable road 2427 users, traffic signals etc.), the dataset is required to have a sufficient number of data elements containing these concepts 2428 so that they can be learned. The AI system metric is that the AI system perceives vulnerable road users under certain 2429 lighting conditions with an accuracy of X%. This AI system metric relates to dataset requirements that Y% of the training 2430 dataset and Z% of the AI test dataset contain vulnerable road users under those lighting conditions. If these dataset 2431 metrics are not met, the dataset is enhanced with additional data elements. 
2432 Dataset verification is repeated every time dataset requirements are added or refined. The details of the 2433 verification carried out are documented as evidence for the verification of the dataset. 
2434 11.4.8 Dataset validation 
2435 Dataset validation ensures the correctness of the dataset requirements from the dataset requirements phase, 2436 i.e., if the correct dataset and data elements are developed for the AI system with the desired safety properties 2437 and if they reflect a correct translation of the AI safety requirements. 
2438 There are two approaches to dataset validation activities which can be applied together or individually: 
2439 a) requirement conformance, which involves checking that the derived dataset requirements meet the 2440 expected objectives of the dataset; 
2441 b) integration testing, which involves checking that the AI system developed using the dataset(s) (i.e., trained 2442 and tested with the dataset(s)) meets the AI safety requirements. 
2443 For the first approach, the expected objectives of the dataset are adequately articulated at the AI system 2444 development phase and handed over to the dataset development team. Often, the expected objectives are in 2445 terms of use cases and edge cases of the AI system, and checking that these are met is done by examining and 2446 reviewing the dataset requirements. As part of this evaluation, the consistency and completeness of the 2447 dataset requirements can be assessed. 
2448 In addition, requirement conformance can be carried out by examining the AI safety requirements. Every 2449 safety requirement that has a potential impact on a dataset is covered by one or more dataset requirements 2450 (although there can be dataset requirements that do not trace to an AI safety requirement). Requirement 2451 conformance involves checking that a correct and desired traceability exists, and can be carried out manually 2452 and/or using some automated support of analysis of the requirements. 
2453 The second approach to dataset validation is integration testing. In this approach, the AI system is derived or 2454 revised using the dataset, and the resulting AI system is verified against its requirements, with the additional 2455 objective of the AI system verification being to check that the right dataset was developed. Any failure in AI 2456 system verification can be traced to deficiencies in the dataset (and subsequently accounted for in the dataset 2457 requirements or design) or to other issues, like aninadequate AI system architecture or an inadequate training 2458 process. 
2459 The dataset validation phase results in evidence describing the relationship of dataset requirements to the AI 2460 safety requirements and summarizing the review results 
2461 11.4.9 Dataset maintenance 
2462 Dataset maintenance refers to the set of activities that ensure that a dataset is up-to-date and compliant with 2463 the dataset requirements. These activities are carried out across the entire dataset lifecycle. 
2464 Along the lines of ISO 26262-8, the dataset maintenance activities can include: 
2465 ¡ª configurations of the dataset, including what they are, what they do, and how they are managed; 
2466 ¡ª management of dataset resources, tools, repositories, access rights, and timelines; 
2467 ¡ª change management of datasets including, what triggers changes to the dataset; 
2468 ¡ª retiterment and decommisioning of dataset and their elements. 
2469 NOTE Guidance for retirement and decommissioning are available in ISO/IEC DIS 5338 and ISO/IEC DIS 8183. 2470 Dataset maintenance activities include actions taken during operations (see Clause 14) and involve 2471 ¡ª general field data collection and monitoring (e.g., monitoring the inputs to the AI system for conformation 
2472 to the AI safety requirements and identifying data elements corresponding to safety-relevant edge cases 2473 encountered during operation); 2474 ¡ª out-of-distribution (OOD) data identification, collection, and processing; 2475 ¡ª AI system adaptation. 2476 Table 11¨C4 ¡ª Examples of dataset maintenance activities 
Sub-Category  Example(s)  
Out-of-distribution (OOD) data identification, collection, and processing  When an AI system fails to detect certain objects, causing unwanted behaviours at the system level, data with those objects are collected and uploaded to fine-tune the AI model. Aninfrastructurefeaturethat wasintheAI system¡¯s input space has become obsolete. Data elements containing that feature are removed from the dataset. An AI system for driver monitoring performs sub-optimally when the driver is wearing a certain thickness of glasses. Data elements containing people wearing that thickness of glasses are added to the dataset.  
AI system adaptation  When an AI system is deployed in another country, it might be expected to detect different signage and adjust system behaviour according to local laws and regulations. Data elements containing that signage are added to the dataset. An ADS that employs an AI system goes from operating on limited access highways to also operating on highways with at-grade crossings. Data elements containing at-grade crossings are added to the dataset.  

2477  11.5 Work products  
2478  11.5.1 Dataset lifecycle,resulting from 11.3.1 and11.3.2.  
2479  11.5.2 Evidence for the outputs of the defined phases of the dataset lifecycle, resulting from 11.3.3.  
2480  11.5.3 Evidence for the safety analyses of the dataset, resulting from 11.3.4 and 11.3.5.  
2481  11.5.4 Dataset requirements specification, resulting from11.3.6 and11.3.7.  
2482  12 Verification and validation of AI system  
2483  12.1 Objectives  
2484  The objectives of this clause are:  
2485  a)  to verify that the AI system fulfils its AI safety requirements;  
2486  b)  to validate that the safety requirements allocated to the AI system are achieved when integrating into the  
2487  encompassing system;  

2488 NOTE 1 This clause includes guidance for: 2489 ¡ª the stand-alone performance analysis of the AI system itself; 2490 ¡ª testing at the AI system and AI component level. Testing at the AI component level can include AI model, and pre-and 
2491 post-processing elements. 2492 NOTE 2 Theterm ¡°validation¡± is typically used differently withinthedevelopment of AIsystemsthanis used within 
2493 safety standards and system safety engineering. Inthisdocument, theterm ¡°AIsystem safety validation¡± defined inAI 2494 system safety validation (3.1.21) is used. 2495 12.2 Prerequisites and supporting information 
2496 The following information shall be available to complete the verification and validation activities associated 2497 with the corresponding phases of the AI safety lifecycle: 2498 a) Safety requirements allocated to the AI system (from external sources); 
2499 b) AI safety requirements, from Clause 9; 2500 c) Known insufficiencies of the AI system and the corresponding subdomains of the input space, from Clause 2501 9; 
2502 d) Input space definition (refined), from Clause 9; 
2503 e) AI Component or AI System Architecture, from Clause 10; 
2504 f) Implemented AI component, fromClause 10; 
2505 g) Dataset lifecycle, from Clause 11; 
2506 h) Evidence for the outputs of the defined phases of the dataset lifecycle, from Clause 11; 
2507 i) Evidence for the safety analyses of the dataset, from Clause 11; 
2508 j) Dataset requirements specification, from Clause 11. 

















2509 12.3 General requirements 
2510 12.3.1 The AI system shall be verified to provide evidence for: 
2511 a) compliance with the AI safety requirements; 
2512 b) confidence in the absence of unintended functionality and properties. 
2513 NOTE Confidence in the absence of unintended functionality and properties can be increased, for example, by 
2514 following safety standards such as ISO 26262, ISO 21448 and this document during the development. 
2515 12.3.2 Testing of an AI system shall be performed on the AI components that can be tested stand-alone, and 2516 on the integrated AI system. 2517 NOTE If an AI component cannot be tested stand-alone, the AI component is tested at a higher level of 
2518 integration.Inthis case, the AI system integration strategy is devised to accommodate this testing. 2519 EXAMPLE 1 a)shows an AI system with three AI components. Assume AI component C can be tested stand-alone, 2520 but AI components A and B cannot. For example, suppose AI components A and B implement a convolutional neural 2521 network to propose bounding boxes and a non-max suppression algorithm to integrate them. In another example, 
2522  suppose AI components A and B implement a backbone/head (neural network), i.e. feature extraction such as vision  
2523  transformers, shared with other AI systems and a task-specific neural network for this AI system. Output A is an  
2524  intermediate value in these cases and cannot be tested stand-alone. However, AI components A and B can be tested if  
2525  integrated together. In such a case, the integrated AI component is considered and tested stand-alone. 1 b)shows the  
2526  integrated AI component (AI component D) within the AI system. Here, testing can be performed on AI components C  
2527  and D and on the integrated AI system.  


1 a) ¡ª An integrated AI system with 3 AI 1 b) ¡ª AI components A and B are integrated so  
components that the integrated AI component D can be  
tested stand-alone  
2528  12.3.3 Test cases for the verification of the AI components shall be derived using best practices for test case  
2529  derivation including an appropriate combination of the methods listed in ISO 26262-6:2018, Clause 9, i.e.  
2530  analysis of the requirements, generation and analysis of equivalent classes, analysis of boundary values and  
2531  error guessing based on knowledge or experience.  
2532  EXAMPLE 1 Analysis of the requirements, including the required safety properties, might be used to select KPIs for  
2533  the V&V activities.  
2534  EXAMPLE 2 Generation and analysis of equivalent classes might be suited to generate complete test sets for pre-and  
2535  post-processing algorithms.  
2536  EXAMPLE 3 Error guessing based on knowledge or experience might be suited to identify yet unknown edge cases  
2537  for testing.  
2538  EXAMPLE 4 Analysis of boundary values might be suited to generate complete test sets for pre-and post-processing  
2539  algorithms.  
2540  NOTE 1 For machine learning,.analysis of requirements relieson statistical tests to analyse whether the safety  
2541  relevant performance requirements are met.  
2542  NOTE 2 If relevant, error guessing should include evaluation of known and potential triggering conditions, and  
2543  evaluation of known and potential functional insufficiencies.  
2544  NOTE 3 Theterm ¡°knowledge¡± should beinterpreted broadly beyondhumanknowledgeandcanincludeknowledge  
2545  automatically derived by an algorithm.  
2546  NOTE 4 For AI models used in perception modules of an autonomous driving vehicle, verification involves driving  
2547  scenes in test datasets. One can refer to ISO 34502:2022 for more details on creating test scenarios for such models.  
2548  12.3.4 Each test case of an AI component shall include pass/fail criteria.  
2549  NOTE Pass/fail criteria can based on the formulation of the thresholds and parameters provided in the AI safety  
2550  requirement allocated to the AI component, if applicable.  
2551  12.3.5 Test cases of an AI component shall adequately verify the AI safety requirements allocated to the AI  
2552  component within the specified input space of the AI system.  

2553 NOTE 1 AI test quality and safety-aware AI testing are considered for these test cases. AI test quality refers to the 2554 necessity of rigorous testing of AI models that goes beyond any simple mean performance calculation with a single test 2555 dataset. Safety-aware testing refers to testing that uses safety-aware metrics and safety-relevant data points or subsets. 
2556 EXAMPLE 1 Test cases are designed to verify the AI model in terms of out-of-distribution performance and in-2557 distribution performance on samples underrepresented in the training data. 
2558 EXAMPLE 2 Test cases reflect data points contributing to unsafe states of vehicles deductively enumerated by safety 2559 analysis such as design FMEA or FTA and inductively known from past products. 
2560 NOTE 2 For automated driving applications, the completeness and sufficiency of the test cases can be evaluated 2561 considering the acceptance criteria defined in ISO 21448. 
2562 NOTE 3 If the AI task is implemented by multiple AI models, the relevant sub-domain of the input space for each AI 2563 model is defined, e.g. one AI model could be used to explicitly identify vulnerable road users (VRU) while another could 2564 be used to explicitly identify traffic signs, resulting in a relevant input space subdomain VRU and traffic signs. The test 2565 cases would then be more focused on the relevant input space subdomains and less on the overall input space of the AI 2566 system. 
2567 12.3.6 The AI system integration approach shall specify the steps for integrating the individual AI 2568 components hierarchically into higher level AI components until the AI system is fully integrated. 
2569 12.3.7 The AI system integration shall be verified to provide evidence that the hierarchically integrated AI 2570 components, and the integrated AI system achieve: 
2571 a) compliance with the AI system architectural design in accordance with Clause 10; 
2572 b) satisfaction of the AI safety requirements. 
2573 12.3.8 AI system safety validation shall confirm that the safety requirements allocated to the AI system are 2574 fulfilled when the AI system is integrated into the encompassing system. 
2575 12.4 AI/ML specific challenges to verification and validation 
2576 AI systems, especially those developed using data-driven methods pose some unique challenges to verification 2577 and validation. 
2578 ¡ª They lack a precise statement of AI safety requirements. AI systems are often expected to identify and 2579 quantify human-interpretable high level semantic concepts like road objects, traffic signals, attendant 2580 driver, etc. Many of these concepts lack precise definitions and are difficult to capture in terms of 2581 mathematical descriptions..
2582 ¡ª The inputs to an AI system are often versatile/diversified and from different sources, e.g. radar, LiDAR, 2583 camera etc. whose representation requires high dimensional objects with a very large range of values and 2584 satisfying complex and unknown constraints. Use of traditional input coverage methods would be 2585 incomplete or expensive. 
2586 ¡ª AI systems involving Deep Neural Networks employ complex architectures, especially for perception-2587 based applications, e.g., Long Short-Term Memory (LSTM) networks, encoder-decoder networks and 2588 contain several layers with millions of parameters whose values are tuned during the training process. 2589 Verification, amounting to checking that these parameters have the desired values for optimum 2590 performance and to satisfy AI safety requirements, leads to scalability issues for many realistic 2591 applications. 
2592 ¡ª Training AI models involves the use of various heuristics for identifying parameter values to optimize 2593 appropriate cost functions. These heuristics could lead to locally optimum parameter values, without 2594 achieving a globally optimum solution (i.e. model generalization) that can lead to AI errors in the AI 2595 system. The verification task involves checking that the computed parameters are adequate to satisfy the 2596 AI safetyrequirements which isa challenging problem..
2597 ¡ª AI systems rely on a large dataset for their reliable performance. Demonstrating the validity and 2598 completeness of the verification dataset is a non-trivial task given the complexity and scope of the input 2599 space. 
2600 ¡ª The non-predictable erroneous behaviour in data-driven AI systems, for example, based on spurious 2601 correlations, limit the ability to predict performance based on a review of the training data or the 2602 implemented model. 
2603 NOTE This property is closely related to the property of robustness. This challenge can be further exacerbated 2604 by the lack of explainability of the trained function when using technologies such as deep neural networks. 
2605 ¡ª Limitation of structural coverage: Due to the lack of a detailed specification as well as the dependency on 2606 parameter values (e.g. weight in a NN) during execution, both black-box and white box coverage metrics 2607 have limited use when extrapolating the results of executed tests in evaluating performance over the 2608 entire input space. 
2609 ¡ª Stability of performance due to changes in the environment or the function. Small changes in the input 2610 space (e.g., one or two pixel values in an image input) can lead to, as yet undiscovered, AI errors in the 2611 function. Furthermore, changes to the function (due to re-training) can lead to an unpredictable impact on 2612 previously verified properties. 
2613 ¡ª An AI system while training with an inadequate number of examples can reach a local optimum that results 2614 in behaviors not aligned with the desired outcome. 
2615 12.5 Verification and validation of the AI system 
2616 12.5.1 Scope of verification and validation of the AI system 

2618 Figure 12¨C2 ¡ª Phases of verification and validation activities of the AI system 
2619 Figure 12¨C2 shows the phases where verification and validation activities of the AI system happen. 2620 Verification of the AI system is applicable to the following phases of the AI system safety lifecycle. 
2621 a) When defining the AI safety requirements (Clause 9), verification ensures that the AI safety requirements 2622 are correct, complete, and consistent with each other and with respect to the encompassing system technical 2623 safety concept and safety requirements. Verification of the AI safety requirements can be performed following 2624 ISO 26262-8, Clause 9 and ISO 21448. 
2625 b) During the development phase of the AI system, verification is conducted in different forms, as described 2626 below: 
2627 ¡ª During the design phase (Clause 10), verification is the evaluation of the work products, such as 2628 architectural design, models, or architectural measures, thus ensuring that they comply with the AI safety 2629 requirementsfor correctness, completeness, andconsistency. Evaluation canbe performed bymethods 2630 such as review, simulation, or analysis. It is planned, specified, executed, and documented in a systematic 2631 manner following ISO 26262-8, Clause 9. 
2632 ¡ª In the data lifecycle (Clause 11), data is verified at each phase for sufficient correctness, consistency and 2633 completeness. 
2634 ¡ª In the test phase, verification of the AI system is the evaluation of the work products and elements within 2635 a test environment to ensure that they comply with the AI safety requirements. The tests are planned, 2636 specified, executed, evaluated and documented in a systematic manner. 
2637 Testing of an AI system is performed at different levels of system integration. 
2638 ¡ª AI component testing (described in Clause 12.5.2); 
2639 ¡ª Testing of the integrated AI system (described in Clause 12.5.4); 
2640 ¡ª Integration of the AI system with the encompassing system and AI system safety validation (described in 2641 Clause 12.5.7); 
2642 ¡ª Post-deployment validation (described in Clause 14). 
2643 In 12.5.2 to 12.5.7, some guidance on the verification and validation of AI systems is provided whilst 2644 addressing AI/ML specific challenges. 
2645 12.5.2 AI component testing 2646 12.5.2.1 Testing workflow of an AI component 

2648  Figure 12¨C3 ¡ª Testing workflow of an AI component  
2649  Figure 12¨C3 shows a typical workflow for the testing of an AI component, based on the process presented in  
2650  [20] for the testing of ML-based systems. Initially, the AI safety requirements allocated to the AI component  
2651  are analysed to select the most appropriate combination of test methods. The AI safety requirements can also  
2652  provide direct input to test case generation approaches which involve (automatically) deriving test cases for  
2653  the AI component under test. The result of the analysis is a definition of expectations on the inputs, methods,  
2654  environment and tools for performing the verification.  
2655  NOTE 1 Test planning of an AI component includes gathering information about related test methods, test evaluation  
2656  criteria, such as test coverage or number of tests passed, etc..  
2657  NOTE 2 AI safety requirements include safety properties of the AI system and safety performance indicators.  
2658  This is followed by the formulation of the pass/fail criteria (test oracles) to be applied to the test results. These  
2659  criteria can include a combination of coverage criteria (e.g. from the input space or from the AI model) as well  
2660  as performance targets (e.g. "at most x% of inputs lead to an output with a deviation from the ground truth of  
2661  no more than y%"), and safety-related KPIs.  
2662  Test inputs are sampled from the collected data or generated using simulation or other methods. See 12.5.2.2  
2663  and 11.4 for more details on test input generation. Test cases are evaluated to ensure that they sufficiently  
2664  cover the scenario space considered, and also that they completely verify AI safety requirements within the  
2665  input space of the AI system.  
2666  NOTE 3 Adequacy of the test cases for verification of the AI safety requirements can also be evaluated upon the  
2667  execution of the tests. This is an ongoing area of research and the idea is similar to the structural coverage analysis for  
2668  conventional software. See 12.5.2.3 for more details.  
2669  During the test execution, the AI component is run with the test inputs, and test results are generated. The test  
2670  results are evaluated with respect to the defined test oracles.  

. ISO 2024 ¨CAll rights reserved 
2671  NOTE 4 When evaluating the risk due to AI errors, one can consider the required target metrics and KPIs. It is possible  
2672  that an AI safety requirement is not violated by the occurrence of a single AI error if the corresponding target metrics  
2673  and KPIs (as defined in Clause 9) associated with the AI safety requirement are nevertheless met.  
2674  If a test fails, a safety analysis is conducted to evaluate the impact of the AI error(s) on safety. If the risk due  
2675  to the failed test is deemed unacceptable, the root cause(s) of the AI error(s) are investigated. Then, depending  
2676  on the potential root cause(s), appropriate mitigation measures are applied to reduce the risk. If the AI  
2677  component ismodified asa result of a risk mitigation measure, the AI component will be retested. .The safety  
2678  analysis (including risk evaluation, root cause analysis and identifying mitigation measures) will be discussed  
2679  in more details inClause 13.  
2680  NOTE 5 It is assumed that at thestageof ¡°Test Execution¡±,theAI modelis ready to betested. ForMLmodels, it means  
2681  that the model is already trained and model parameters are set and the model is ready to be tested.  
2682  12.5.2.2 Test generation  
2683  Test generation involves identifying a test dataset that determines whether the AI system meets its AI safety  
2684  requirements. The test dataset contains data elements which are representative of the inputs that the AI  
2685  system receives in operation.  
2686  EXAMPLE An AI system used in a camera-based perception component of an automated driving system might  
2687  receive road scenes as inputs which are dependent upon the ODD of the underlying feature, and the test input scenes  
2688  would be required to have sufficient coverage over the entire ODD.  
2689  NOTE Test Generation for validation at the whole vehicle level is decribed in ISO 34502.  
2690  The AI safety requirements can include some requirements related to AI errors, and testing these  
2691  requirements require generating edge cases based upon the input space definition. Applying methods for the  
2692  systematic exploration of the input space can support an argument that both nominal and edge case conditions  
2693  triggering AI errors are covered during testing. In general, there can be an expected distribution of nominal  
2694  and edge cases over the input space. The test cases are representative of the input space and capable of  
2695  uncovering critical AI errors.  
2696  In general, it is difficult to achieve the desired distribution for the test data when generating the data from  
2697  actual measurements taken from the environment and using the target sensor set. Simulation environments  
2698  can be used to generate additional (synthetic) test data to achieve therequired coverage and distribution over  
2699  the input space. The input space can be described by a set of constraints on the input parameters and the  
2700  desired coverage can be expressed and achieved using techniques such as Design of Experiments. When  
2701  synthetic test data is used, the test evaluation activities include checking the validity of the generated test  
2702  data.Due to the black box nature of certain ML methods (e.g. CNN), formal validation of the generated data is  
2703  not possible. It is also not feasible to determine what features of the data the AI model is sensitive to (e.g.  
2704  shadow, edge softness, blurs). Proving that insights gained on synthetic data can be transferred to real world  
2705  data involves training multiple models on various composition of real and artificial data and comparing the  
2706  performances.  
2707  12.5.2.3 Test case evaluation  
2708  When test cases are generated, they are evaluated to make sure they:  
2709  a) are appropriate and correct to evaluate the AI safety requirements allocated to the AI component, in  
2710  particular regarding the expected outcome (the ground-truth);  
2711  b) adequately cover the AI safety requirements allocated to the AI component within the specified input  
2712  space of the AI system;  
2713  c) effectively cover the scenario space considered.  

2714 In SOTIF, a triggering condition is always characterized by some range of parameters. For example, for a 2715 pedestrian wearing black clothing, there can be different interpretations, reflecting a range of RGB values that 2716 can be defined as black. Another example is the rain intensity, where for heavy rain, the intensity of 2717 precipitation is not a concrete value but instead included in a pre-regulated range. Therefore, the set of test 2718 cases ensures coverage of the range of values within the syntactic space that correspond to triggering 2719 conditions defined in terms of the semantic input space. 
2720 Once the input space has been characterized, methods such as combinatorial testing (see Clause12.5.3) can be 2721 used to perform such a check on relative completeness, where each relevant input dimension is partitioned 2722 into aset of finite valuations(e.g. for¡°fog¡±, further partition the intensityinto three discrete classes¡°strong¡±, 2723 ¡°moderate¡± and ¡°low¡±), followed by ensuring all combinations beingconsidered in theset of test cases. 
2724 NOTE Evaluation of test cases on the whole vehicle level is described in ISO 34505. 
2725 For covering the scenario space, for deep neural networks, a complementary method is to perform 2726 interpretability analysis (e.g. saliency maps) to understand the scenario or feature of why a particular neuron 2727 gets excited or suppressed. This can be used to define a coverage criterion to ensure the relative completeness 2728 of learned-feature combinations (e.g., neuron coverage and its variations or k-way combinatorial testing on 2729 neuron activations). However, achieving full coverage using such methods might not be possible, and methods 2730 to artificially ensure coverage can lead to generating random images that are never observed in practice..
2731 12.5.3 Methods for testing the AI component 
2732 A suitable testing method of an AI component needs to consider, under available resources (e.g., time or 2733 computing power), both the breadth (to efficiently covering the input space) and depth (to enable effective 2734 issue/error detection) of testing. Practically, a portfolio of diversified methods is used for testing AI 2735 components. The following is an incomplete list of methods for testing AI models: 
2736 ¡ª Statistical testing: This method evaluates the achieved values of the metrics defined within the AI safety 2737 requirements and associated safety-related properties within a given confidence interval. Effective 2738 experimental design plays a crucial role in statistical inference (including statistical hypothesis testing) by 2739 safeguarding the validity and reliability of findings, thus preventing the production of spurious or 2740 distorted associations. 
2741 ¡ª Data / scenario replay: Thismethod refersto collectinga set of scenarios(for example, recorded during 2742 test drives) and subsequently using the collected data to stimulate the AI model under test and evaluate 2743 the responses. Examples of these scenarios include known pre-crash scenarios from the National Highway 2744 Traffic Safety Administration (NHTSA) for motion planning [21]or WildDash for perception [22]. 
2745 ¡ª Random testing: This method refers to test cases created based on randomly generated parameters from 2746 the input domain. For automotive, vision-based noise can include Gaussian noise or artificial occlusions. 
2747 ¡ª Metamorphic testing[23], [24]: This method refers to defining metamorphic relations to transform one 2748 test case into another. Metamorphic relations characterize the relationship between the change of input 2749 and the change of output..
2750 EXAMPLE 1 The metamorphic relation might describe that when switching from daytime to nighttime while keeping 2751 the rest of the factors the same, the predicted bounding box size of the pedestrian should be the same. 
2752 ¡ª K-way combinatorial testing[25]against pre-specified input space dimensions[26][27]: This class of 2753 methods is introduced during the definition of test coverage. Dimensions of the input space are analysed 2754 from the input domain to identify equivalence classes in which a uniform behaviour of the system is 2755 expected. Then for any K dimensions, test all possible discrete combinations of those parameters on the 2756 AI model. 
2757 EXAMPLE 2 Consider a simple input domain is characterized using the following dimensions: time-of-day ¡Ê{daytime, 2758 evening, night,dawn},weather¡Ê{fine, cloudy,rainy, snow,fog},androad-intersections¡Ê{lane-diverging, lane-merging, 2759 straight}. 2-way combinatorial testing needs to ensure that the set of collected test cases can cover, for any arbitrary two 2760 dimensions (e.g., <time-of-day, weather> or <time-of-day, road-intersections>), every pair of elements (e.g., <night, 2761 snow> or <dawn, lane-merging>) can be covered with a minimum amount of test cases (e.g., at least 1 test case). Using 2762 combinatorial testing,one can argue the relative completeness of testing efforts. 
2763 ¡ª Boundary value testing or corner case testing: This technique refers to testing boundary values of the 2764 input parameters..Typically, the parametrization of the boundaries of the input domain for complex tasks 2765 can only be accomplished approximately and with significant effort. Thus, testing AI systems at the 2766 boundaries of the input domain requires additional methods as compared to the non-AI case. 
2767 NOTE 1 The equivalence classes within the semantic input space might not correspond to the features within the 2768 syntactic input space learned by the AI model. 
2769 ¡ª Gradient-based search methods or other open-box optimization-based testing methods: This class 2770 of methods utilizes the knowledge of internal model parameters of the ML model to guide the generation 2771 of test cases. One can design optimization objectives such as creating erroneous prediction and, 2772 subsequently, utilize a given input's derivatives (gradient) to move towards the optimization objective. It 2773 is an open-box technique as the gradient is made available to the testing tool. Methods in adversarial 2774 perturbation (e.g., FGSM[28] or PGD [29]) utilize this principle. 
2775 ¡ª Genetic algorithms or other closed-box optimization-based testing methods: In contrast to open-box 2776 testing methods where the gradients are made available, closed-box optimization methods still try to 2777 perform the change of the input without utilizing the gradient information. Genetic algorithms use an 2778 initial population of test inputs and perform mutation of parameters to generate new test candidates. The 2779 next-generation of test cases are based on the pool of current-generation test cases that are best 2780 performers, with the definition of best characterized by the degree of violation. Other closed-box methods 2781 in the falsification of cyber-physical systems (e.g., simulated annealing[30] or Bayesian optimization [31]) 2782 use random samples to guess the gradient direction, in order to transform an input into another that can 2783 lead to undesired situations..
2784 ¡ª Probabilistic sampling-based test methods[32]: These methods assume the availability of some prior 2785 belief about the distribution of AI errors within the input space. The areas with higher AI error distribution 2786 would then be sampled more often with the aim of finding triggering conditions of AI errors more 2787 efficiently. 
2788 ¡ª Synthetic test case generation: The synthetic test case generation allows generating edge cases that can 2789 be dangerous to be reproduced in physical world, as well as creating diversities in the scenarios being 2790 collected. 
2791 ¡ª Testing based on expert knowledge: Knowledge-driven testing refers to applying domain-specific 2792 know-how to create test cases, thereby checking if the model under analysis exhibits performance 2793 limitations that lead to safety concerns. For example, automotive perception component providers 2794 maintain a database of edge cases (e.g. jaywalking on a foggy night, pedestrian walking out of a pile of 2795 snow) that are considered challenging scenarios in products of the previous generation. These edge cases 2796 correspond to potential triggering conditions to the model under analysis. 
2797 ¡ª Tests that analyze resource limitations (e.g. runtime): This method covers activities such as ensuring 2798 that the model can be operated under the specified frequency (e.g., 10 FPS). 
2799 ¡ª Robustness testing: This method refers to considering the application's noise patterns or reasonable 2800 transformations and checking if the prediction subject to noise or transformation produces results 2801 consistent with the input without noise. 
2802  EXAMPLE 3 For an AI component working on audio data, robustness with respect to noise is of interest. In robustness  
2803  testing, noisy audio signals are applied to the component, and it is verified whether the behaviour is as described in the  
2804  requirements, e.g. that the performance does not fall below a certain threshold for noise up to a certain amplitude.  
2805  EXAMPLE 4 Autonomous vehicles rely on their perception capabilities to interact with the surrounding environment,  
2806  which can be influenced by changes such as weather and lighting conditions. During robustness testing, it is important  
2807  to consider adding perturbations to the test data across multiple dimensions simultaneously. For instance, in the case of  
2808  image data captured by cameras, various types and intensities of weather conditions such as dusk and heavy rain can be  
2809  introduced under different lighting conditions. This enables the evaluation of the system's accuracy in perceiving targets  
2810  under challenging scenarios and assesses the model's adaptability to different combinations of disturbances. By  
2811  subjecting the AI system to diverse and realistic environmental variations during testing, the effectiveness and  
2812  robustness of the AI system in handling real-world conditions can be ensured.  
2813  ¡ª Tests based on model analysis/review: The performance on subsets of data is analysed to identify weak  
2814  spots and/or lack of fairness. Subsets of data are identified where an AI component has weak performance.  
2815  The errors are analysed to determine whether this is due to a systematic problem, e.g., low perception  
2816  performance for bright frames because there were no bright frames in the AI training data set.. 
2817  Furthermore, test for potential assumed weaknesses of the model architecture fall into the class of test  
2818  methods. For example, if a DNN for object detection is sensitive to rotated objects due to its architecture,  
2819  this might motivate tests with rotated objects.  
2820  Apart from testing, one can also apply methods that make use of formal verification: Methods making use of  
2821  formal verification can be categorised into those that are sound and complete or just sound.  
2822  ¡ª 
Exact methods (sound and complete) via specialized constraint solvers (e.g. [33]) or a reduction to mixed-
 
2823  integer linear programming or convex optimization.  
2824  ¡ª 
Sound methods based on methods such as abstract interpretation [34][35]. These types of methods can  
2825  guarantee safety when the solver returns "safe". However, when the solver returns¡°unsafe¡± and provides  
2826  a counterexample, the counterexample can be spurious due to over-approximating the state-space in the  
2827  verification process.. 
2828  NOTE 2 For formal verification of deep neural networks, beyond the issue of scalability, the lack of a precise  
2829  specification and characterisation of the input space is one of the critical challenges[36] in the application of formal  
2830  verification approaches to AI-components with high-dimensional input spaces (such as images). Therefore, some state- 
2831  of-the-art approaches on image models, due to inability to mathematically characterize the input space, restrict the use  
2832  of formal verification to the evaluation of robustness against perturbations over selected test samples.  
2833  12.5.4 AI system integration and verification  
2834  Based on ISO 26262-6:2018, Clause 10.2, software integration and verification refers to the activities where  
2835  suitable integration levels and the interfaces between the software elements are verified according to the  
2836  software architectural design. Moreover, ISO 26262-4:2018, Clause 7 discusses system and item integration  
2837  and testing. ISO 21448 (Clause 10.6, Table 10) offers an additional list of methods for integrated-system  
2838  verification. In principle, activities conducted in ISO 26262 and ISO 21448 can also be used to support the AI  
2839  system integration and verification.  
2840  If two components that both contain AI components are integrated and if these AI components are not  
2841  stochastically independent, test methods that address the statistical nature of the AI components are used to  
2842  verify statistical properties of the composition. This can be done using the methods listed in 12.5.3. Stochastic  
2843  independence, in this context, informally means that the correctness of the output of the first AI component  
2844  does not influence the probability for a correct output of the second component. Formally, two events are  
2845  stochastically independent if the probability for the occurrence of both events is equal to the product of the  
2846  probability of occurrence of the individual events.  
2847  EXAMPLE Two object detection components, one based on camera, and one based on LiDAR, are both affected by  
2848  occlusion of objects. If the component using the camera input does not detect an occluded object, it is more likely that the  

2849  component using the LiDAR input also does not detect it. Thus, the probability that both components do not detect a  
2850  certain object is not the product of the probabilities that each component does not detect the object. They are not  
2851  stochastically independent.  
2852  NOTE If statistical properties need to be verified after an integration step, statistical test methods should also be  
2853  employed. In more detail, statistical properties of a component can be verified on component level. However, typically  
2854  there are also statistical properties on higher integration levels or AI system level that are relevant. Often these cannot  
2855  directly be taken over from component level because of the effect of other components.Thus, statistical verification can  
2856  be deferred to higher integration levels or the system level.  
2857  12.5.5 Virtual testing vs physical testing  
2858  ISO/IEC TR 5469 provides a detailed discussion on virtual testing and physical testing for functional safety in  
2859  AI systems. In addition, it provides guidance on how to assess the virtual test platforms.  
2860  This subclause briefly describes challenges with physical testing of AI systems and focuses on some of the  
2861  advantages of virtual testing for AI systems in road vehicles.  
2862  NOTE Requirements for the usage of virtual test platforms in the context of validation of whole vehicle systems are  
2863  described in ISO 34502:2022-4.6.4.3.  
2864  12.5.5.1 Virtual testing  
2865  For AI systems operating within complex environments, there are challenges in physically testing an adequate  
2866  range of use case conditions (e.g., weather effects, behaviour of other road users, etc.), and in particular to  
2867  achieve a sufficient coverage of edge cases. In these particular situations, virtual testplatforms can be used to  
2868  simulate all the desired variations.  
2869  NOTE 1 Edge cases can be defined as scenarios with very specific and rare conditions like for example extreme  
2870  weather conditions, sun glare, specific environment conditions (erased road marking), etc.  
2871  Another capability of virtual testplatforms is the synthetic generation of datasets used for AI system  
2872  development. Synthetic data generation allows for the generation of synthetic ground-truth information  
2873  automatically from virtual test platforms. Indeed, within a simulation environment, not only the virtual image  
2874  as seen by the sensor is simulated, but as an omniscient environment, simulation frameworks can also  
2875  generate additional information (also known as ground-truth data) such as depth, object segmentation, object  
2876  materials, bounding boxes or optical flow. Those data are important in the evaluation of performances of the  
2877  AI models as it provides data as seen from a perfect sensor. Ground-truth data are challenging to obtain from  
2878  real-world observations, and require in most cases a complicated, error-prone, and time-consuming manual  
2879  process. Additionally, generating synthetic data sets allows for the training, validation and testingof AI models  
2880  with independent data, which is key to avoid coincidental correlations, as stated in Clause 11.4.3.2.  
2881  NOTE 2 Independence between synthetically generated data is a complex subject and can be achieved by coverage  
2882  analysis of the different environment parameters (daytime, weather conditions, sensors positioning, etc.), by using  
2883  different sensor types among the available in the catalogues, by using different ground-truth generators, by variation of  
2884  different simulation parameters (for example, scenarios length), by following independent processes when generating  
2885  the synthetic data (for example, relying upon different methods or stakeholders to accomplish process tasks), etc.  
2886  As the dataset generated for AI training would be produced within a virtual environment, the entire generation  
2887  workflow needs to be validated, and correlation with real data has to be made. To make sure we can rely on  
2888  virtual datasets, comparisons between virtual and physical datasets have to be performed. Real world  
2889  conditions, for instance, adverse weather, can be reproduced, tuned, and measured precisely in the laboratory  
2890  (for example, see [37]). Once such data are collected, the same scenario and conditions can be set in the virtual  
2891  environment for advanced correlation. It also helps understanding the gaps and domain of validity.  
2892  Moreover, the use of physics-based solvers on different disciplines (optics, electromagnetic, thermal, etc.) can  
2893  be validated by independent certification bodies. This means being able to handle physics-based data as an  

. ISO 2024 ¨CAll rights reserved 
2894 input (e.g. materials, emitters, sources, etc.), implement laws of physics within the solver but also generate the 2895 outputs that imitates the real conditions (e.g. spectral images, point clouds, range-doppler, etc.). 
2896 Using HiL for synthetic data validation 
2897 Hardware-in-the-Loop testing can be also used to test the accuracy of synthetic datasets ; it is one of the most 2898 accurate ways to correlate between real and virtual datasets. The sensors can be used to record scenes from 2899 the real world and thus, by reproducing and injecting the exact same scene from the virtual world into the real 2900 sensor, comparison between the real and the virtual dataset can be performed (by comparing the results of 2901 the HiL testing) as the only changing parameter is the dataset (sensor receivers and post-processing unit used 2902 in the real and virtual cases are the same). 
2903 12.5.6 Evaluation of the safety-related performance of the AI system 
2904 The performance of an AI system refers to the level of precision for prediction, the accuracy for classification, 2905 and the efficiency of an algorithm. Evaluation of the performance of an AI system is typically carried out with 2906 comparison ofthesystem¡¯soutput to theoutput fromabenchmark,usingadataset whichisproposed as, or 2907 has become, a standard dataset by which different solutions are evaluated. However, the performance of the 2908 AI systemis¡°brittle¡±inthe sense that the AI systemthat performswell hasgenerally either been tailored to 2909 solve particular problems, or trained on specific set of data relating to the problems in a particular domain. 2910 Therefore, the lack of universally accepted or formalized criteria to assess the safety-related performance of 2911 the AI system poses an additional hurdle for widely adoption or utilization of an AI technology. This subclause 2912 aims to provide some guidance that fits into the framework of evaluation process of the safety-related 2913 performance of the AI system. 
2914 NOTE 1 AI models, particularly ML-based models, manifest the characteristics of statistical models. Traditional 2915 functionalsafety requiresthesystem tobepredictable, and hencetheAI model¡¯sbehaviour might bepredictableina 2916 probabilistic sense. It should be noted that predictability does not equate determinism, as it does in traditional software 2917 development. This implies that the AI system can contribute to a failure which is caused by software itself. 
2918 The following are examples of common causes of the negative impacts on safety-related performance due to 2919 AI errors. 
2920 ¡ª Inadequacy and uncertainty in the learning process: The learning process is instrumental for any ML-2921 based system to generate accurate and reliable outputs. Insufficiencies in the training and test data, 2922 dynamically changing environments, and unpredictable intentions of road users, etc. can lead to unreliable 2923 learning results or misinterpretations by the AI system. 
2924 ¡ª Inappropriate cost function selection: The cost function is either less representative, or not affordable to 2925 re-evaluate in a consistent manner. This would result in negative side effects or reward hacking. 
2926 ¡ª Inappropriate metrics: using metrics not suitably matching the actual goals and priorities obscures the 2927 general system performance. 
2928 ¡ª Inconsistency between trained AI model and deployed AI model. 
2929 ¡ª Lack of benchmarks: Lack of universally adopted benchmarks which are reliable, transparent, standard 2930 and vendor-neutral results in performance differences between different parameters, even within the 2931 same application domain. 
2932 The applications of ML-based AI systems are usually categorized as follows: 
2933 ¡ª Regression, where the task is predicting a continuous quantity; and 
2934 ¡ª Classification, where the task is predicting a discrete class label. 
2935 Clearly and unambiguously defined metrics are required to evaluate the performance of an AI system, which 2936 in turn implies the safety level of an AI system (e.g., using performance indicator as a pass/fail criterion of the 
ISO/CD PAS 8800:2024(en)  
2937  system). A summary of the widely-adopted performance metrics for both categories are listed in Annex H. The  
2938  metrics included in Annex H are by no means an exhaustive list of what industry is currently using. Other  
2939  safety-related metrics can be derived, based on particular use cases and domain experts¡¯ knowledge and  
2940  judgement, to evaluate the AI system from specific aspects of the system requirements.  
2941  NOTE 2 The performance metrics included in Annex Hare different from the loss functions. Loss functions are  
2942  measures to quantify the model's performance during training process, while metrics are used to monitor and evaluate  
2943  the performance of trained models in testing phase.  
2944  12.5.7 AI systemsafety validation  
2945  In contrast to verification, AI system safety validation refers to the activity of checking if the safety  
2946  requirements allocated to the AI system (from the encompassing system) are met after the AI system is  
2947  integrated into the encompassing system. AI system safety validation activities are usually done by the system  
2948  integrator (e.g. OEM), where thevalidation target isdefined separately..The AI systemdeveloper might need  
2949  to support the activity.  
2950  For AI system safety validation, the individual methods listed in 12.5.3 can also be used. However, the focus is  
2951  on the systematic exploration of all relevant scenarios within the input space and to examine abnormal  
2952  situations. Systematic random testing by first discretizing the input space [38][39] is an example of such  
2953  methods to argue relative completeness.. 
2954  NOTE If the verification of the encompassing system admits the usage of virtual techniques like simulation, then the  
2955  safety validation of the AI system, once integrated into the encompassing system, can also be based upon virtual  
2956  techniques, for instance simulation can be conducted to systematically explore relevant scenarios and identify corner  
2957  cases or abnormal situations.  
2958  For deep neural networks, AI system safety validation using field testing (e.g., by operating a fleet of  
2959  autonomous driving vehicles) can be done with the assist of active learning methods or other methods for  
2960  detecting out-of-distribution data. The underlying idea is that active learning methods try to infer if an input  
2961  can be included in the training dataset by considering how different is this input with all existing training data.  
2962  As explained in Clause 9, in addition to the standard SOTIF-related AI safety requirements that directly  
2963  address the performance targets, the workflow can introduce additional AI safety requirements that  
2964  concretize AI safety-related properties (e.g. robustness, interpretability). Safety validation of the AI system  
2965  also considers validating the appropriateness of these additionally introduced requirements. The purpose of  
2966  the validation is to ensure that no insufficiencies of the specification exist. In particular, the activity ensures  
2967  that the quantitative thresholds being set in the requirements are appropriate (via methods such as  
2968  hypothesis testing as introduced in statistics [40]) and can positively impact safety (by positively influencing  
2969  the safety-related properties). Since these requirements are not exposed to the system integrators (OEM),  
2970  validating these requirements is the task of the AI component/system provider.  
2971  12.6 Work products  
2972  12.6.1 AI system verification report, resulting from requirements 12.3.1 to12.3.5and 12.3.7.  
2973  12.6.2 Integrated AI system, resulting from requirements 12.3.6.  
2974  12.6.3 AI system validation report, resulting from requirement 12.3.8.  
2975  13 Safety analysis of AI systems  
2976  13.1 Objectives  
2977  The objectives of this clause are:  
2978  a) to identify safety-related faults and AIerrors that can lead to the violation of AI safety requirements;  

ISO/CD PAS 8800:2024(en)  
2979  b) to identify their potential causes;  
2980  c) to support the definition of safety measures to prevent or control safety-related AI errors;  
2981  NOTE 1 These measures can include improving AI design, AI methods, dataset generation, updating AI  
2982  safetyrequirements and related AI system development processes.  
2983  d) tosupport the verification of AI safety requirements, through modification or identification of new AI  
2984  safety requirements on data specifications and collection, design specifications, and test specifications.  
2985  NOTE 2 The objectives, scope, and level of granularity of the safety analysis can depend on the phases of AI safety  
2986  lifecyle.  
2987  NOTE 3 Safety analysis of the AI system complements the safety analysis inaccordance with ISO 26262 and ISO 21448.  
2988  NOTE 4 Safety analysis of an AI system can be performed within the safety analysis of the encompassing system, e.g.  
2989  an item or a vehicle.  
2990  NOTE 5 Dependent Failure Analysis (DFA) is an important activity that follows the safety analysis of an AI system.  
2991  DFA of AI systems is a new area of research, and is not covered in this document. The reader can refer to ISO 26262- 
2992  9:2018, Clause 7 for guidance on DFA, which can be applicable to AI systems.  
2993  This clause aims to provide confidence that the risk of violation of the AI safety requirement at the AI system  
2994  level due to AI errors is sufficiently low, i.e. within the acceptable residual risk.. 
2995  13.2 Prerequisites and supporting information  
2996  The following information shall be available at the initiation of the safety analysis activity:  
2997  a) AI safety requirements, from Clause 9;  
2998  b) input space definition(refined) , from Clause 9;  
2999  c) known insufficiencies of the AI system and the corresponding subdomains of the input space, from Clause  
3000  9;  
3001  d) AI component or AI system architecture (refined), from Clause 10;  
3002  e) dataset requirements specification, from Clause 11;  
3003  f) dataset design specification, from Clause 11;  
3004  g) dataset verification report, from Clause 11;  
3005  h) dataset validation report, from Clause 11;  
3006  i) dataset safety analysis report, from Clause 11;  
3007  j) AI system verification report, from Clause 12;  
3008  k) AI system validation report, from Clause 12.  
3009  NOTE 1 The AI component or AI system architecture (refined) can be used to determine the boundaries of the safety  
3010  analysis.  
3011  NOTE 2 Safety analysis can be performed at different phases of an AI safety lifecycle. Therefore, during early phases  
3012  of an AI system development, availability of the prerequisites can be limited.  

3013 NOTE 3 Safety analysis can be performed at different levels of integration, e.g. AI system, AI components, or AI models, 3014 or with different focus, e.g. architectural aspects, data aspects, or combination thereof. 
3015 





3016 13.3 General requirements 
3017 13.3.1 Safety analysis techniques suitable for identifying the safety-related AI errors of the AI models in AI 3018 systemsshall be applied. 
3019 13.3.2 Safety analysis of the AI system shall identify the AI errors of the AI system and its components that 3020 have the potential to violate one or more AI safety requirements. 
3021 13.3.3 Safety analysis shall identify the safety-related faults, potential functional insufficiencies, and their 3022 potential underlying issues of the identified safety-related AI errors, if one or more AI safety requirements are 3023 violated due to the identified AI errors. 
3024 13.3.4 Safety analysis results shall be used to identify prevention or mitigation measures to address the 3025 causes of the AI errors that are potentially violating one or more AI safety requirements. 
3026 13.3.5 Safety analysis results shall be used to verify the completeness of the AI safety requirements. 
3027 13.4 Safety analysis of the AI system 
3028 13.4.1 Scope of the AI safety analysis 
3029 Safety analysis of AI systems includes a systematic identification of AI errors in an AI system and, in particular, 3030 functional insufficiencies and safety-related faults that could lead to the violation of an AI safety requirement. 3031 These AI errors can be related to: 
3032 ¡ª an AI component consisting of an AI model; 
3033 ¡ª an AI component not consisting of an AI model. 
3034 NOTE Safety-related faults and functional insufficiencies related to an AI component can be originated in that AI 3035 component or be due to the interaction of the AI component with other components within the AI system or outside of 3036 the AI system. 
3037 When AI models are in the scope of the safety analysis, safety analysis addresses the safety-related faults and 3038 functional insufficiencies of the AI models, their causes and their impact on vehicle behaviour. Safety analysis 3039 starts early during the development. Figure 13¨C1 shows a flowchart of a top-down safety analysis approach 3040 in an AI system as an example. In this example, safety analysis is started upon the observation of an undesired 3041 safety related behaviour at the vehicle level, however, in general, safety analysis can start from the level 3042 required determined by the team performing the analysis. In case that safety-related faults and functional 3043 insufficiencies are related to an AI component that does not contain an AI model, safety analysis can be 3044 performed following the requirements and recommendations of ISO 26262-9:2018, Clause 8 and ISO 21448. 3045 It should be noted that during the development of an AI system, other safety analysis methods, for example, 3046 bottom-up approaches can also be used to identify the faults and potential insufficiencies which might lead to 3047 AI errors. 
3048 In general, AI errors of an AI model are either due to issues in data specification and collection or issues in 3049 design and implementation or issues in requirement specification. Safety analysis to identify issues in data 3050 specification and collection and to define safety measures for prevention, or control of safety-related issues is 3051 discussed in Clause 11.4.3 (safety analysis of datasets). 
3052 Safety analysis at the design phase identifies design related issues which can contribute to AI errors violating 3053 an AI safety requirement. Safety analysis at the design phase is discussed in Clause 10. If safety analysis 3054 identifies insufficiencies in an AI safety requirement specification, the requirement specification might need 3055 to be modified or one or more new requirements might need to be added. Requirement modification/addition 3056 follows the guidance provided for requirement modification/addition in Clause 9. 

3057 
3058 Figure 13¨C1 ¡ª An example flowchart for a top-down safety analysis in an AI system upon the 3059 observation of an undesired safety-related behaviour 
3060 13.4.2 Safety analysis based on the results of testing 
3061 If testing of an AI system, at any level, reveals the presence of AI errors, the results of the safety analysis are 3062 used to evaluate the impact of the AI errors on the compliance of the system under test to its AI safety 3063 requirements, to identify the causes of the safety-related AI errors and to define mitigation measures. Here, 3064 safety analysis activities consist of risk evaluation, root-cause analysis and risk mitigation as shown in the AI 3065 component testing workflow in Figure 12¨C3. 
3066 a) Risk evaluation: during this activity, the risk due to the failed test is evaluated to estimate the impact on 3067 safety. In general, if any of the AI safety requirements are violated due to an AI error, it can be concluded 3068 that safety is not achieved. 
3069 NOTE 1 A test can also fail due to the violation of non-safety requirements. In case multiple non-safety requirements 3070 are violated as the result of the failed test, the risk due the violated requirements is evaluated to assess the impact on 3071 safety. 
3072 b) Root-cause analysis: in this step, underlying issues for the AI error(s) are identified. Issues in an AI 3073 model, in general, can be related to many areas including: 
3074 ¡ª AI safety requirements allocated to the AI component consisting of the AI model; 3075 ¡ª AI data including data sets and 
3076 ¡ª AI model design. 
3077 Once a potential category of causes has been identified, a more detailed safety analysis related to that area can 3078 be performed to evaluate the cause in more detail. For this, the results of safety analysis performed during AI 3079 model design or data set generation can be used. For safety analysis on data sets, see Clause 11, and for safety 3080 analysis during AI model design, see Clause 10. 
3081 c) Risk mitigation: When root-causes of the issues are identified, prevention, detection, and/or control 3082 measures regarding the identified root causes need to be defined. These risk mitigation measures include: 
3083 ¡ª Modification/addition/removal of AI safety requirements; 
3084 NOTE 2 For supervised machine learning, the activities in root-cause analysis and the proposal of 3085 modification/addition/removal of AI safety requirements are further detailed in Clause 9.5.3. 
3086 ¡ª Changes in the AI model; 
3087 EXAMPLE Testing of an ML model developed for detecting pedestrians in an autonomous driving system reveals 3088 that the ML model does not detect pedestrians which are standing next to a traffic post. Safety analysis shows that the 3089 hyperparameters of the neural network are not selected optimally. The hyperparameters of the AI model are changed to 3090 mitigate the issue. 
3091 ¡ª Changes in the dataset 
3092 ¡ª Modification of the AI development processes 
3093 Mitigation measures are discussed in more details in Clause 9, Clause 10, and Clause 11. These risk mitigation 3094 measures then need to be implemented as part of the AI systemdevelopment including requirement 3095 derivation, design and dataset creation according to Clause 9, Clause 10 and Clause 11, respectively. This 3096 activity might require the creation of additional safety-related test cases. 
3097 13.4.3 Safety analysis techniques 
3098 Safety analysis techniques should provide adequate identification of hazards and their potential causes. The 3099 sufficiency of a safety analysis technique to model a system is argued by the following methods: 
3100 ¡ª proven-in-use-argumentation; and 
3101 ¡ª critical review of the chosen technique, where pros and cons of the technique for safety analysis of the 3102 system is evaluated and its limitations are identified. 
3103 Since safety of AI systems is a relatively new topic, the proven-in-use-argumentation is challenging to apply. 3104 Safe application of AI is challenging, because AI introduces new classes of mechanisms, and concerns how 3105 risks can emerge. The concerns include inclusion of training data instead of system specifications, no clear 3106 design as system architecture, uncertainties and the explainability challenges in the models' outputs. Some of 3107 the salient features of AI systems that can impact the safety analysis are: 
3108 ¡ª AI systems can behave nonlinearly. Depending upon their current state and context, they might react to 3109 the same inputs very differently. Additionally, smaller disturbances in the input can produce irregular 3110 outputs. 
3111 ¡ª In some cases, the environment that the AI system is deployed in is ever evolving. For example, in case of 3112 highly-automated driving vehicles operating in the open context, new traffic participants can appear over 3113 the course of their operations. 
3114  ¡ª AI systems can produce complex interactions within its elements and with the environment. The models  
3115  that result for such systems might introduce complex correlations.  
3116  Safety analysis techniques analyse the systems with underlying assumptions. Some of the commonly used  
3117  safety analysis methods are shown in Table 13¨C1. These salient features of AI systems require a thorough  
3118  understanding of the safety analysis method selected to analyse these systems. Some of the existing analysis  
3119  techniques have been enhanced to model the AI systems [41], [42] while newer modelling techniques have  
3120  been introduced with stronger assumptions to model the AI systems [43],[44], [45].  
3121  Table 13¨C1 ¡ª Safety analysis techniques  

Safety Analysis  Modelling Assumptions  Advantages  Limitations  
Fault Tree Analysis [41]  ¡ª Independence of events ¡ª Bernoulli model ¡ª Simplified causal relation ¡ª Static temporal concept  Based on Boolean algebraic concepts  Generally static  
Failure Mode and Effects Analysis [46]  ¡ª Single point of failure ¡ª Simplified causal relation ¡ª Static temporal concept  ¡ª Documented process ¡ª Early design decision ¡ª Easy to implement  ¡ª Inability to determine complex failure mode Cause-and-effect chains (might not be predictable)  
¡ª  
System Theoretic Process Analysis [47]  ¡ª Simplified causal relation ¡ª System fails in a certain pattern  Models' interactions Implementation is more comprehensive  Limited keyword set  
Event Tree Analysis[48]  Single point of initiations  Assessment of multiple faults and failure  Probability identification is difficult  
Bayesian Network/ Causal Bayesian Network [44]  ¡ª Model is the best representation ¡ª Probability distributions are known  ¡ª Can model complex relations ¡ª Hybrid modelling is possible ¡ª Models multiple point initiations and failure ¡ª Can handle conditional independence concepts  ¡ª Difficult to model ¡ª Probability identification is difficult  
HAZOP [49]  Single point of initiations  ¡ª Documented process  ¡ª Single point of failure  

Safety Analysis  Modelling Assumptions  Advantages  Limitations  
¡ª Early design decision ¡ª Easy to implement  ¡ª Propagation of failures is not clear ¡ª Identification of causes is weak  







3122 13.5 Work products 
3123 13.5.1 Safety analysis report, resulting from13.3.1 to 13.3.5. 3124 14 Measures during operation 3125 14.1 Objectives 3126 The objectives of this clause are: 
3127 a) to define the process requirements to continuously assureAI safety after deployment, 
3128 b) to use the measures defined in clause 8 and/or additional measures for the identification of safety risk 3129 associated with the AI system during operation and measures to maintain AI safety during operation, 3130 c) to ensure responses are in place to address unacceptable safety risks associated with the AI system and 
3131 ensure re-approval of the modified AI system before release. 3132 14.2 Prerequisites and supporting information 3133 The following information shall be available at the initiation of this phase: 
3134 a) AI system definition (from external sources), including: 3135 1) interfaces with the encompassingsystem; 3136 2) Assumptions on the use of the AI system; 3137 b) field data collected by the encompassing system; 3138 c) safety requirements on the AI system,from Clause 9; 3139 d) AI component or AI system architecture, from Clause 10; 3140 e) dataset requirements specification, from Clause 11; 3141 f) dataset design specification, from Clause 11; 3142 g) dataset maintenance plan, from Clause 11; 3143 h) known insufficiencies of the AI system and the corresponding subdomains of the input space, from Clause 
3144 9 
3145 i) results of verification and validation activities including known functional insufficiencies of the AI system 3146 (if available),fromClause 12; 3147 j) safetyassurance argument, from Clause Clause 8. 
. ISO 2024 ¨CAll rights reserved 
3148 14.3 General requirements 
3149 14.3.1 The process and its activities necessary to assure the AI safety and the validity of the assurance 3150 argument during operation shall be specified. 3151 NOTE 1 This process can include the procedure to terminate the safety support and notify properly to the user of AI 
3152 system regarding this termination. 
3153 NOTE 2 These activities include the identification of safety issues of the AI system during operation and their 3154 resolution procedure. 3155 14.3.2 The on-board and off-board measures necessary to execute the specified activities in 13.3.1 shall be 
3156 developed and implemented. 3157 EXAMPLE Measures can include monitor the operational status of the AI system, detect safety-related errors, etc. 3158 14.3.3 The identified safety-related field events shall be evaluated and, if the risk is deemed unacceptable, 
3159 countermeasures shall be taken to mitigate the risk. 
3160 14.3.4 The effectiveness of the countermeasures shall be evaluated after their application during the 3161 operation phase, and the countermeasures shall be modified if the residual risk is still unacceptable. 3162 14.3.5 The specified maintenance activities during operation shall be executed in order to continuously keep 
3163 AI safety to a reasonable level. 
3164 EXAMPLE Field data collection, AI re-training, re-validation and re-approval, etc. can be executed in order to 3165 continuously keep the AI safety. 3166 14.4 Planning for operation and continuous assurance 3167 14.4.1 Safety risk of the AI system during operation phase 3168 Upon achieving recommendation for release, the residual risk is evaluated to be acceptable based on the 
3169 evidence and assumptions generated during the development phase. However, post-deployment field risk 3170 evaluation could detect an elevated risk associated with the AI system due to hazards resulting from: 3171 a) Development uncertainties, for example: 3172 ¡ª Incorrect estimation of residual risk; 3173 ¡ª Previously unknown hazardous functional insufficiencies; 3174 ¡ª Incorrect estimation of the occurrence of AI related faults occurred during operation; 3175 b) Incorrect unexpected operation-specific activities, for example: 
3176 ¡ª During maintenance, e.g. retrofit camera or radar without recalibration or poor tolerance; 3177 ¡ª During update of the AI system, or update of external systems that interacted with the AI system, e.g. 3178 outdated software version, out of sync of component updates; 
3179 c) Changes in the operation environment, for example: 3180 ¡ª New traffic rules; 3181 ¡ª New traffic facilities; 
3182 ¡ª New type of traffic participants; 
3183 ¡ª Changes of assumptions on the operating conditions. 
3184 NOTE Changes in the operation environment could introduce OOD samples and potentially cause OOD errorsin the 3185 AI system. 
3186 Specific activities can be necessary during the operation phase to address these risks and assure a continuous 3187 level of AI safety. 3188 14.4.2 Safety activities during the operation phase 
3189 Figure 14¨C1 illustrates the flow of the activities of this clause to assure the safety of AI during the operation 3190 phase. 

3192  Figure 14¨C1 ¡ª 
Safety assurance during operation phase2  
3193  Safety assurance during operation starts with applying measures to assure AI safety in the operation phase  
3194  (14.6). Technical measures are applied to monitor the behaviour of AI system during operation phase, and if  
3195  anomalies or undesired safety-related behaviour at the vehicle level are detected, mitigation measures are  
3196  taken (14.6.2). Some additional measures are introduced to address misuse related risk, e.g. user guidance  
3197  (14.6.3). Field data will be collected during monitoring (14.7), in order to support afterwards risk evaluation,  
3198  mitigation and AI system modification.  
3199  Risks identified in the field are evaluated (14.8.1). If it is shown that the level of identified risk is acceptable,  
3200  then no further activities are applied. Alternatively, if the risks are found to be unacceptable, countermeasures  
3201  are  determined based  on  the evaluation results (14.8.2). The AI system  can  go through  re-training,  re- 
3202  validation and re-approval process, if necessary (14.8.3).  
3203  14.5 Continual, periodic re-evaluation of the assurance argument  
3204  Due to the complexity of the functionality to be implemented, the environment in which it is deployed, as well  
3205  as the nature of the AI technologies themselves, some uncertainty in the assurance argument might inevitably  
3206  remain.  
3207  This leads to a residual risk that the safety requirements allocated to the AI system might be violated during  
3208  operation.  This  residual  risk  can  be  related  to  previously  unknown  triggering  conditions  of  residual  
2 Safety assurance activities during operation phase rely on previous clauses, i.e. 12.5 refers to Clause 10, 12.6 refers to  
Clause 11, 12.7 refers to Clause 9Clause 10Clause 12Clause 13Clause 8.  

3209 insufficiencies in the AI system, inadequacies of the assurance argument, or to changes within the operating 
3210 context. A continual, periodic re-evaluation of the assurance argument can offset this emergent risk. 3211 The residual risk can be offset by operational restrictions, until sufficient evidence can be collected to increase 3212 confidence in the assurance argument. Examples of operational restrictions that can be applied include: 
3213 ¡ª restricting the set of operational conditions, thus reducing the risk of violating assumptions in the 3214 assurance argument; 
3215 ¡ª limiting the functionality of the AI system, thus reducing the severity of residual errors. 3216 The re-evaluation of the assurance argument can be performed based on the criteria outlined in 8.7. In 3217 particular, evidence collected during operation can be re-used to provide additional support for claims of the 3218 assurance argument, as well as to identify potential defeaters to these claims. 
3219 EXAMPLE An assurance argument uses a set of assumptions on the input space to define the context of the 3220 assurance argument. A run-time anomaly detection identifies out-of-distribution inputs that were not considered within 3221 the set of assumptions. 
3222 In reaction to this new information, the assurance argument is re-evaluated using a wider set of assumptions, which 3223 includes the identified out-of-distribution inputs. 3224 Triggers for the re-evaluation of the assurance argument can include: 3225 ¡ª periodic review; 3226 ¡ª collection of observations that can be used as additional evidence in the assurance argument; 3227 ¡ª analysis of reported field incidents; 3228 ¡ª results of on-board and off-board monitoring; 3229 ¡ª change in operational parameters or environment conditions; 3230 ¡ª modification or maintenance of the encompassing system and 3231 ¡ª changes in operating procedures. 3232 14.6 Measures to assure safety of the AI system during operation 
3233 14.6.1 General 3234 The intention of this section is to give guidance for applying measures to assure AI safety during operation 3235 and trigger potential updates to the AI system. This section also provides additional non-technical measures, 3236 for example user involvement to prevent the misuse and to assure the safe operation of AI, if possible. 
3237 14.6.2 Technical safety measures 3238 Monitoring, detection, and mitigation, which are used to evaluate the behaviour of AI systems against the 3239 errors or insufficiencies, are measures used to assure safety of the AI system during operation. These 
3240 measures can rely on the on-board mechanisms and/or off-board mechanisms (e.g. cloud monitoring). 3241 NOTE 1 The architectural measures to assure safety of AI during the operation phase are defined in Clause 10 3242 NOTE 2 In contrast with the on-board mechanisms which are used to detect and mitigate abnormal behaviours of AI 
3243 system and the vehicle equipped with AI, the off-board mechanisms can detect abnormal behaviours with higher 3244 accuracy due to, for example, larger computing power and a more precise model. 
3245 NOTE 3 The off-board measures (e.g. cloud monitoring) can also be used to monitor the general behaviours of all 3246 vehicles equipped with the AI systems. These off-board measures can then be used to support the evaluation of the overall 3247 risk after deployment of the AI systems. 
3248 Regarding monitoring and detection, the following events, including related context can be reported with the 3249 purpose of finding insufficiencies or errors of AI system, if applicable: 3250 a) Input space related events: 3251 ¡ª Detection of out-of-distribution-data and data distributional shift; 
3252 ¡ª Detection of exiting from operating context; 3253 ¡ª Detection of conceptdrift or changes in features (e.g. new objects, different behaviours, new or changed 3254 rules, etc.) ; 
3255 b) Model behaviour related events: 3256 ¡ª Detection of abnormal behaviour; 3257 NOTE 4 Some abnormal behaviours could be caused by rare input conditions and these behaviours could be 
3258 evaluated as safe after detection. For example, while reversing, the AI model stopped at a shorter distance than 3259 specified from a parked car. Behaviour is detected by the system and logged. After analysis of the report, the 3260 behaviour is considered safe and an update is not necessary. 
3261 c) Output related events: 3262 ¡ª Detection of abnormal output; 3263 NOTE 5 Exercise caution when implementing plausibility checks as these can lead to missed objects and safety 
3264 concerns (e.g. rejecting humans taller than 7 ft could lead to mis-detection of a pedestrian carrying a flagpole or on 3265 stilts or having a child on their shoulders, etc.) 3266 ¡ª Detection of output bias; 3267 ¡ª Outputs with low confidence level; 
3268 d) Incidents/accidents analysis: 3269 ¡ª Incidents/accidents where the AI system was directly or indirectly involved are analysed to support the 3270 improvement of the AI system. 
3271 NOTE 6 For detected errors or insufficiencies of the AI system which can lead to a hazard, the risk can be mitigated by 3272 measures within the AI system or the encompassing system. For example, switching to non-AI system or executing a 3273 manoeuvre that results in a minimal risk condition. 
3274 As the insufficiencies of the AI system can influence the behaviour of the encompassing vehicle system, any 3275 abnormal behaviours or emergency events of the vehicle might also imply or influence on insufficiencies of 3276 the AI system, for example: 
3277 ¡ª function degradation; 3278 ¡ª take-over request; 3279 ¡ª emergency manoeuvre; 3280 ¡ª transition to a minimal risk condition; 3281 ¡ª collision or near-collision event; 
3282 ¡ª contradiction between AI system and non-AI system 
3283 EXAMPLE The AI system and non-AI systems, which are both used for decision making, mayprovide diametrically 3284 opposed results under an unprotected left-turnscenario,forinstance, onefor¡°yield¡±,and theotherfor¡°notyield¡±. 
3285 Besides triggering modification activities, errors or insufficiencies of the AI system identified during operation 3286 may indicate the weaknessesin the development and safety assurance process, architectural measures or 3287 incorrectness of their usage assumptions, thus modification of these measures may be needed. 
3288 14.6.3 Safe operation guidance and misuse prevention in the field 
3289 The user of the AI system can lack understanding of its capabilities which results in misuse due to 3290 overconfidence in the AI system. As a potential prevention for overconfidence, users are made aware of the 3291 limitations of AI systems via, for example, user training if possible or relevant information through the human-3292 machine interface. 
3293 EXAMPLE The user is trained to correctly use the AI system and be informed of scenarios in which the AI system is 3294 intended for use, considering the performance limitations of the AI system within these scenarios. 
3295 Another possible prevention of misuse are technical measures (e.g., warning or, degradation or disablement 3296 of services) that are triggered when the AI system is misused by the userduring operation. 



3297 14.7 Field data collection 
3298 The intention of this section is to introduce field data collection as a supplementary data source for AI system 3299 maintenance to improve dataset integrity, distribution and usage (see Clause 11) 
3300 NOTE 1 Field data collection is related to AI systems whose safety can be affected by field conditions. An autonomous 3301 driving system that makes use of AI technologies is a typical case and selected as example within this section. 
3302 The motivation to collect field data during operation includes, for example, addressing environment changes 3303 which may affect the behaviour of AI system, identifying and removing residual insufficiencies and collecting 3304 additional training data. The quality of the collected field data needs to be ensured and the data needs to be 3305 transmitted to relevant parties (e.g. manufacturers, suppliers and/or regulators) for use to support the update 3306 of the AI system if necessary. The following topics can be considered when collecting field data: 
3307 a) Competence management 
3308 To ensure the efficiency and quality of the field data collection, competence management measures can be 3309 applied to persons responsible for field monitoring, data collection or data analysis. All systems involved in 3310 field monitoring activities will be tested, validated and released to ensure required reliability level. 
3311 b) Data characteristics 
3312 The data characteristics of the field data can be defined dependent on the planned usage of the data. 
3313 EXAMPLE 1 In some cases, a large number of images of a high resolution are needed in order to improve the 2D image 3314 perception performance of the AI system, such as classifying a certain type of traffic sign. In other cases, the AI-based 3315 image processing algorithm might be dependent upon relationships between sequences of images over time. For such 3316 cases, a minimum length of video sequences along with other associated sensor data and the results of the current 3317 iteration of the AI system are required to improve performance. 
3318 When analysing the field data, the following data characteristics can be considered: 
3319 NOTE 2 The data characteristics given below are not exhaustive. 
3320 ¡ª data categories, 
3321 EXAMPLE 2 Data source (radar, LiDAR, camera or HD map). 3322 ¡ª data content, 3323 EXAMPLE 3 Vehicle Identification Number (VIN), images from front camera, parsing data from front perception, 
3324 changes of control mode, received remote control command, operation status and HMI data. 3325 ¡ª data format, 3326 EXAMPLE 4 JPEG, PNG and BIN. 3327 ¡ª data size. 3328 c) Data collection trigger and transfer 3329 To ensure that the collected data is sufficient to identify, analyse and improve safety-related issues, clear data 
3330 collection triggering criteria are defined including the triggering conditions, triggering interval, start time and 3331 end time, triggering priority according to different cases. 3332 EXAMPLE 5 The triggering rules of field data collection can be: 3333 ¡ª accident or incident: collision event involving the automatic driving vehicle equipped with AI system; 
3334 ¡ª functional termination: autonomous function failure/insufficiencies, terminated by the human taking over; 3335 ¡ª exiting operational design domain (ODD): the specific objects are detected, such as the red light, stop signs, etc. which 3336 are not within the ODD scope for a highway pilot feature, the value reported by rain sensor exceeds the threshold for 3337 a feature designed for no or small rain weather; 
3338 ¡ª implausible events: the distance or speed jitter of the detected object exceeds a certain value, the distance deviation 
3339 of the detected object is greater than a certain value measured by different sensors; 3340 ¡ª other functional insufficiencies: the target motion predicted by the AI algorithm is inconsistent with the actual 3341 situation; 
3342 ¡ª diverging decisions of redundant diverse AI elements or between AI and non-AI based elements. 3343 NOTE 3 Triggering rules can be updated over-the-air (OTA) in order to collect different kinds of data. To collect 
3344 sufficient data for each event, a timing buffer can be considered, for example: recording starts from at least X s before the 3345 event to at least Y s after the event. 3346 When transferring the data, the conditions that may affect the reliability of the transfer are considered, for 
3347 example: the data transfer is interrupted by loss of power and may therefore lead to loss of data. 3348 d) Data storage 3349 To ensure the integrity of data storage, safety mechanisms are implemented where reasonably practicable, for 
3350 example, adding data integrity protection. The operation conditions that may influence the data storage are 3351 also considered. 3352 NOTE 4 The general data storage requirements used for AI data collection can also be used for field data storage. 
3353 e) Configuration information 3354 To ensure correctness of data collection, the configuration information about the field data to be collected is 3355 specified, which may include access rights, tools and repositories, and aligned with the requirements for 3356 datasets (see 11.3) 
3357 14.8 Evaluation and continuous development 
3358 14.8.1 Field risk evaluation 
3359 Based on field measures (14.6) and data collected (14.7), the accidents, anomalies and undesired safety-3360 related behaviour at the vehicle level potentially related to AI systems can be manually or automatically 3361 reported to the manufacturers or service providers. The number of reported issues might be large during the 3362 early phase after deployment. In order to solve the reported issues efficiently and economically, the 3363 manufacturers or service providers investigate the causes and evaluate the field risk of the issues, to 3364 determine the proper reactive actions to be taken, such as recall or OTA update. 
3365 The field risk evaluation is different compared to the hazard risk evaluation during the development phase. In 3366 particular field risk evaluation is based on the real consequence of issues occurred during the operation 3367 instead of assumptions or estimations made at development phase. 
3368 To objectively evaluate the effects of the issues, the probability of occurrence, the severity and the 3369 effectiveness of countermeasures addressing the risk of the existing issues can be considered. This is similar 3370 to the occurrence, severity and detection parameters used by FMEA method for a systematic evaluation of 3371 risk. 
3372 NOTE 1 Alternative risk evaluation methods to FMEA based on a systematic methodology and predefined criteria can 3373 also be applied. 
3374 a) evaluation of the probability of occurrence 
3375 As described in Figure 6¨C12, safety-related issues of the AI system can be caused by random hardware faults 3376 and/or systematic factors (e.g. systematic faults or functional insufficiencies). 
3377 ¡ª For issues associated with random hardware faults, the occurrence considered is determined by the failure 3378 rate and the probability of exposure to a hazardous scenario which has been considered by ISO 26262; 
3379 ¡ª For issues associated with systematic faults or functional insufficiencies, the risk is mainly determined by 3380 the probability of the exposure to the critical situations or probability of triggering events; 
3381 ¡ª As the quantity and location of the vehicles can be known at this phase, it is possible to provide the 3382 occurrence with higher accuracy than during development. 
3383 EXAMPLE The occurrencerate of the issue over a given time period can be predicted based on the failure rate of the 3384 component, the quantity of vehicles in the field and the probability of the vehicles facing the hazardous scenarios. 
3385 b) evaluation of the severity 
3386 The severity evaluation can be based on the method defined by ISO 26262-3, which recommends Abbreviated 3387 Injury Scale (AIS) ranking method. 
3388 NOTE 2 In addition, other issues that can cause loss due to cybersecurity risk, violation of traffic rules or serious 3389 customer complaints might also be considered. 
3390 c) evaluation of the detection and mitigation measures 
3391 The measures in 14.6 can help to detect and mitigate the risk of errors in the AI system. The potential 3392 controllability by the driver can also be considered as a mitigation of the issue, if the field data shows relevant 3393 evidence. 
3394 It is possible to give risk evaluations based on the factors above in a qualitative way or quantitative way (if 3395 rates are defined for each factor). The evaluation results will support theidentification of the response actions 3396 to be taken. 
3397 NOTE 3 For serious accidents (e.g. fatalities), even if the occurrence had been rated as low or detection as high based 3398 on predefined criteria, the rating criteria can be adjusted and risk can be considered differently. 
3399 14.8.2 Countermeasures addressing field risk 
3400 The safety development of AI systems does not end after the safety release. The field risks could be higher 3401 than expected in case the on-board measures cannot detect and mitigate all risks. If hazardous events occur 3402 in the field, the following additional countermeasures can be taken: 
3403 ¡ª Issue investigation actions to determine the causes of risk, e.g. scenario reconstruction based on the data 3404 collected, especially for AI-related incidents or accidents; 
3405 ¡ª Risk evaluation as introduced in 14.8.1; 
3406 ¡ª Restrictions on context of use or functionality deactivation or replacement; 
3407 ¡ª Update of the AI system, for example over-the-air (OTA) update, when unacceptable systematic faults or 3408 insufficiencies are identified 
3409 ¡ª Customer notification, which can be taken together with the restrictions on context of use or AIsystem 3410 update actions, or dedicated notification to address misuse risk, e.g. emphasizing the operation 3411 requirements to the passengers by placing a warning card in the robotaxi. 
3412 NOTE Depending on the urgency of identified field risks, immediate actions or long-term actions could be taken 3413 based on risk evaluation. 
3414 An appropriate issue management process is important to ensure the effectiveness of countermeasures, 3415 including incidents or accident reporting, issue investigation, risk evaluation and countermeasure 3416 management processes. 
3417 The effectiveness of the countermeasures taken are monitored and evaluated after implementation and 3418 adjusted, if the risk is still unreasonable. 
3419 14.8.3 AI re-training, re-validation, re-approval and re-deployment 
3420 The system can be incrementally developed onthe basis of collected field data and countermeasures to 3421 compensate for the identified risks, making the system safer and more robust. The intention of this section is 3422 to introduce AI model re-training, re-validation, re-approval and re-deployment. 
3423 NOTE AI system update and re-approval involves the activities described in Clause 7toClause 13, if relevant. 
3424 a) re-training 
3425 During operation, valuable field data can be collected. Together with the data from the previously trained 3426 model, this data can be used to re-train the new model with the expectation of better performance. Re-training 3427 can be achieved by fine-tuning the pre-trained model or by training from scratch. 
3428 ¡ª Fine-tune: fine-tuning refers to small adjustments to model parameters. The newly acquired field data can 3429 be used to fine-tune the released model. When fine-tuning, a small learning rate is used so as not to over-3430 distort the existing model. 
3431 EXAMPLE 1 For some DNN specific multi-task networks, it is often the case that only one specific task head needs to 3432 be fine-tuned while freezing the backbone and other task heads, for example, only the detection head can be fine-tuned 3433 when input training data are labelled for detection. 
3434 ¡ª Train from scratch: usually after a long period of operation, all parameters of the model can be randomly 3435 initialized to re-train the model from scratch. This approach is expected to get better performance than 3436 fine-tuning in the original model. However, compared to fine-tuning, this method requires more data 3437 volume, computing time, and computing resources. Using a pre-trained backbone is a common method. 3438 Applying an existing backbone to train on the desired task can reduce the computational cost and speed 3439 up the convergence. 
3440 b) re-validation 3441 After re-training, the updated AI model is integrated into AI system, and re-validated to provide evidence that 3442 the safety-related issues are solved and all relevant safety requirements are met. 
3443 EXAMPLE 2 The datasets of known issues can be used to re-validate the updated AI system in virtual or real-world 3444 testing, or a combination of both, and to demonstrate the absence of safety performance degradation, if applicable. 3445 c) re-approval and re-deployment 3446 After an update to the AI system , the safety assurance argument is re-evaluated (see clause 11). Once re-
3447 approved, the AI system update can be deployed. 3448 14.9 Work products 3449 14.9.1 The specification of the process and its activities for assuring AI safety during operation, 3450 resulting from 14.3.1. 3451 14.9.2 The specification of the necessary off-board and on-board measures, resulting from14.3.2. 3452 14.9.3 Field data and functional insufficiencies detected during operation, resulting from 14.3.3. 3453 14.9.4 Evidence of the effectiveness of measures for ensuring AI system during operation, resulting 3454 from 14.3.4. 3455 14.9.5 Evaluation report of functional insufficiencies detected during operation, and updated version 3456 of the safety assurance argument if applicable, resulting from 14.3.3 and 14.3.5. 






3457 15 Confidence in use of AI development frameworks and software tools used for AI 3458 model development 3459 15.1 Objectives 
3460 The objective of this clause is: 3461 a) to provide requirements and guidance to identify, mitigate and document possible sources of errors and 3462 inappropriate biases in the off-line processes, tools and principles used to develop, verify and deploy 3463 safety-related AI models. 3464 15.2 Prerequisites and supporting information 
3465 The following information shall be available at the initiation of this activity: 3466 a) documentation of development processes and tools used within the AI safety lifecycle (from external 3467 sources); 3468 b) AI system-specific development measures and procedures (from Clause 7to Clause 14). 3469 15.3 General Requirements 3470 15.3.1 Processes,tools, and work products used for the development of safety-related AI models shall be 3471 analysed to identify, mitigate and document possible sources of errors. 
. ISO 2024 ¨CAll rights reserved 
3472  EXAMPLE Errors can be caused by inappropriate biases in processes such as field data collection, labelling,  
3473  sampling, tools such as data processing, deep learning frameworks, work products such as data, AI models.  
3474  NOTE The approaches discussed in ISO/IEC TR 5469:2024 11.5.3 and ISO 21448:2022 D.2.5 canbe used to analyse  
3475  offline training processes.  
3476  15.3.2 Confidence shall be demonstrated that software tools used to develop, verify and deploy safety-related  
3477  AI models are suitable to be used to support activities or tasks required by this document.  
3478  NOTE ISO 26262-8:2018 Clause 11 canbe used to demonstrate confidence in the use of software tools.  
3479  15.3.3 Appropriate principles for data-driven AI models shall be applied to training and evaluation to ensure  
3480  control or avoidance of safety-related faults in the AI models.  
3481  NOTE A specific level of robustness and quality in software, including data-driven AI models, is essential for assuring  
3482  safety. Design principles that govern software unit design and implementation at the source code level, such as enforcing  
3483  a single entry and exit point in subprograms and functions, have traditionally been employed to attain the desired quality  
3484  and robustness in conventional software. However, these principles are applicable solely to the software implementation  
3485  aspect of data-driven AI models that have been trained and evaluated on data. Therefore, appropriate principles  
3486  are necessary for ensuring the requisite level of robustness and quality in the training and evaluation aspects of data- 
3487  driven AI models.  
3488  EXAMPLE The influencing factor classes listed in Table 9¨C3and their managing approaches elaborated in Table 9¨C1  
3489  can be used as principles.  
3490  15.4 Confidence in the use of AI development frameworks  
3491  A robust process used to develop AI models will reduce the risk of introducing errors in the development of  
3492  the AI system, thereby making the AI system safer. Specific analysis depends on each system, e.g., automated  
3493  emergency braking systems and driver status monitoring systems. Typically, AI models are developed using  
3494  a multi-step process such as the onegiven in Figure 15¨C1. The offline training used in the process can be a  
3495  source of errors in the final AI model. ISO/IEC TR 5469:2024 11.5.3 PFMEA of offline training of AI technology  
3496  proposes the use of a Process Failure Mode and Effects Analysis (PFMEA) to analyse AI offline training. ISO  
3497  21448:2022 D.2.5 Analysis of the off-line training process of machine learning algorithms describes a similar  
3498  analysis approach used for SOTIF issues.  


3500  Figure 15¨C1 ¡ª Example offline multistep AI training process for PFMEA  
3501  PFMEA is a well-known technique in the automotive industry [50]. PFMEA is an inductive method often  
3502  applied to manufacturing processes. The analogy is that the offline training process is "manufacturing" an AI  
3503  model and many of the benefits of a PFMEA apply. It is beyond current technology to trace AI¡¯s safety-related  
3504  systematic issues to root causes, e.g., training and deployment errors, SOTIF issues, etc. Therefore, the overall  
3505  integrity of training processes, which can be a source of errors in the final AI model, is analysed during AI  
3506  development. The perspectives used in the safety analysis of systems, e.g., four influence factor classes  
3507  described in Table 9¨C1, are connected to PFMEA.  
3508  A PFMEA finds failure modes in each element of the AI training processes. Then, their effects on subsequent  
3509  processes and countermeasures to detect such failures  are  reviewed. Table 15¨C1describes examples of  
3510  potential failure modes and effects in the AI training processes of Figure 15¨C1which could result in a safety-
 
3511  related systematic issue, i.e., performance insufficiencies and safety-related classic systematic faults  as  
3512  included in Figure 6¨C12.  
3513  Table 15¨C1 ¡ª Example of potential failure modes and effects in PFMEA  

Process  Potential failure modes  Potential effects  
Data acquisition Description: Process step for collecting data to be used in model  Specific scenes are missing intest datasets.  The model has degraded performance in scenarios involving missing scenes.  
training and test.  Test data coverage is biased.  In the model evaluation process, evaluation results are biased.  
Only a small number of routes are planned for data collection, and the collected training and test datasets lack variation.  Due to lack of variation, the model training process results in low-performance models, and test results are unreliable in the model evaluation process.  
Unintended data collection scenarios are used, and the collected datasets haveinappropriateattributes (meta  The mixture of training data samples using data attributes in the model training process and scene-wise evaluation based on data attributes  

labels), e.g., weather and time of the day.  in the model evaluation do not work as intended.  
Data ingestion Description: Process step for uploading collected data to servers used for off-line ML model training  Data is corrupted during upload from data collection vehicle to cloud storage  Corrupted or lost data during training  
Data curation Description: Process step generates input data set for further labelling and to be used for training  Curation recognises edge cases as outliers and unintentionally excludes them.  In the model training process, the trained models have performance degradation for these edge cases.  
Data labelling Description: Process step identifies and labels objects within the data set to be used for model training  Objects carried or pushed by pedestrians may be included or excluded in the bounding box, leading to inconsistent labelling.  In the model training process, the trained models perform differently for different labelled objects.  
Labeller has bias (e.g. omits to label motorcycles)  The training models have performance degradation due to bias.  
Displayed labels and recorded labels are different in data labelling tools.  In the model training process, the trained models learn the wrong labels.  
Model training Description: Process step to create trained model from labelled data (e.g. Figure 11¨C2)  Static random seeds are used during the development for debugging, and these are left in the production code.  In the model training process, random number generators do not work appropriately, and machine learning and hyperparameter optimisation frameworks do not work as intended. As a result, the trained models have consistently low performance.  
Unintended training and test data sets are loaded.  Within the model training process, the trained model is optimised to different contexts, and the trained models have consistently low performance.  
Model evaluation Description: Theprocess step verifies whetherthe model meets KPIs. A decision is then made to continue with more training, collect more data or end training  The harmonic mean of precision and recall is specified as an evaluation metric, but only recall was evaluated.  As a result, the trained models become recall oriented, i.e., many false positives and few false negatives, which does not meet system requirements.  
Training datasetsare leaked to AI test datasets. AI testdatasets are not covering the ODD in a suitable manner.  In the model evaluation process, the evaluation results are not reliable or are overestimated.  

3514  Each step of the process can be further broken down (e.g. model training broken down to flow of Figure 11¨C 
 
3515  2) for a more detailed analysis.  
3516  Theprocessanalysismay beginassoonasone hasabasicunderstandingof theconsidered process¡¯sinputs,  
3517  outputs,  and  internal  architecture,  even  if  that  means  proceeding  without  a  complete requirements  
3518  specification or a complete architecture specification of the process. This iterative process analysis may lead  
3519  to the specification of additional requirements and process updates. The analysis completes by considering  
3520  the fully refined architecture and requirements. Even though the analysis focuses on the process for the  

3521 creation of the AI model, the process analysis can be iterative and may also influence architectural and design 3522 decisions. 
3523 Example information to start a PFMEA: 
3524 ¡ª Process flow diagram(s) showing the entire AI model creation process flow 
3525 ¡ª Block diagram of individual process steps including internal process steps showing major components of 3526 the process step 
3527 ¡ª Boundary showing what is the scope of the analysis, the neighbouring process steps to the considered 3528 process element 
3529 ¡ª Identified purpose(s) of the individual steps 
3530 ¡ª Conceptual data flow(s) between the considered process step, its neighbouring steps, and its internal 3531 components 
3532 ¡ª List of tools used in the AI model creation process 
3533 ¡ª Training and evaluation requirements 
3534 15.5 Confidence in the use of tools used to support the AI-safety lifecycle 
3535 The training of AI models often involves tools (e.g. data labelling tools, machine learning frameworks, and 3536 hyperparameter optimisation frameworks) to train or optimize models. Tools may also be used in the labelling 3537 and curation of data along with other steps in the training process. These tools are potential sources of training 3538 and deployment errors. ISO 26262-8:2018 Clause 11 can be used to ensure that the tools do not cause an 3539 unreasonable safety risk. 
3540 15.6 Principles for data-driven AI model training and evaluation 
3541 Training and (data-driven) evaluation form key parts of the development of data-driven AI models. The causes 3542 of insufficiencies of data-driven AI models are classified into the influencing factor classes (Table 9¨C1), as 3543 depicted in Figure 9¨C3. The certainty in influencing factor classes during the AI model development process 3544 ensures the development quality of data-driven AI models. Uncertainties in influencing factor classes can 3545 impact a multi-step process such as given in Figure 15¨C2. Table 9¨C1elaborates on the approaches to manage 3546 the certainty of influencing factor classes, and these can be used as the principles for data-driven AI model 3547 training and evaluation. 

3549 Figure 15¨C2 ¡ª Example offline multistep AI training process and influencing factor classes 
3550 15.7 Work products 3551 15.7.1 Evidence for the analysis of the AI model creation processes,resultingfrom 15.3.1. 3552 15.7.2 Evidence for the confidence in the software tools,resulting from 15.3.2. 3553 15.7.3 Evidence for the execution of the AI model creation processes with the principles,resulting from 
3554 15.3.3. 
. ISO 2024 ¨CAll rights reserved 
3555  Annex A  
3556  
3557  
3558  Overview and workflow of ISO PAS 8800  

3559 Table A¨C1 ¡ª Summary of the normative clauses of this document 
Clause  Objectives  Pre-requisites  Work products  
Clause 7) AI safety  a) to define an AI safety  a) the AI system definition  7.6.1 AI safety lifecycle  
management  lifecycle and its activities to ensure that contributing errors of the AI system do not lead to unreasonable risk of undesired safety-related behaviour at the vehicle-level;  (from external sources), including: 1) the AI system functionality; 2) the interfaces of the AI system with the encompassing system,  resulting from 7.3.1 to 7.3.4. 7.6.2 Work products of ISO 26262-2:2018, 5.5, resulting from 7.3.4. 7.6.3 Work products of ISO 26262-2:2018, 6.5,  
b) to ensure that overall and project specific safety management processes and activities are appropriate to ensure the safety of the AI system;  including if applicable, the ASIL capability of the inputs to the AI system. 3) the safety requirements allocated to the AI system, including if applicable:  resulting from 7.3.3 and 7.3.4, in particular the safety plan. 7.6.4Work products of ISO 26262-2:2018, 7.5, resulting from 7.3.4.  
c) to plan, initiate and conduct the AI safety activities.  i) the ASIL value of the safety requirements; ii) the acceptance criteria or validation targets derived in compliance with ISO 21448:2022, Clause 6 or 9.  
Clause 8) Assurance  a) to develop an assurance  a) the AI system definition  8.8.1Safety assurance  
arguments for AI  argument demonstrating  (from external sources),  argument, resulting from  
systems  that the safety requirements allocated to the AI system are fulfilled; b) to evaluate whether the assurance argument reflects the actual residual risk of the AI system violating its safety requirements;  including: 1) a specification of the safety requirements allocated to the AI system; 2) a definition of the technical context within the encompassing system (e.g. definition of interfaces, conditions under which the AI system functionality is triggered, etc.); 3) a specification of the input space; b) requirements on the assurance argument and work products for the AI system (from external sources). These requirements can be derived from the  8.3.1, 8.3.2. 8.8.2Confirmation measure reports, resulting from 8.3.3.  

Clause  Objectives  Pre-requisites  Work products  
assurance argument of the encompassing system as well as safety management procedures from Clause 7; The following information shall be available for the finalization of these activities: c) the work products of the AI safety life cycle;  
Clause 9) Derivation of  a) to specify a complete  a) AI system definition  9.6.1Input space  
safety requirements on  and consistent set of safety  (from external sources),  definition (refined),  
AI systems  requirements on the AI system, that are sufficient to ensure AI safety; b) to refine AI safety requirements based on learnings from development, verification and validation;  including: 1) safety requirements allocated to the AI system; 2) input space definition; 3) functional requirements; 4) impacted stakeholders;  resulting from 9.3.1 and . 9.6.2AI safety requirements, resulting from9.3.2, 9.3.3, 9.3.4, and 9.3.6. 9.6.3Known insufficiencies of the AI system and the  
c)to specify the limitations of an AI system over its input space to be escalated to its encompassing system development process.  5) the interfaces of the AI system with the encompassing system, including if applicable, the ASIL capability of the inputs to the AI system; 6) interfaces to the environment, if applicable.  corresponding subdomains of the input space, resulting from9.3.5.  
Clause 10) Selection of AI  a) to select and justify  a) safety requirements on  10.6.1AI component or AI  
Technologies, AI  appropriate AI  the AI system, from Clause  system architecture  
Measures and design- technologies for use in the  9:  (refined), resulting from  
related considerations  AI system;  b) training and validation  10.3.1 to 10.3.11.  
b) to identify appropriate  datasets, from Clause 11;  10.6.2AI component or AI  
architectural and  c)AI component or AI  system development  
development measures to  system architecture, if  process (refined),  
fulfil the safety  already existing;  resulting from 10.3.1 to  
requirements prior to deployment;  d) AI component or AI system development  10.3.11. 10.6.3Implemented AI  
c)to identify appropriate  process, if already existing.  component, resulting  
architectural measures to  from 10.3.12.  
mitigate residual  
functional insufficiencies  
of the AI system revealed  
after deployment;  
d)to identify measures for  
ensuring the safety  
requirements of the AI  
system are fulfilled within  
its target execution  
environment.  
Clause 11) Data-related  a)to define the dataset  a)AI system definition,  11.5.1Dataset life cycle,  
considerations  lifecycle of activities related to the gathering, creation, analysis,  including:  resulting from 11.4.2  

Clause  Objectives  Pre-requisites  Work products  
verification and validation, management, and maintenance of the datasets used in the development of the AI system; b)to identify the dataset insufficiencies that may impact the safety of the AI system; c) to identify the data-related safety properties that have a bearing on the safety of the AI system and that support dataset safety analysis; d)to define the countermeasures to prevent or mitigate dataset insufficiencies using dataset safety analysis methods at different steps in the dataset lifecycle; e)to define the data-related work products that support providing evidence of the safety of the AI system.  1) AI safety requirements, from Clause 9; 2) input space definition(refined), from Clause 9; b)field data and functional insufficiencies detected during operation, from Clause 14; c)safety analysis report, from Clause 13.  11.5.2Evidence for the outputs of the defined phases of the dataset life cycle, resulting from 11.3.3. 11.5.3Evidence for the safety analyses of the dataset, resulting from 11.3.4 and 11.3.5. 11.5.4Dataset requirements specification, resulting from 11.3.6 and 11.3.7.  
Clause 12) Verification and validation of AI systems  a) to verify that the AI system fulfils its AI safety requirements; b) to validate that the safety requirements allocated to the AI system are achieved when integrating into the encompassing system;  a) Safety requirements allocated to the AI system (from external sources); b) AI safety requirements, from Clause 9; c) Known insufficiencies of the AI system and the corresponding subdomains of the input space, from Clause 9; d) Input space definition (refined), from Clause 9; e) AI Component or AI System Architecture, from Clause 10; f) Implemented AI component, from Clause 10; g) Dataset lifecycle, from Clause 11; h) Evidence for the outputs of the defined phases of the dataset lifecycle, from Clause 11;  12.6.1AI system verification report, resulting from requirements 12.3.1 to 12.3.5 and 12.3.7. 12.6.2 Integrated AI system , resulting from requirements 12.3.6. 12.6.3AI system validation report, resulting from requirement 12.3.8.  

Clause  Objectives  Pre-requisites  Work products  
i) Evidence for the safety analyses of the dataset, from Clause 11; j) Dataset requirements specification, from Clause 11.  
Clause 13) Safety analysis  a)to identify safety-related  a) AI safety requirements,  13.5.1Safety analysis  
of AI systems  faults and AI errors that  from Clause 9;  report, resulting from  
can lead to the violation of  b) input space  13.3.1 to 13.3.5.  
AI safety requirements;  definition(refined) , from  
b)to identify their  Clause 9;  
potential causes;  c) known insufficiencies of  
c)to support the definition  the AI system and the  
of safety measures to  corresponding subdomains  
prevent or control safety- of the input space, from  
related AI errors;  Clause 9;  
d)to support the  d) AI component or AI  
verification of AI safety  system architecture  
requirements, through  (refined), from Clause 10;  
modification or  e) dataset requirements  
identification of new AI  specification, from Clause  
safety requirements on  11;  
data specifications and collection, design specifications, and test specifications.  f) dataset design specification, from Clause 11;  
g) dataset verification  
report, from Clause 11;  
h) dataset validation  
report, from Clause 11;  
i) dataset safety analysis  
report, from Clause 11;  
j) AI system verification  
report, from Clause 12;  
k) AI system validation  
report, from Clause 12.  
Clause 14) Measures  a)to define the process  a) AI system definition  14.9.1The specification of  
during operation  requirements to  (from external sources),  the process and its  
continuously assure AI  including:  activities for assuring AI  
safety after deployment,  1) interfaces with the  safety during operation,  
b)to use the measures  encompassing system;  resulting from 14.3.1.  
defined in clause 8 and/or  2) Assumptions of the use  14.9.2The specification of  
additional measures for  of the AI system;  the necessary off-board  
the identification of safety risk associated with the AI system during operation and measures to maintain AI safety during operation, c) to ensure responses are  b) field data collected by the encompassing system; c) safety requirements on the AI system, from Clause 9;  and on-board measures, resulting from14.3.2. 14.9.3Field data and functional insufficiencies detected during operation, resulting from  
in place to address  d) AI Component or AI  14.3.3.  
unacceptable safety risks associated with the AI  System Architecture, from Clause 10;  14.9.4Evidence of the effectiveness of  
system and ensure  measures for ensuring AI  

Clause  Objectives  Pre-requisites  Work products  
reapproval of the modified  e)dataset requirements  system during operation,  
AI system before release.  specification, from Clause 11; f) dataset design specification and maintenance plan, from Clause 11; g) dataset maintenance plan, from Clause 11; h) known insufficiencies of the AI system and the  resulting from 14.3.4. 14.9.5Evaluation report of functional insufficiencies detected during operation, and updated version of the safety assurance argument if applicable, resulting from 14.3.3 and14.3.5.  
corresponding subdomains of the input space, from Clause 9 i) results of verification and validation activities including known functional insufficiencies of the AI system (if available), from Clause 12; j) safety assurance argument, from Clause Clause 8.  
Clause 15) Confidence in  a)to provide requirements  a)documentation of  15.7.1Evidence for the  
use of AI development  and guidance to identify,  development processes  analysis of the AI model  
frameworks and  mitigate and document  and tools used within the  creation processes,  
software tools used for  possible sources of errors  AI safety lifecycle (from  resulting from 15.3.1.  
AI model development  and inappropriate biases in the off-line processes, tools and principles used to develop, verify and deploy safety-related AI models.  external sources); b) AI system-specific development measures and procedures (from Clause 7 to Clause 14).  15.7.2Evidence for the confidence in the software tools, resulting from 15.3.2. 15.7.3Evidence for the execution of the AI  
model creation processes with the principles, resulting from 15.3.3.  





3560 Annex B 3561 3562 3563 Example assurance argument structure for an AI-based vehicle function 
3564 B.1 General 
3565 This Annex is informative and provides an example of how an assurance argument for the safety of an AI 3566 system based on the principles outlined in this document can be expressed using the goal structuring notation 3567 (GSN)[6]. The assurance argument structure is expressed as an argument pattern that is intended to be 3568 instantiated for a given AI system. 
3569 The assurance argument depicted within this Annex is for illustrative purposes only and can be used as a 3570 starting point for AI system-specific assurance arguments. The argument is not necessarily complete and 3571 additional arguments and evidence may be required dependent on the AI system context and specific 3572 requirements. 
3573 Evidence can be referenced multiple times within the assurance argument. 
3574 Work products as defined within this document can contain multiple pieces of evidence as referenced in the 3575 assurance argument. 
3576 A description of the notation used can be found within the Goal Structuring Notation Community Standard V3 3577 [6]. 
3578 B.2 Assurance argument pattern for supervised machine learning 
3579 The assurance argument pattern described here can be used to construct an assurance argument for an AI 3580 system that makes use of supervised machine learning algorithms (e.g. DNNs). Example applications to which 3581 this pattern could apply include the use of AI for image processing tasks such as classification or object 3582 detection or predictive maintenance of safety-critical components. 
3583 The top level of the assurance argument is depicted in Figure B.2¨C1. Information that is to be replaced for an 3584 AI system-specific instantiation of the pattern, are indicated using the following notation: {Instantiable 3585 element}. The goal of the argument (G1) is to demonstrate that the AI system satisfies the requirements 3586 allocated to it within the overall system context. This context is defined in terms of: 
3587 ¡ª a set of assumptions on the input space (A1.1); 
3588 ¡ª a set of assumptions on the system context (A1.2); 
3589 ¡ª a definition of the functionality to be implemented by the AI system (C1.1); 
3590 ¡ª a definition of the safety requirements allocated to the AI system (C1.2). 
3591 The assurance argument also assumes that: 
3592 ¡ª quality management principles have been applied during the development of the AI system and its 3593 assurance argument (A1.3), that reduce the risk of systematic errors and increase the confidence in the 3594 assurance structure and evidence. The assurance argument is supported by a documented and repeatable 3595 development process. 
3596 ¡ª malfunctioning behaviour caused by random hardware faults or systematic faults are adequately 3597 addressed and confirmed through an additional argumentation, not described here.For example by 3598 following the guidance of the ISO 26262 series (A1.4). 
3599  ¡ª Development frameworks and tooling for the AI system do not impact AI safety (addressed by Assumption  
3600  A1.5, see Clause 15), which may be justified by the use of pre-qualified development frameworks and tools.  
3601  The argument is structured to demonstrate that all potential functional insufficiencies in the AI system have  
3602  been prevented, minimised, or mitigated during the specification, design and operation of the AI system (S1).  
3603  This strategy makes use of a set of causes of functional insufficiencies for the type of application and applied  
3604  AI technology (C1.3). These causes can include those described within this document as well as AI system- 
3605  specific causes that are identified based on safety analyses (see Clause 13).  


3607 Figure B.2¨C1 ¡ª Assurance argument pattern for a supervised machine learning-based AI system 
3608 Figure B.2¨C2 elaborates the claim G2 of the argument pattern that demonstrates that potential insufficiencies 3609 of the specification have been addressed as described in Clause 9. This argument pattern consists of 3610 demonstrating: 
3611 ¡ª A sufficient understanding of the input space (G2.1); 
3612 ¡ª The derived AI safety requirements are complete and consistent with respect to the safety requirements 3613 allocated to the AI system. This includes demonstrating that each individual AI safety requirement is well 3614 defined on the basis of safety-related properties of ML models (S2.2) as well as that the combination of all 3615 derived AI safety requirements are sufficient to fulfil the safety requirements allocated to the AI system 3616 (G2.2.1); 
3617 ¡ª The performance limitations of the AI system are sufficiently well defined that a safe behaviour at the 3618 system level can be ensured (G2.3). 
3619 The derivation of the AI safety requirements as well as the definition of residual performance limitations are 3620 supported by the use of safety analyses (see Clause 13) that determine the potential for safety-related 3621 functional insufficiencies and potential causes in the AI system. 

3623  Figure B.2¨C2 ¡ª Assurance argument pattern for demonstrating potential insufficiencies of the  
3624  specification of the AI system have been addressed  
3625  Figure B.2¨C3, Figure B.2¨C4 and Figure B.2¨C5elaborate the claim G3 of the argument pattern that demonstrates  
3626  that the datasets used for training and verification of the AI system are sufficient to achieve and demonstrate  
3627  AI Safety, as described in Clause Clause 11. This claim is further refined as follows:  
3628  ¡ª The datasets consist of suitable selections of observations from the overall input space (G3.1);  

3629  ¡ª The integrity of the datasets is maintained throughout the data lifecycle (G3.2).  
3630  The assurance argument is supported by a set of safety-related properties of the datasets, which can be specific  
3631  to the application and applied AI technology (see Clause 11.4.3.2 for examples).  


3633 Figure B.2¨C3 ¡ª Assurance argument for the sufficiency of the datasets 3635 Figure B.2¨C4 ¡ª Assurance argument for claim G3.1.1 


3637 Figure B.2¨C5 ¡ª Assurance argument for claim G3.2 
3638 Figure B.2¨C6 elaborates the claim G4 that demonstrates that functional insufficiencies have been addressed in 3639 the design of the AI system, as described in Clause 10. This claim is made based on an understanding of the 
. ISO 2024 ¨CAll rights reserved 
3640 factors that influence the fulfilment of the AI safety requirements as well as the effectiveness of proposed 3641 development and architectural measures. The claim is further refined as follows: 
3642 ¡ª The chosen AI technology is inherently suitable for achieving the safety requirements allocated to the AI 3643 system (G4.1), 
3644 ¡ª Development and architectural measures have been chosen that ensure that the AI system meets its AI 3645 safety requirements (G4.2), 
3646 ¡ª Architectural measures have been identified to mitigate residual insufficiencies in the AI model (G4.3), and 
3647 ¡ª The functional adequacy and sufficient performance is also ensured within its target environment (G4.4). 3648 Note: G4.4 is not elaborated further in this version of the document. 

3650 Figure B.2¨C6 ¡ª Assurance argument pattern that functional insufficiencies have been addressed 3651 during design 
3652 Figure B.2¨C7 elaborates the claim that sufficient evidence exits that the safety requirements allocated to the 3653 AI system have been fulfilled as demonstrated through verification and validation as described in Clause 12. 3654 This argument considers: 
3655 ¡ª The fulfilment of the safety requirements allocated to the AI system in itsentirety (G5.1), and 
3656 ¡ª The fulfilment of the derived AI safety requirements allocated to the individual components of the AI 3657 system (G5.2). 
3658 In each case, an argument is made over the appropriateness of the verification and validation strategy as well 3659 as the evidence used to evaluate each individual requirement. 

3661 Figure B.2¨C7 ¡ª Assurance argument pattern that the fulfilment of the safety requirements has been 3662 adequately demonstrated 
3663 Figure B.2¨C8 elaborates the claim that residual and emerging insufficiencies are identified during operation 3664 and mitigation measures are defined, as described in Clause 14. This argument considers: 
3665 ¡ª The definition of effective operating procedures for the safe operation of the encompassing system based 3666 on known insufficiencies of the AI system (G6.1), 
3667 ¡ª The use of effective processes for continuous re-evaluation of residual risk (G6.2), and 
3668 ¡ª Effective countermeasures are taken to address emerging insufficiencies (G6.3). This claim in the 3669 assurance argument can only be made after initial release of the AI system during re-evaluation of the 3670 overall safety assurance argument before deployment of changes. 
3671 In each case an argument is made over the effectiveness of the measures to control risk during operation. 

3673  Figure B.2¨C8 ¡ª Assurance argument pattern that emerging and residual insufficiencies are identified  
3674  and mitigated during operation  
3675  B.3 Use of  assurance  claim points  to  increase confidence in the  assurance  
3676  argument  
3677  B.3.1  General remarks on the use of assurance claim points  
3678  The GSN pattern outlined in Clause B.2 reflects the basic structure of an assurance argument that safety  
3679  requirements allocated to  a  supervised ML-based AI system  are  fulfilled. The GSN pattern reflects the  
3680  objectives and requirements of this document.  

3681 For any given AI system, the strength of the assurance argument may depend on a number of factors related 3682 to the complexity of the task and its environment, the availability of sufficient training and test data and the 3683 types of AI-technique used. These factors can lead to uncertainty and therefore diminished confidence in the 3684 argument. 
3685 As outlined in clause 8.7, an evaluation of the argument can include identification of defeaters based on the 3686 following types of assertions within the assurance argument [11]: 
3687 ¡ª Asserted context: relationship between the claim, contextual information and assumptions; 
3688 ¡ª Asserted evidence: relationship between the claim and the evidence supporting that claim; 
3689 ¡ª Asserted inference: relationship between the claim and the strategies used to structure the sub-claims 3690 and evidence to support that claim. 
3691 Version 3.0 of the GSN standard [6] provides the mechanism of assurance claim points (ACPs) to add further, 3692 deeper reasoning for particular relationships that would otherwise potentially undermine the confidence in 3693 the argument. 
3694 The reasoning linked to a particular ACP can be supplied in various forms. A separate GSN model for each ACP 3695 is one option, evaluation reports with links to further supporting evidence is another. 
3696 The following subclauses provide examples for ACPs to support each of the above types of assertion. 
3697 B.3.2 Example assurance claim points to support assumptions or context: ACP-A2 for assumption 3698 A2 
3699 Referring to Figure B.2¨C1, this subclauses addresses ACP-A2 related to the asserted context associated with 3700 ACP-A2 as illustrated in Figure B.3¨C1. 
3701 The assumption A1.2 reads ¡°{Assumptions on the technical system context}¡±. This refers to the technical 3702 integration of the AI-system into the encompassing system, i. e. this assumption refers to the interfaces to the 3703 other systems and sub-systems as part of the vehicle. 
3704 EXAMPLE 1 ISO/IEC/IEEE 15289:2019 clause 10.28 mentions some of the properties which are subject to proper 3705 interface definitions: ¡°systemsorconfiguration itemsperforming theinterface(including human-system and human-3706 human interfaces), standards and protocols, responsible parties, information or data records transmitted by the 3707 interface, interfaceoperationalschedule, and errorhandling¡±. 
3708 The information linked to ACP-A2 would demonstrate why the documented interface properties are 3709 considered complete in the sense that no safety-relevant interface property remains unspecified. I. e. none of 3710 the unspecified interface propertiesare able to interfere with the achievement of the safety requirements 3711 allocated to the AI-system. 
3712 EXAMPLE 2 For a camera-based object detection and classification function implemented by an AI system, 3713 information regarding the resolution, depth of focus, quality (e.g. sensor noise), etc., of the camera providing the raw 3714 images is documented and analysed to ensure that it meets the assumptions made during the development and test of 3715 the AI system. 

3717  Figure B.3¨C1 ¡ª Example use of ACPs within the GSN assurance argument pattern  
3718  B.3.3 Example assurance claim point to support inference: ACP-S1 for strategy S1  
3719  ThestrategyS1 reads¡°Functional insufficienciesthat could violate safetyrequirementshave been prevented,  
3720  minimized or mitigated duringspecification, designandoperation¡±. S1 issupported bysub-goals which reflect  
3721  the various clauses of this document. The ACP-S1 in Figure B.3¨C1 is inserted in order to strengthen the  
3722  assertion that the argumentation strategy based on the set of hypothesized causes of insufficiencies derived  
3723  from safety analysis is complete and sufficient to demonstrate that the safety requirements allocated to the AI  
3724  system have been met.  
3725  This could be achieved by providing further information on previously (successful) applications of the  
3726  strategy. Alternatively, this could be achieved by provisioning an evaluation procedure that supervises the  
3727  strategy and would flag slipped, untreated functional insufficiencies. Additional forms of reasoning could  
3728  include reference to effectiveness of the safety analysis approach (seeClause 13) to identify potential  
3729  insufficiencies and their causes that may otherwise not have come to light during the development and test of  
3730  the AI system.  
3731  B.3.4 Example assurance claim point to support evidence: ACP-E5  
3732  This subclause provides an example of an assurance claim point to support the assertion directly related to  
3733  evidence. Figure B.3¨C2 illustrates the ACP-E5 in the context of the GSN provided previously in Figure B.2¨C7.  
3734  This portion of the argument pattern relates to how a combination of evidence demonstrates that individual  
3735  safety requirements are met.  
3736  Evidenceis asserted to show the achievement of eachAI safety requirement. Such evidence might be a  
3737  collection of test results documented in one or more test reports. In the case that the safety requirement, test  
3738  results and test reports are closely aligned with each other, no additional argumentation may be required.  

3739 However, in some cases the alignment between test cases and requirement might need additional justification. 3740 In other cases, the challenges might be associated with the testability of the requirement itself. Additional 3741 reasoning, such as traceability from requirements to test cases, or a description of the approach to indirectly 3742 verify a requirement, might be added to the argument using an assurance claim point such as ACP-E5. 
3743 This additional reasoning may address both the integrity of the evidence (e.g. have the results of the tests been 3744 collected and analysed without loss of critical information) as well as its validity (e.g. have sufficient tests been 3745 performed to ensure a high level of statistical confidence). 
3746 As in the previous subclauses, the assurance claim point can be linked to yet another separate GSN or be 3747 backed by some argument in natural language or other supportinganalyses. 

3749 Figure B.3¨C2 ¡ª Example use of ACP to reason about the asserted evidence 
. ISO 2024 ¨CAll rights reserved 
ISO/CD PAS 8800:2024(en)  
3750  Annex C  
3751  
3752  
3753  ISO 26262:2018 Gap Analysis for ML  
3754  This annex presents the results of a gap analysis of ISO 26262:2018 related to ML. The gap analysis is in the  
3755  form of an example tailoring and guidance for ISO 26262:2018 Parts 4 and 6. The analysis did not find  
3756  significant gaps related to ISO 26262:2018 Parts 1,2,3,5,7,8,9.  
3757  C.1 ISO 26262-4:2018 Tailoring and Guidance for ML  
3758  Table C.1¨C1 presents the example tailoring and guidance for the requirements of ISO 26262-4:2018, Clause 8  
3759  "Safety Validation" related to ML. The requirements of ISO 26262-4:2018 and requirements from Clause 8 that  
3760  are not listed in the table are considered to not need any additional tailoring or guidance for ML. Different  
3761  tailoring can be applied to different AI technologies.  
3762  Table C.1¨C1 ¡ª ISO 26262-4:2018, Clause 8 example tailoring/guidance for ML  

Clause  Requirement  Proposed Tailoring/Guidance for ML  
8.4.3.3  The safety validation at the vehicle level, based on the safety goals, the functional safety requirements and the intended use, shall be executed as planned using: a) the safety validation procedures and test cases for each safety goal including detailed pass/fail criteria; and b) the scope of application. This may include issues such as configuration, environmental conditions, driving situations, operational use cases, etc.  Additional guidance: intended use includes representative inputspace definition (e.g. Operating environment, input domain, conditions of use) a) explicitly includes safety-related KPIs  
8.4.3.4  An appropriate set of the following methods shall be applied: a) repeatable tests with specified test procedures, test cases, and pass/fail criteria; b) analyses; c) long-term tests, such as vehicle driving schedules and captured test fleets; d) operational use cases under real-life conditions, panel or blind tests, or expert panels; and e) reviews.  Guidance b) may be limited (e.g. simulation only) e) is typically not applicable for ML validation  

3763  
3764  C.2 ISO 26262-6:2018 Tailoring for ML  
. ISO 2024 ¨CAll rights reserved  

3765  Table C.2¨C1 presents the example tailoring and guidance for the requirements of ISO 26262-6:2018 related to  
3766  ML. Where noted, the tailoring/guidance is related to NN models only. The requirements of ISO 26262-6:2018  
3767  that are not listed in the table are considered not to need any additional tailoring or guidance for ML.  
3768  Table C.2¨C1 ¡ª ISO 26262-6:2018 example tailoring/guidance for ML  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
5.4.3 Table 1  1a Enforcement of low complexity 1b Use of language subsets 1c Enforcement of strong typing 1d Use of defensive implementation techniques  Tailoring For ML applications, Table 1 applies unchanged for use case independent elements (e.g. CUDA C++ libraries). For the use case dependent elements (i.e. the models), 1c), 1d), 1g), 1i)  
1e Use of well-trusted design principles 1f Use of unambiguous graphical representation 1g Use of style guides 1h Use of naming conventions 1i Concurrency aspects  with ¡°o¡±forallASILs(justification see ISO/IEC TR 5469 Tables A.3 and A.4) NOTE Use case independent elements refer to elements that behave the same independent of the use case (i.e., CUDA libaries fulfill the same purpose independently if the trained models use case is in autonomous driving or predictive maintanance). In contrast, use case dependent elements like neural network models are dependent on the specific use case, which changes their properties.  
6.4.1  The software safety requirements shall be derived considering the required safety-related functionalities and properties of the software, whose failures could lead to the violation of a technical safety requirement allocated to software  Guidance in the form of additional considerations Requirements in the form of: a) KPIs; b) data attributes; and c) dataset requirements (e.g. inputspace definition) specification for the training/validation/testing data set The ML implementation not meeting its KPIs is an ISO 26262 issue. Incorrect or insufficient KPIs are a SOTIF concern.  
6.4.4  The hardware-software interface specification initiated in ISO 26262-4:2018, Clause 6, shall be refined sufficiently to allow for the correct control and usage of the hardware by the software, and shall describe each safety related dependency between hardware and software  Guidance in the form of examples EXAMPLE 1 Software is specified to run on one CPU on a multi-CPU system. EXAMPLE 2 NN specified to run on a GPU  
6.4.7  The software safety requirements and the refined requirements of the hardware-software interface specification shall be verified in accordance with ISO 26262-8:2018,  Guidance in the form of additional considerations e) adequate coverage of the input space of the software.  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
Clauses 6 and 9, to provide evidence  Adequate coverage of the input  
for their:  space typically involves i) sufficient  
a) suitability for software  labels that comprehend the entire  
development;  labelling space (e.g. labels for  
b) compliance and consistency with  emergency vehicles), ii) data with multiple views (e.g. multiple  
the technical safety requirements;  examples of emergency vehicles)  
c) compliance with the system design; and  f) address the handling of out-of-distribution inputs  
d) consistency with the hardware- 
software interface.  
7.4.3  In order to avoid systematic faults, the software architectural design shall exhibit the following characteristics by use of the principles listed in Table 3: a) comprehensibility; b) consistency; c) simplicity; d) verifiability; e) modularity; f) encapsulation; and g) maintainability.  Guidance 1) Software components implemented using machine learning are considered to be difficult to verify. A heuristic component is preferred over a machine learning component assuming the function can acceptably be implemented using a heuristic component. 2) NN models are considered as individual units. The principles of Table 3 typically cannot be met for NN applications. Usually, they are generated from higher level languages using tools. The design principles therefore are applied to the code that generates the NN model and tool qualification are applied to the generator. This is similar to the usage of code generation in normal SW development.  
7.4.4  The software architectural design shall be developed down to the level where the software units are identified.  Guidance 1) An individual NN may consist of many nodes and layers but is typically considered as one unit. It may not be possible to express an NN software design at any level lower than the individual NN level. 2) A SW unit can be an NN so long as suitable interfaces can be defined and requirements allocated to those units. 3) An architecture description for an NN model, e.g. in ONNX, can be created. Nevertheless, explainability based on the architecture may be low however a justification for the choice of the structure can be provided, e.g. motivated by ablation studies.  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
7.4.7  If a pre-existing software architectural element is used without modifications in order to meet the assigned safety requirements without being developed according to the ISO 26262 series of standards, then it shall be qualified in accordance with ISO 26262-8:2018, Clause 12.  Guidance For pre-existing ML based software when the specification characteristics such as dataset attributes, KPIs, inputspace definition and output metrics are articulated, the verification should ensure that the specification characteristics are sufficiently met.  
7.4.13  An upper estimation of required resources for the embedded software shall be made, including: a) the execution time; b) the storage space; and c) the communication resources.  Guidance in the form of additional considerations d) parallel computation resources  
8.4.3  To avoid systematic faults and to ensure that the software unit design achieves the following properties, the software unit design shall be described using the notations listed in Table 5. a) consistency; b) comprehensibility; c) maintainability; and d) verifiability.  Guidance in the form of additional considerations Additionally, use the derived AI safety-related properties for the given AI system as appropriate, for example: e) Interpretability (ISO/IEC 5469 for definition); f) Explainability (see Annex D and ISO/IEC 5469 for definition); g) Predictability (see Annex Dfor definition); h) Specificability (see ISO/IEC 5469 for definition); i) Generalisation (see Annex D and ISO/IEC 5469 for definition); j) Domain shift (see ISO/IEC 5469 for definition); k) Robustness-Safeness (see ISO/IEC 5469 for definition); m) Diversity (see ISO/IEC 5469 for definition); and n) Confidence (see ISO/IEC 5469 for definition).  
8.4.4  The specification of the software units shall describe the functional behaviour and the internal design to the level of detail necessary for their implementation.  Guidance For the case of a unit containing an NN, the structure of the NN (e.g. number of nodes, layout, interconnects and activation function) and hyperparameters and training methods of NN (e.g learning rate) are part of the specification of the software unit.  
8.4.5  Design principles for software unit design and implementation at the  Guidance  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
source code level as listed in Table 6 shall be applied to achieve the following properties: a) correct order of execution of subprograms and functions within the software units, based on the software architectural design; b) consistency of the interfaces between the software units; c) correctness of data flow and control flow between and within the software units; d) simplicity; e) readability and comprehensibility; f) robustness; g) suitability for software modification; and h) verifiability.  1) For NN units, a) and c) may not apply since the NN is considered as one function and the order of execution of individual nodes is not guaranteed. 2) For h), the structure of the network can be verified, for example, by inspection that the correct structure is implemented. 3) For AI models trained using data, the influencing factors of Table 9¨C1 may be considered as additional design principles: observation certainty, label certainty, model certainty, and operation certainty  
9.4.2  The software unit design and the implemented software unit shall be verified in accordance with ISO 26262-8:2018, Clause 9 by applying an appropriate combination of methods according to Table 7 to provide evidence for: a) compliance with the requirements regarding the unit design and implementation in accordance with Clause 8;  Guidance a) The NN model software verification report documents the test result KPI and the dataset used for the testing. Tailor d) to d) confidence in the absence of unintended functionality and properties (Unintended functionality is primarily a SOTIF concern for systems modelled using ML).  
b) the compliance of the source code with its design specification; c) compliance with the specification of the hardware-software interface (in accordance with 6.4.4), if applicable; d) confidence in the absence of unintended functionality and properties; e) sufficient resources to support their functionality and properties; and f) implementation of the safety measures resulting from the safety-oriented analyses in accordance with 7.4.10 and 7.4.11.  
9.4.2 Table 7  1a Walkthrough  Tailoring  
1b Pair-programming  1a) through 1i) For ML "o" since  
1c Inspection  often infeasible to do effectively  
1d Semi-formal verification  Guidance  
1e Formal verification  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
1f Control flow analysis 1g Data flow analysis 1h Static code analysis 1i Static analyses based on abstract interpretation 1j Requirement-based test 1k Interface test 1l Fault injection test 1m Resource usage evaluation 1n Back-to-back test between model and code, if applicable  1) For ML, 1j feasible for only some properties such as invariants and equivariants 2) For ML, 1l Fault injection has limited applicability 3) For ML, 1n Applicable when comparing off-line versus optimized code versions  
9.4.3  To enable the specification of appropriate test cases for the software unit testing in accordance with 9.4.2, test cases shall be derived using the methods as listed in Table 8.  Guidance For ML, unit test cases can be selected from test dataset  
9.4.4  To evaluate the completeness of verification and to provide evidence that the objectives for unit testing are adequately achieved, the coverage of requirements at the software unit level shall be determined and the structural coverage shall be measured in  Tailoring Requirement NA for NNs, includes NA for Table 9 For NNs without separate program statements, branches or decision logic this requirement does not apply .  
accordance with the metrics as listed in Table 9. If the achieved structural coverage is considered insufficient, either additional test cases shall be specified or a rationale based on other methods shall be provided.  An example of where this requirement is still applicable is conditional computation in neural networks, which is sometimes implemented to reduce latency and save energy (i.e., only part of a net is activated). It is possible to select inputs as unit tests to cover all branches.  
9.4.5  The test environment for software unit testing shall be suitable for achieving the objectives of the unit testing considering the target environment. If the software unit testing is not carried out in the target environment, the differences in the source and object code, as well as the differences between the test environment and the target environment, shall be analysed in order to specify additional tests in the target environment during the subsequent test phases.  Guidance in the form of an example EXAMPLE A NN model is trained using fp32 math, but the on-line inferencing uses int8 for throughput and bandwidth savings. Off-line unit testing of the model uses int8 to match the target environment.  
10.4.2 Table 10  1a Requirements-based test  Tailoring  
1b Interface test  1c) For NNs, targeted SW fault  
1c Fault injection test  injection might only be appropriate at certain interfaces. HW fault  

Clause  Requirement/Method  Proposed Tailoring/Guidance for ML  
1d Resource usage evaluation 1e Back-to-back test between model and code, if applicable 1f Verification of the control and data flow 1g Static code analysis 1h Static analyses based on abstract interpretation  injection on the target environment can test the response to permanent and transient faults. 1g) For NNs "o" 1h) For NNs "o" Justification g) and h) Static code analysis aiming to verify functionality of NN does not scale beyond small networks  

3769 
. ISO 2024 ¨CAll rights reserved 








3770 Annex D 3771 3772 3773 Detailed considerations on safety-related properties of AI systems 
3774  This Annex provides a list of properties of AI systems that are considered desirable/necessary from a safety  
3775  perspective. These properties are conceptual, and the list is based on past AI development experience and is  
3776  not exhaustive.  
3777  Safety-related properties can be quantitative in nature as well as qualitative. As a result, they are not always  
3778  completely achievable. For example, while the robustness property indicates that a model is either robust or  
3779  not,  a  DNN model for classifying objects in  an  open world is  never  100% robust against all possible  
3780  insignificant input changes. The choice of safety-related properties relevant to the AI system should be  
3781  validated through safety analysis to ensure their contribution to the system's safety, and target thresholds  
3782  should be provided with justification.  
3783  A safety-related property may or may not apply depending on use cases, systems, AI models, etc. For example,  
3784  while a self-driving vehicle¡¯s actions, such as acceleration and steering, could be controllable, the outputs of a  
3785  DNN model for object detection in the perception pipeline of the vehicle are not.  
3786  The scope of a safety-related property of AI systems refers to the entity to which the property is attributed.  
3787  For example, the organization can effectively and efficiently update an AI model whenever necessary. In this  
3788  context, the overall system is considered the whole product, e.g., the vehicle.  
3789  Table D¨C1 ¡ª Safety-related properties of AI systems  
3790  NOTE 1  One or more KPIs are typically defined to characterize each safety-related property. The safety requirement specifies the  
3791  acceptable threshold value for these KPIs.  

Property  Description  Scope  
AI robustness  Ability to maintain an acceptable level of performance under the presence of semantically insignificant, but reasonably expected changes to the input (see definition AI reliability (3.1.14)) NOTE 2 AI robustness focuses on foreseeable/relevant perturbations (type of perturbation as well as amplitude of perturbation) which can occur in the real world, to avoid defining unnecessary safety requirements.  Model, system  
AI generalization capability  Ability of a model to adapt and perform well on the previously unseen data during inference  Model, system  
AI reliability  Ability to maintain all functionalities for a specified period (see definition AI reliability (3.1.12))  Model, system  
AI resilience  Ability to quickly recover from an incident (see definition AI resilience (3.1.13))  (Overall) system, organization  

Property  Description  Scope  
AI controllability  Ability of an external agent to overwrite the behaviour or output of an AI system  (Overall) system, organization  
AI explainability  Ability to explain in natural language which factors influence the AI element decision and how (see definition AI explainability (3.1.4))  Process  
AI predictability  Reliable confidence information for AI refers to the ability of an AI model to reliably indicate if its prediction can be trusted or not. This is not always true for all kinds of models. For example, the output of a softmax function is frequently misinterpreted as some kind of posterior distribution which indicates confidence.  Model, system  
AI alignment  AI alignment is ensuring that the AI system behaviour is aligned with human values and with the human expected intent of the system  Process  
Justified design decisions  Bad design decisions may have detrimental effects on the behavior of the AI model/system. Therefore, design decisions need to be justified and their negative effects need to be analyzed. This is also valid for training process and data selection decisions where applicable.  Process  
Maintainability  Ability to effectively and efficiently identify operational insufficiencies and countermeasures, change the encompassing system design and the AI system design, collect relevant data, label them, train the AI model, and update the AI system and other systems in a timely manner.  Organization, process  
AI bias and fairness  AI bias refers to the notion that an AI model or dataset maybe systematically prejudiced towards some kind of (potentially erroneous) assumption. This assumption stems from the inherent statistical distributions (e.g., over classes) in a dataset that can be learned by a model. If the model bias is linked to a difference in treatment of certain subgroups of humans (e.g., ethnic minorities, age or sex) this model is considered unfair. AI fairness is the reasonable absence of unfairness.  Model, (overall) system  
Distributional shift over time  Distributional shift over time refers to the potential distributional change  Overall system  

Property  Description  Scope  
in any input data stream due to natural changes (e.g., sensor aging, new object classesonstreets, ¡­). This distributional shift can cause a performance decrease of the AI model in the field since this model has been developed with a different data distribution.  

3792 Annex E 3793 3794 3795 STAMP/STPA example 
3796 
3797 E.1 Overview 
3798 Clause E.2 describes a vulnerable road user (VRU) recognition and braking system as an informative example. 3799 An example of a Vulnerable Road User (VRU) is a pedestrian whose trajectory intersects with, and moves 3800 toward, the path of the ego vehicle during nighttime. The system identifies VRUs and, depending on the 3801 situations, sends a brake command to either reduce speed to a certain extent or to stop completely. E.2.4 3802 demonstrates how to identify causes of safety-related errors of the recognize of pedestrians. E.2.5 focuses on 3803 deriving safety measures to mitigate the safety risks due to the safety-related AI errors. These mitigation 3804 measures for the AI system can be categorized into those applied during design-time and those applied during 3805 operation-time [51]. 
3806 NOTE The term "ego vehicle" in this example is used for the vehicle fitted with functionality that is being analysed 3807 for the SOTIF [Source : ISO 21448 3.6 ego vehicle]. 
3808 E.2 STPA Example 
3809 E.2.1 STPA Step 1: Defining the purpose and scope of the analysis 
3810 The first step of STPA identifies thestake holders¡¯ losses to be prevented. Table E.2¨C1 provides an example of 3811 STPA losses and hazards. 
3812 Table E.2¨C1 ¡ª Example of loss and hazard identification 
Loss  Hazard  
[L1] Loss of life or human harm (severe or fatal injuries)  [H1] Vehicle violates minimum distance threshold/ requirement from/with vulnerable road users.  

3813  E.2.2  STPA step 2: Modelling of the control structure  
3814  The development and operation of AI systems involve various complexities and uncertainties in the assurance,  
3815  such as data quality, training, complexity of AI models, etc., which potentially correlate to or influence safety- 
3816  related AI errors.  
3817  NOTE  Control structures  are  defined  as  hierarchical structures where each level imposes constraints  on  the  
3818  activities of the level beneath them and accidents  are  viewed as the consequence of inadequate control of safety  
3819  constraints [52].  
3820  Figure E.2¨C1 illustrates the control structure of the VRU recognition and braking system utilizing bounding  
3821  boxes or sematic segmentations. The AI system receives the camera or the LiDAR data from the sensors to  
3822  recognises theVRU.. 


3824  Figure E.2¨C1 ¡ª Control structure of the VRU recognition and braking system  
3825  E.2.3  STPA step 3: Identification of unsafe control actions  
3826  Table E.2¨C2 shows a few examples of the unsafe control actions (UCAs) of the AI system when sending the  
3827  brake command to actuators. These UCAs are control actions which in a particular context and operational  
3828  situation can lead to a hazard.  
3829  NOTE  A control action is defined as a command or feedback item created by, or used by, a system element to perform  
3830  its function(s) [53].  
3831  Table E.2¨C2 ¡ª Example of unsafe control actions for the control action ¡°AI system output state¡±  

Control action  Not providing  Providing  Providing too early, too late, or in the wrong order  Providing for too long or stopping too soon  
CA1: AI system sends the brake command  UCA1: AI system does not send the brake command when the VRU is approaching to the trajectory of the ego vehicle..[H1] (False negative)  UCA2: AI system send the brake command when there are no VRU in the trajectory of the ego vehicle. [xx] (False positive)  UCA3: AI system sends the brake command too late when the VRU is moving in the trajectory of the ego vehicle..[H1]  

3832  E.2.4  STPA step 4: Identification of causal scenarios  
3833  In STPA Step 4, the focus is to identify the causal scenarios that could lead to the occurrence of each UCA and  
3834  the failure of control actions to be executed or executed correctly. Figure E.2¨C2 shows detailed control  
3835  structure focusing on control loops and interactions.  

. ISO 2024 ¨CAll rights reserved 

3837  Figure E.2¨C2 ¡ª Detailed control structure of the VRU recognition and braking system  
3838  Table E.2¨C3 shows some examples of causal scenarios which are correlated or influenced by using influencing  
3839  factors defined in Clause 6 (Table E.2¨C4).  
3840  NOTE  The example presented here addresses only a few causal scenarios that lead to UCA1. However, in an STPA,  
3841  all UCAs need to be analysed completely.  
3842  UCA1: AIsystem doesnotsendthebrakecommandwhentheVRUisapproachingtothetrajectoryofego vehicle..[H1]  
3843  (False negative)  
3844  Table E.2¨C3 ¡ª Example of causal scenarios of safety-related AI errors  

Process model or control algorithm  Category of causal scenarios  Causal scenarios  Influencing factors (Link to countermeasure)  
Process model  Ontological uncertainty of recognizing VRU  A vulnerable road user, dressed in unusual clothing and walking in a peculiar posture at the intersection, was a scenario that had not been previously trained for and evaluated during the development, resulting in a high rate of false negatives.  Observation  
Process model  Epistemic uncertainty of recognizing VRU  Steam rising from the surface obscures vulnerable road users (VRUs) and distorts the images captured, leading to AI errors such as misplacement or incorrect sizing of bounding boxes, resulting in some objects being missed or mislabeled. (Bounding box use case)  Operation, Label  
Process model  Aleatoric uncertainty of recognizing VRU in specific  A pedestrian unexpectedly reverses direction while still looking forward,  Model  

behaviors and  resulting in an increased  
appearances  rate of AI errors in predicting the pedestrian's trajectory.  
Process model and Control algorithm  Aleatoric uncertainty of recognizing VRU in environmental noise and performance of resources  Dust and moisture, which can accumulate on the surface of the sensors, are captured as homogeneous noise in the images. This leads to a higher increase in computational demand than originally designed for, along with lowered prediction scores, ultimately resulting in incorrect semantic segmentation beyond the required time value. (semantic segmentation use case)  Operation, Model  

3845  By illustrating these correlations or influencing factors, it is possible to develop effective safety measures such  
3846  as design-time measures and/or operation-time measures [51] that can prevent or mitigate the occurrence of  
3847  UCA.  
3848  E.2.5  Identifying safety measures to mitigate the safety-related issues  
3849  After identifying safety-related issues in an AI system, the next step is to incorporate safety measures into the  
3850  design, dataset generation and implementation of the AI system according to Clause 9, Clause 10 and Clause  
3851  11 of this document.  
3852  Table E.2¨C4 shows examples of chain of the safety related issues of AI systemand safety measures.. 
3853  Table E.2¨C4 ¡ª Examples of the chain of the safety related issues of the AI system and safety measures  
3854  [51], [54].  

Chain of the safety related issues  Safety measure  
AI Error class  Insufficiency  Cause  Design-time measure  Metric  Operation-time measure  Metric  
Observation (Incorrect classification)  Lack of generalization  Under specification, scalable over-sight  Balanced training set  Coverage of the ODD model  N/A  N/A  
Observation (Incorrect classification)  Unreliable confidence values  Overconfiden ce due to uncalibrated soft-max values  Temperature scaling  Remaining AI error rate, remaining accuracy rate  N/A  N/A  
...  ...  ...  ...  ...  ...  ...  
Operation (False negatives)  Lack of robustness  Instability of DNNs for minor changes to the inputs  Adversarial training  Adversarial and perturbation robustness  Robustness certificates  Certifiable perturbation strength  

Operation (Sequence of false negatives)  Lack of generalization  Under specification, scalable oversight  Balanced training set  Coverage of the ODD model  Comparison with other sensor data  Diagnostic coverage  
...  ...  ...  ...  ...  ...  ...  
Observation  Clever Hans  Spurious  Diversified  Conceptual  Plausibility  Diagnostic  
(False  effect  correlations  training set  disentanglem  checks  coverage  
positives)  in the training data  ent  
Operation (False positives)  Lack of generalization  Distributional shift  N/A  N/A  Out of distribution detection  Diagnostic coverage  
Label  
...  ...  ...  ...  ...  ...  ...  
Model  
...  ...  ...  ...  ...  ...  ...  






3855 Annex F 3856 3857 3858 Identification of software units within NN-based systems 
3859 ISO 26262 defines the concept of a SW unit as comprising of its interface, and SW architecture designs. This 3860 approach is central to developing a modular process for software development and verification and validation 3861 (V&V). Some benefits of a modular software unit are: 
3862 ¡ª With clearly identified software units and their interfaces, detecting the negative impact of one software 3863 unit modification on the other software units can be anticipated and mitigated early on. 
3864 ¡ª Modularity improves V&V and testing, allowing an exponential reduction in the complexity of the number 3865 and size of test inputs to achieve comparable coverage [64] and also improved fault localization. 
3866 ¡ª Modularity also allows for incremental V&V and testing of individual software and hardware elements 3867 without waiting for complete systemintegration..
3868 AI models are increasingly used to implement complex functionality, although the notion of software units for 3869 some AI methods (e.g. CNNs) has not been clearly defined. Applying the concept of software units to AI models 3870 therefore presents certain challenges: 
3871 ¡ª While the design of AI models is routinely specified using neural network architectures understood as 3872 computational graphs and NN layers, individual neurons and layers are not separately testable units. 
3873 ¡ª The size and complexity of groups of neurons and layers alone are not valid criteria to distinguish them as 3874 modular units. 
3875 NOTE 1 For conventional algorithms, software units are typically identified based on the size, complexity, and 3876 relatedness of implementation artefacts; each software unit¡¯s implementation is kept to a reasonable size and 3877 complexity, andpreferably represent only onefunction¡¯simplementation. 
3878 ¡ª In contrast to conventional software, any function implemented by a NN, especially a monolithic one, 3879 might not be clearly mappable to a subset of the network with a clear boundary, and the allocation to 3880 individual neurons and layers evolves as the NN is re-trained. 
3881 This annex describes a method to identify SW units within a given NN-based system, and their organization 3882 into a SW architecture. While monolithic NN designs are possible, the intent of the method is to encourage 3883 dividing a NN-based system performing complex functions into separate elements that can be tested or 3884 analysed more easily. 
3885 The key aspects to defining a SW unit are: 
3886 ¡ª A clearly defined interface; 
3887 ¡ª A clearly defined function; 
3888 ¡ª The ability to perform V&V activities at the unit interface level, such as unit testing and inspections. 
3889 In contrast to conventional SW units, different parts of an NN, such as NN layers, often exchange 3890 representations that are latent and also evolve as the NN is re-trained. A latent representation is the task-3891 specific information extracted from the input and mapped into a latent vector or tensor space that aids 3892 performing the task. 
3893 EXAMPLE 1 In image classification, features extracted from images that belong to the same class are located in the 3894 latent space close to each other to aid their subsequent classification. 
3895 The mapping into the latent representations and the representations themselves normally evolve as the NN is 3896 re-trained. Further, latent representations might not be interpretable by humans, but it might be possible to 3897 map them into interpretable representations using suitable decoders. 
3898 NOTE 2 A latent feature is a property of a model that captures underlying characteristics or patterns in the data. Latent 3899 features are inferred during a model's learning process, and contribute to a models ability to represent complex 3900 relationships within the data. Latent features can also be used to demonstrate compliance with AI safety requirements, 3901 please refer to G.4.8 
3902 As opposed to conventional SW architectures where the information passed through the interface is defined 3903 and fixed, in a NN architecture, the information available at the interface might change, for example the 3904 number of object classes, but the intent of the interface can remain unchanged, e.g. visualizing a classification. 
3905 Locations in an NN where latent representations are exchanged and can also be used for V&V activities, 3906 become candidates for SW unit interfaces within the NN-based system. These locations might require some 3907 form of decoding of the latent representation into one amenable to these V&V activities. Examples of such V&V 3908 activities at these points include: 
3909 ¡ª Assessing the performance of an NN-based element; for example, by comparing the NN-based element 3910 output with ground truth data. 
3911 ¡ª Assessing the level of uncertainty within the NN-based element; for example, an epistemic uncertainty 3912 could be assessed looking at the output distribution of an NN ensemble. 
3913 ¡ª Assessing the plausibility of the representations; for example, physical objects are expected to respect 3914 spaciotemporal consistency. 
3915 ¡ª Providing visualization for human inspections; for example, semantic segmentation, instance 3916 segmentation, panoptic segmentation; 
3917 ¡ª Obtaining some insight on the interpretability of an NN-based element; for example visualizing the 3918 attention of an NN-based element within its input to produce its output. 
3919 NOTE 3 Semantic Segmentation is the process of assigning a label to every pixel in the image. This is in stark contrast 3920 to classification. Instance segmentation is the task of detecting and delineating each distinct object of interest appearing 3921 in an image. Panoptic segmentation unifies two distinct concepts used to segment images namely, semantic segmentation 3922 and instance segmentation. 
3923 Once candidate interfaces are determined, potential SW subsystems and SW units within the NN-based system 3924 can be identified. These SW subsystems and SW units can be organized hierarchically into a SW architecture. 3925 A key guiding principle is to perform this decomposition with respect to system functions, where the atomic 3926 elements are SW units with still clearly assigned functional responsibilities. In contrast to SW units in 3927 conventional software, NN-based SW units can be trained end-to-end, which can evolve the latent 3928 representations passed at the interfaces. 
3929 ¡ª The developer can check whether or not all features are correctly identified therefore using the NN output 3930 as a visualization point; this defines an interface which intent is to visualize detected features. 
3931 ¡ª Initially the NN can be trained to detect n features. At a later stage the NN can be re-trained to detect n+1 3932 features. The information passed from this NN to the fusion NN has changed, from n to n+1 features, but 3933 the intent, the visualization of the features, has not changed. 
3934 In summary, the concept of an ML element, which is either a NN-based SW unit or subsystem has these 3935 properties: 
3936  a)  the function represented by the NN-based element can be swapped or composed with other functions to  
3937  implement a higher-level function.  
3938  b)  As per ISO 26262 definition of ¡°SW unit¡±,. the NN-based element can be subjected to unit-level V&V  
3939  analyses, such as testing and inspection, separately.. 
3940  EXAMPLE 2  As part of a sensor fusion architecture, combining LiDAR and map-based input signals, the NN-based  
3941  element used to create a latent representation of LiDAR features can be defined as a SW unit. The LiDAR features output  
3942  of this element could be compared, via a suitable decoder like an object detector head, with ground truth data. The NN- 
3943  based element performing the conversion between the Voxelized LiDAR data and the LiDAR features data can therefore  
3944  betested inisolationfrom theotherarchitecture¡¯selements.  
3945  ¡ª It is worth noting that with NN-based architecture, not all SW units are testable in isolation. Instead, some  
3946  SW units will be tested separately, while others will be tested after integration with other units. For  
3947  example, when the input of a SW unit is a complex representation, it might not be efficient to emulate that  
3948  representation for the sake of unit testing as opposed to testing that unit when integrated with the unit(s)  
3949  providing its input representation.  

3950 Annex G 3951 3952 3953 Architectural and Development Measures for AI Systems 
3954 G.1 Examples of architectural and development measures for AI systems 
3955 A variety of AI system development and architectural measures exist, are being improved, and new ones are 3956 emerging in this rapidly evolving field. The decision about what measures can be used to develop an AI system 3957 and their impacts on safety cannot be generally assessed in such a diverse context. 
3958 Evidence to justify the selection of development and architectural measures can include, amongst others, 3959 benchmarks, qualitative and quantitative safety analyses, ablation studies or analytical arguments related to 3960 intrinsic properties of the AI models (e.g., capability for generalization, explainability, robustness, 3961 transparency, maintainability etc.). Architectural decisions can be documented by means of strategy used, 3962 solution and evidence to support the decision. 
3963 The following sub-clauses provide details on the measures captured in Table 7¨C1. 
3964 G.1.1 Measures for Architectural Redundancy 
3965 ISO/TR 5469, Figure 5, describes some architectural redundancy patterns for systems using AI technology 3966 components. Figure G.1¨C1 translates them in the context of the reference architecture for an AI system as 3967 represented in Figure 6¨C7. 

3969 Figure G.1¨C1 ¡ª Architectural redundancy patterns for AI system 
3970 Some of the possible architectural redundancy patterns are: 3971 ¡ª Use of redundant and diverse AI models, including ensembles 3972 ¡ª Use of N-version programming of AI models 3973 ¡ª Use of supervisory and limiting logic 
ISO/CD PAS 8800:2024(en)  
3974  Usage of non-AI models as backup decision system.  
3975  NOTE 1 Depending on the use case, one or more architectural redundancy patterns are used  
3976  NOTE 2 DNN models could include redundancy and early fusion within themselves such that it is not necessary to  
3977  have late fusion on complete models. In some cases, this could lead to better nominal performance.  
3978  The architectural redundancy is expected to either enhance the AI properties such as robustness, resilience,  
3979  etc, or to add different features or functionalities or to ensure that the failure of the AI model is detected and  
3980  mitigated. If for the detection of the failure of an AI model another AI model is used then, in order to achieve  
3981  the required independence, diversity can be necessary. The following subclauses provides more details on  
3982  each of the listed architectural redundancy patterns.  
3983  G.1.1.1 Diverse redundant models  
3984  Redundancy can be achieved by voting of diverse models. It involves combining multiple AI technologies  
3985  fulfilling the same functionality, but implemented starting from different problem formulations, using  
3986  different training data or different models.  
3987  EXAMPLE There can be a set of AI models that are used for dynamic and static object detection and classification,  
3988  lane detection, and path/trajectory planning and another end-to-end AI models) that can be based on behavioural cloning  
3989  [55]. In this case the diversity is qualitatively easier to argue as the stated AI models are based of different dataset,  
3990  different input and outputs and different network architecture. The AI models on one side are learned to identify specific  
3991  obstacles and then do the specific path/trajectory planning to avoid them whereas the end-to-end AI model(s) on the  
3992  other side are learned to do the end-to-end task by identifying the drivable area without necessarily identifying the  
3993  obstacles or lanes. It is likely that there is no full equivalence/overlap of ability/performance; in such a case additional  
3994  AI or non-AI models can be added to overcome the deficiency.  
3995  G.1.1.2 Model ensembles  
3996  Ensemble methods combine the predictions obtained by multiple models to create a consolidated output [56].  
3997  The multiple models can be of different architecture and have different hyperparameters or of similar  
3998  architecturebut trained ondifferentdata sets.. These methodshave been successfully employed for improving  
3999  accuracy in object detection tasks. Various forms of ensemble methods exist. A single model, multiple data  
4000  ensemble method [57] proposes to use data augmentation for creating multiple inputs, and uses a fuzzy  
4001  integral method to combine the output across these inputs. The same model is used across different inputs.  
4002  Ensembles can be achieved by using various methods including but not limited to: bagging, boosting, random  
4003  Forest, gradient boosting, and stacking.  
4004  G.1.1.3 N-version diverse programming  
4005  In this method, multiple independent versions of an AI model that are built to predict the same output when  
4006  the same input is provided [58]. The independence and diversity are targeted to be attained via using different  
4007  training data, different AI model architectures or different training process. Some averaging or majority voting  
4008  isthenperformed to select a more robustprediction.. Theobjective isto achieve fewer common errorsacross  
4009  themultiple versionsofthemodels..Theindependenceanddiversitycan also be targeted byusingdifferent  
4010  model input (e.g. two cameras with different angle) [59]. N-version can also be achieved by using the same  
4011  base model and using different dropouts creating different models. N-version programming can improve the  
4012  fault tolerance and reliability of AI models.  
4013  G.1.1.4 Supervisory, limiting logic and non-AI backup system  
4014  It is possible that an AI system can be constrained to work within a predefined safe envelope. Safe limits  
4015  require that a subset of the action space (safe envelope) can be determined and are minimally restrictive on  
4016  safeAI components¡¯behaviour. Simple limitsonanAI model¡¯soutput(s)can result inthe.AI model to mimic  
4017  the limiter itself therefore negating the benefit. This subsystem architecture is sometime referred to as a safety  
4018  cage, which enforces behaviour onto the subsystem. Different types of online monitoring of AI System can be  
4019  implemented in the supervisor and limiting logic, such as uncertainty modelling and out-of-distribution  

4020 detection [60]. Safety monitoring is also a well-known dependability technique. This approach is generally 4021 based on a system model or the environment and on properties they are designed to guarantee [61]. 
4022 G.1.1.5 Selection techniques for architectural redundancy (voting and switching) 
4023 With architectural redundancy patterns, there is the need of a decision procedure to compute the final result 4024 based on the outputs of redundant models. A simple decision procedure could be voting-based. Taking AI-4025 based classifiers as example, different voting schemes are common:In hard voting (also known as majority 4026 voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted 4027 target label of the ensemble is the mode of the distribution of individually predicted labels. In soft voting, every 4028 individual classifier provides a probability value that a specific data point belongs to a particular target class. 4029 The predictions are weighted by the classifier's importance and summed up. Then the target label with the 4030 greatest sum of weighted probabilities wins the vote. Switching is another approach where the selection of 4031 the base predictor model is made based on predefined rules. Two types of switching strategies are 4032 common:Threshold switching: model whose predictions satisfy certain conditions or a set of performance 4033 thresholds. Typically for this case the various models have different strengths and can therefore satisfy 4034 selection criteria differently depending on the input space. Performance switching: best performing model is 4035 selected. 
4036 G.1.1.6 Usage of ¡°AI model¡± and ¡°conventional software¡± 
4037 Components using conventional software can be used to perform the plausibility checks or verification of the 4038 output generated by the AI components. They can be also used if they provide redundant functionality (i.e., 4039 same objectivesare also met, e.g.,classical computer vision that doesn¡¯t involve AI). In both the casesthe intent 4040 is to detect the errors of the AI components. Switching to a system based on conventional software is a fallback 4041 possibility in case of error detection related to AI model. 







4042 G.2 Qualitative and quantitative analysis of AI architectures 
4043 This clause describes aspects related to a systematic development process of the AI component. A rigorous 4044 and systematic development process provides evidence that activities during development lead to safe 4045 outcomes and that AI work products have been analysed for systematic faults. For example: 
4046 ¡ª hyperparameters can be checked by peer reviews. 
4047 ¡ª it can be checked whether the AI architecture meets the AI safety requirements, or whether individual 4048 elements violate AI safety requirements allocated to them. 
4049 ¡ª an analysis of the AI architecture can be used to identify failures in the system at interpretable interfaces, 4050 and incorporate appropriate mechanisms for monitoring. 
4051 To support those activities, it is necessary to give preference to AI architectures with transparent interfaces 4052 between their AI components and interpretable representations so that errors become measurable. 
4053 EXAMPLE 1 For DNNs, the computational graph provides a reasonable opportunity for analysis, because the 4054 framework generates it automatically and it can be considered as an application graph. Object detection models, including 4055 many Faster R-CNN based ones, include operations on lists of objects in the application graph. Their graph contains 4056 interpretable information about sets of object bounding boxes, which are refined in several steps. Potential objects can 4057 be discarded by an operation that limits the number of elements in the list. In the worst case, e.g., this results in a 4058 pedestrian not being detected due to this operation. The impact of the limitation operation on safety requirements is 4059 considered because it is a potentially systematic fault in the design. 
4060 Architecture analyses aiming to discover the potential for contributing AI errors will investigate the tolerance, 4061 adaptability and information flow of the proposed architectural concept. Investigation into the tolerance of 4062 the architectural construct will provide arguments and evidence addressing at least the following: 
4063 ¡ª how failures of sub systems that provide inputs are tolerated, 
4064  ¡ª how the concept processes inputs inconsistent with existing training, test and validation data,  
4065  ¡ª how faults and failures internal to the computation are tolerated,  
4066  ¡ª how adversarial attempts to disrupt the computation are tolerated and  
4067  ¡ª how incorrect computation outputs are tolerated.  
4068  Investigation into the adaptation of the architectural construct will provide arguments and evidences  
4069  addressing at least the following:  
4070  ¡ª how the architecture concept prevents or mitigates unauthorised adaptations,  
4071  ¡ª computational behaviour is assured before, during and after an adaptation and  
4072  ¡ª how new requirements are fulfilled.  
4073  Investigation into the information flow of the architectural construct will provide arguments and evidences  
4074  addressing at least the following:  
4075  ¡ª information between sub-systems and interacting systems,  
4076  ¡ª how information is available to support maintenance and future development and  
4077  ¡ª how information is structured and stored to support real-time and post-incident analysis.  
4078  This allows potential safety-related failures to be identified. Nevertheless, failures are tolerable within certain  
4079  limits. However, the occurrence of the failures depends on the specific trained DNN components. The safety  
4080  analyses are supported by further measurements of a trained DNN, which quantify the concrete occurrence of  
4081  failures. Based on this measurement appropriate safety measures can be derived. Suitable safety analysis  
4082  techniques, their assumption, advantages and limitations are discussed in the Table 13¨C1. For a sufficiently  
4083  described AI component, the individual elements can be systematically analysed. In particular, the description  
4084  can represent AI-specific aspects, such as data representations using tensors in the case of DNNs, or the  
4085  different development and operation phases of theAI components such astrainingorinference.  
4086  EXAMPLE 2 The hierarchical operations of the application graph can be analysed using the HAZOP method to a  
4087  reasonable level in the hierarchy. For this purpose, the HAZOP guide words are interpreted accordingly, and the analysis  
4088  examinesifadeviationcanlead to theviolationof safety requirements. Forexample, theguideword ¡°too large¡± canbe  
4089  associated with an element in the architecture such as an object bounding box, and the consequences are assessed to  
4090  identify whether a safety requirement is violated. In a following step where the network is now trained, it can be observed  
4091  how often the violation occurs on real data. If it is too often, additional measures can be taken iteratively to reduce the  
4092  violation.  
4093  G.2.1 Identifying software units within AI architectures  
4094  ISO 26262 defines the concept of a SW unit its interface, and the associated SW architecture as central to the  
4095  modular development and verification and validation (V&V) of software.  
4096  Annex F presents a method to decompose an AI architecture into elements that can be treated as software  
4097  units similarly to the concept of software units presented in ISO 26262:2018 -part 6. In doing so, most of the  
4098  software development measures specified in ISO 26262:2018 can be applied, with some tailoring (see Annex  
4099  C), to gain confidence in the correctness of each software unit.  
4100  Additionally, each element of the AI architecture can be probed during development and forensic analyses and  
4101  provide useful information such asconfidence levels or representations of the AI element¡¯s output for human  
4102  inspection, helping in the resolution of insufficiencies.  

ISO/CD PAS 8800:2024(en)  
4103  G.3 Data distributions and their impacts on AI models  
4104  AI systems are prone to poor generalization when the distributions of the development data set (training and  
4105  validation)differ fromthose of the test data set and/or the deployment (real life) data sets.. There are typically  
4106  two types of distribution related problems associated with AI models: distributional shift and out of  
4107  distribution (OOD) samples. Distributional shift refers to changes within the same underlying distribution,  
4108  while out-of-distribution data refers to data that is significantly different from the data the model was trained  
4109  on. In both cases a previously well-trained and validated model might suffer from reduced generalization  
4110  performance.. 
4111  G.3.1 Out of distribution data and its mitigation  
4112  OOD data might cause the model to make incorrect predictions or be overly confident in its predictions.  
4113  Examples of OOD samples in autonomous vehicles can include unique, unusual or unknown road signs, road  
4114  marks, rare objects or scenarios not seen in the training data set. Architectural and development measures  
4115  can help detect OOD and make the encompassing system robust to OOD data.  
4116  G.3.1.1 Architectural measures to address OOD robustness  
4117  Architectural measures to enhance robustness to OOD data include but are not limited to:  
4118  Ensemble models: combine the predictions of multiple models to make a final prediction. This can help to  
4119  reduce the impact of individual models that might be sensitive to OOD data. Please refer to G.1.1.2 for details  
4120  Probabilistic Models: use probabilistic methods (such as Bayesian NNets) to model the predictive  
4121  uncertainty of a model. This can help the model to better handle OOD data by being more cautious about  
4122  making confident predictions on data that is different from the training data.  
4123  Domain Adaptation: involves adjusting the model architecture as needed to better reflect new features in the  
4124  new domain.  
4125  Open set recognition: is a technique to enable models to be robust to unknown classes. These include  
4126  measures such as novelty detection, outlier detection and threshold-based classification.  
4127  OOD Error Detection using reject function: is a technique where both in distribution and out of distribution  
4128  data is considered as an input during the modified training step [62]. Furthermore, additional nodes as reject  
4129  functions are used in the output layers of the AI model, this represents multiple reject classes.  
4130  NOTE All the OOD detection measures need to be designed in such a way that the OOD inputs are identified and  
4131  rejected while meeting the relevant timing related safety requirements.  
4132  G.3.1.2 Development measures to address OOD robustness  
4133  Domain Adaptation: involves adapting the model to the new domain by re-weighting the loss function to  
4134  better reflect new features in the new domain.  
4135  Data pre-processing: Pre-processingof data before trainingcan improve model robustness.. Some methods  
4136  include methods such as normalization, feature scaling, outlier removal and data augmentation (adds diversity  
4137  to dataset)... 
4138  Training measures: methods such as calibrating the prediction certainty of a model (see G.4.4), ensuring that  
4139  the validation/test data sets are significantly different from the training set.  
4140  Adversarial training: these methods involve using adversarial training techniques to train the model on  
4141  examples that are specifically designed to fool it and can also be used to establish OOD robustness. Examples  
4142  include stickers on road speed limit signs.  
4143  G.3.2 Distributional shift and its mitigation  

4144  Distributional shift refers to changes in the distribution of the data that occur within the same underlying  
4145  distribution. For example, if a model is trained on images of cars taken primarily during the day and then  
4146  tested on images of cars taken only during night, this would be a distributional shift because the data  
4147  distribution has changed within the same underlying distribution of cars but impacted by lighting conditions.  
4148  One relevant example is the sudden appearance of face masks during the COVID pandemic. Face recognition  
4149  algorithms were impacted by a shift in the distribution of faces. Distributional shift can lead to a drop in model  
4150  performance because the model has not seen enough examples from the new distribution during training and  
4151  might not generalize to the new data. In some cases, the shift might also occur from concurrently occurring  
4152  effects such as for example low light and rain even though the model has been trained on each effect  
4153  individually.  
4154  G.3.2.1 Types of distributional shifts  
4155  Monitoring of AI models is achieved by assessing shifts in three primary distributions associated with AI  
4156  models. These distributional shifts are:  
4157  a) Covariate shift refers to a shift in distribution of the input features between the source and target  
4158  domains while the input/output (the model) relationship remains unchanged [63],  
4159  EXAMPLE 1 Target domain has frequent occurrence of low-light driving conditions, but source (training) data did not  
4160  includesufficientdatafromlowlightconditions..Thedeployed MLmodelwillmakeconfidentbut incorrect predictions  
4161  in the now predominantly low light driving conditions.  
4162  b) Label shift, also knownasprior shift, refersto a shift inthe distribution of themodel'soutput.. Label shift  
4163  arises when class proportions of the labels differ between the source and target, but the input  
4164  distributions of each class and the input/output (the model) relationship remain the same.. One  
4165  pathological case is target data imbalance between source and target domains.  
4166  EXAMPLE 2 Training was performed on clearly separated classes of humans and vehicles, but the target domain is a  
4167  very crowded scene of multiple variants of vehicles and people. The deployed ML model might totally ignore certain  
4168  vehicle classes.  
4169  c) Model shift or concept shift is a shift in the input/output relationship describing the model. That is  
4170  ¡°same input differentoutput¡±...Concept ormodel shift can occur if the environment itself shiftsbetween  
4171  training and deployment.. Concept shift can be gradual, sudden or recurrent.. 
4172  EXAMPLE 3 Training data was from driving environments that do not allow turns at red lights, but the target domain  
4173  environment allows turns on red light. The deployed ML system cannot anticipate road users will turn on red lights.  
4174  G.3.2.2 Distributional shift monitoring  
4175  This clause discusses monitoring of AI models post deployment for detecting and quantifying shifts in the  
4176  behaviour of AI models. The clause also describes the need for mitigating actions to manage the impacts from  
4177  such distributional shifts on the performance of deployed AI models.  
4178  NOTE 1 For the purpose of this document, shift implies distributional shift.  
4179  NOTE 2 A deployed AI model is said to have shifted (or drifted) if the response of the AI model in deployment differs  
4180  fromtheexpected responseachieved during development.. 
4181  The development environment is referred to as ¡°Source¡±.  
4182  The deployment environment is referred to as ¡°Target¡±.  
4183  NOTE 3 Shift in the behaviour of a deployed AI model will occur when the distributions of data and/or model in the  
4184  target domain are different from those in the source domain.  
4185  NOTE 4 Model monitoring methods are essential for monitoring the behaviour of AI models in the target environment.  

4186 There are three fundamental objectives of any monitoring strategy: 
4187 ¡ª Detect: in the target domain, detect the occurrence of one or more of the three primary distributional shifts, 4188 preferably, with as few samples as possible. 
4189 ¡ª Quantify: characterize and quantify the distributional shifts associated with the deployed AI model. 
4190 ¡ª Mitigate: set of architectural and development measures that mitigate the impact of shifts. 
4191 G.3.2.3 Distributional shift detection and mitigation 
4192 The AI system can provide alerts when facing unexpected inputs, and/or changes in the input distribution. ML 4193 models can also provide a confidence score associated with their predictions. There are typically two classes 4194 of approaches that can be adopted for monitoring shifts and providing alerts. 
4195 a) Distribution based methods rely on estimating distance between distributions such as KL-divergence, or 4196 population stability index [64]. 
4197 b) Anomaly detection-based methods rely on single point out of distribution methods such as anomaly or 4198 outlier detection. 
4199 While detecting data shift is critical, actions that mitigate the impact of shifts also are considered. Where 4200 possible, shift-related risk prevention actions are considered during development.. Additionally, shift-related 4201 risk reduction provisions are provided during deployment, these include offline methods and online methods: 
4202 c) Architectural measures: Any measures that reduce model complexity and tendency to overfit. Feature 4203 ablation is one approach that eliminates features that are sensitive to shift but have small predictive 4204 power. 
4205 d) Development measures: Development measures are data adequacy measures that reduce the 4206 incidence/severity of distributional shifts post deployment...Methods include data diversity with 4207 adequate and balanced representation of the operating domain, including but not limited to variations in 4208 sensing (sensor state) and environment [65]. Data augmentation via synthetic data sources is another 4209 approach for achieving data sufficiency. However synthetic data might not be the primary data source for 4210 model development.. AI safety requirements related to data to minimize the incidence of shifts in AI 4211 models post deployment are covered in Clause 11. 
4212 NOTE 1 Development phase mitigation approaches such as importance-weighted empirical risk minimization 4213 (IWERM) can be used but this method relies on prior knowledge of the target domain [66]. 
4214 e) Post-deployment measures: Offline methods refer to techniques where shifts are determined from in-use 4215 data that has been off-boarded to a cloud server. 
4216 NOTE 2 Data shifts are identified and quantified. If the effect of distributional shift is proven or estimated to be leading 4217 to contributing AI error, the associated model/s are retrained in an offline manner and the new weight/biases are 4218 updated toallimpacted assets..This is atypeof iterativelearning. Domainrandomizationis anotherofflinetechnique 4219 leveraged during training and relies on exhaustive simulations. 
4220 Online methods refer to techniques where any retraining or adaptation is performed in real-time. 
4221 NOTE 3 One online approach often utilized is to have an ensemble of inferencing models, where a comparison between 4222 the predictions of the main and surrogate models are utilized to provide prediction uncertainty. Online methods can be 4223 subject to other requirements. 
4224 G.4 Training safety measures 
4225 G.4.1 Hyperparameter tuning 
4226  Hyperparameters are model-training related parameters that cannot be learned from data and are set prior  
4227  to training. The choice of hyperparameters can have a significant impact on model performance and its  
4228  robustness and hence a robust set of hyperparameters must be determined.  
4229  Typical hyperparametersarefor DNNs: learningrate, number ofhidden layers, numberof unitsperlayer,  
4230  batch size, number of training epochs,.regularization parameters, activation functions, dropout rates etc... 
4231  Typical methods used for hyperparameter tuning include: random search (large set of hyperparameters), grid  
4232  search (small set of hyperparameters), Bayesian optimization, hyperparameter scaling, etc..Batch  
4233  normalization is another approach that can make an AI model robust to the choice of hyperparameters.  
4234  In practice hyperparameter tuning can be done either at a single model level though systematic iterations or  
4235  through runningmultiple modelssimultaneously... 
4236  Hyperparameter tuning allows fine-tuning a model's to achieve the best balance between underfitting and  
4237  overfitting, leading to better overall performance. Underfitting can be managed by increasing model  
4238  complexity via tuning hyperparameters like adding more layers or units can help the model capture more  
4239  intricate patterns in the data. Overfitting can be addressed by reducing model complexity through  
4240  regularization or adjusting learning rates can prevent the model from fitting the training data too closely and  
4241  improve its generalization to unseen data.  
4242  G.4.2 Robust Learning  
4243  Robustness of AI systemsis very relevant for achievingsafety of the intended performance.. Good robustness  
4244  isinmanywaysan extension of good generalizationof an AI system.. Therefore, a robust AI systemisone that  
4245  isinvarianttosmall perturbationsinthe inputs.. Typical sourcesof input perturbations include adversarial  
4246  input examples and noise corrupted input examples. Methods for improving robustness include both training  
4247  and architectural methods.. Some typical methods for improving the robustnessof AI modelsarediscussed.. 
4248  1) Data pre-processing techniques such as cleaning, normalization and scaling are training time methods  
4249  that improve model robustness.. 
4250  2) Regularization is a training approach to prevent AI models from over-fitting.. Thisimprovesamodel¡¯s  
4251  generalizability and stability making the model robust to unseen or new data..Typical regularization  
4252  methods include L1, L2, Lasso, Dropout etc.  
4253  3) Adversarial training is a learning method that improves model robustness by injecting noise into the  
4254  input duringtraining, therebymimickingattacks.. One such approach to improve adversarial robustness  
4255  is to use randomized smoothing.  
4256  4) Error analysis methods help isolate patternsinamodel¡¯s behaviour andcanbe used toinformmodel  
4257  adjustments for improved robustness.  
4258  5) Ensemble methods such as bagging, boosting and stacking help improve robustness by combining  
4259  predictions from multiple models, however, these are computationally expensive methods and might not  
4260  be preferred for compute constrained applications.. Please refer to G.1.1.2 for details.  
4261  6) Domain Randomization (or generalization) approaches make the models robust to domain shifts,  
4262  where the source and target domains are shifted (sunlight to low light) but the tasks remain the same.  
4263  Thiscan also be considered asadataaugmentationtechnique.. Domainrandomization also makesthe  
4264  modelsmore robust to real world scenarios... 
4265  7) Fault Aware Training: When the AI system disturbances are predictable (e.g. for hardware errors), fault- 
4266  aware training that includes error modelling during training [67] can be used to make neural networks  
4267  resilient to specific fault models on the device.  
4268  G.4.3 Transfer learning  

4269  Transfer learning is an effective method to leverage some knowledge learned from a possibly different source  
4270  domain and tasks and reuse it for the target domain and tasks. Transfer learning can help alleviate a lack of  
4271  target domain data or simply speed up the task(s) performance increase [68]. Transfer learning methods can  
4272  be classified based:  
4273  ¡ª on the availability of information for the source and target domains; methods are classified as  
4274  ¡°Transductive transfer learning¡±, ¡°Inductive transfer learning¡±or¡°Unsupervised transfer learning¡±.  
4275  ¡ª on the level of consistency between the sources and target features and label spaces; methods are  
4276  classified as ¡°Homogeneous transfer learning¡±or¡°Heterogeneous transfer learning¡±.  
4277  In re-using a model trained on a large corpus of data, transfer learning can foster generalization and  
4278  robustness of the model resulting from the transfer [69]. However, transfer learning presents some  
4279  challenges:  
4280  ¡ª Risk of emergent properties, such as biases, inherited from the source model.  
4281  ¡ª Transfer learning can result in reduction of performance compared with a model training solely on the  
4282  source domainwithout transfer learning. Thisisreferred to as¡°negative transfer¡±andmight occur when  
4283  there is not enough relatedness between the source and target domain or tasks, or between the source  
4284  and target feature spaces.  
4285  An appropriate assessment of relatedness between the source and target domains, tasks and features,  
4286  followed by an appropriate extraction of the knowledge to transfer, can justify the use of transfer learning for  
4287  safety-related AI systems. Sufficient V&V to meet the target domain KPIs is also required to ensure the  
4288  effectiveness of the transfer learning.  
4289  NOTE If the source model is complex then the explainability and transparency of the model resulting from transfer  
4290  learning can be adversely affected.  
4291  G.4.4 Confidence calibration and uncertainty quantification of AI models  
4292  The prediction certainty of the AI components can be estimated, where applicable, and calibrated for high  
4293  certaintyof predicted confidence duringdesigntime.. The confidence score ofapredictor isdefined asthe  
4294  predicted probability of correctness, ex. 90% confident this is a car. The observed accuracy, however, is the  
4295  observed (in-use) probability of correctness, ex. 8 correct predictions on 10 predictions is an 80% observed  
4296  accuracy.. Usingthese metrics, acalibration error (CE)can be calculated as:Calibration Error=|Confidence  
4297  Score ¨CObserved Accuracy|..For AI modelsthat do not explicitlyreport aconfidence score,thepredictive  
4298  certainty can still be established at design time via measures that establish desired statistical properties of  
4299  predictions over expected variations of the covariates that cover anticipated noise factors in real life  
4300  applications. These can include simple input perturbations such as, for example, variations in lighting  
4301  conditions. Typically, the calibration error is minimized during design time. There are several techniques  
4302  commonlyused tocalibrate model'spredictive certainty.. These include,Brier Score,Temperature Scaling,  
4303  Isotonic Regression, Platt Scaling, etc.. It is important that a suitable scoring rule be used to encourage honest  
4304  forecasts by the predictor... 
4305  A model's predictive certainty and calibration error is re-evaluated periodically during a models life-cycle..  
4306  Thisis especially critical for any retraining of a model.  
4307  G.4.5 Verifying feature selection  
4308  Verification of feature importance or relevance can support the development of AI models that fulfil their AI  
4309  safety requirements. An optimal feature set not only prevents overfitting and improves generalizability but  
4310  also makesa model more interpretable.. Feature verification isacombinationof data pre-processing, feature  
4311  selection/engineering, model training and testing, feature importance and feature sensitivity analyses[70].. 
4312  For deep learning models, such as computer vision models, feature selection can be automated such as through  
4313  the use of convolution filter kernels (edge detectors). However, features can still be studied via visualization  

4314 of feature heat mapsatvariouslayers of the model.. Saliency mapsareanexample of such amethod and are 4315 used for understanding visual properties of an image to help improve performance of computer vision tasks. 4316 Another approach is to use Shapley values that attribute to each feature their contribution to the final output. 
4317 G.4.6 Monitoring multiple scores 
4318 Monitoring multiple scores during model training provides a more comprehensive picture of a model¡¯s 4319 performance and its insufficiencies. Additionally monitoring multiple scores also helps identify and reduce 4320 overfittingandtherebyimprovesamodelsgeneralization capability...There areseveral well-known model 4321 scoring metrics that can be used to choose the appropriate scoring method for the AI model being used. For 4322 example, accuracy is a common metric used to evaluate classification models, but it might not be the best 4323 metric to use if the classes are imbalanced. In such cases, metrics such as precision, recall, and F1-score might 4324 provide more useful information about the model's performance. Some commonly used metrics include: 
4325 ¡ª Accuracy 
4326 ¡ª Precision 
4327 ¡ª Recall.
4328 ¡ª F1-Score 
4329 ¡ª Area under the Receiver Operator Characteristic Curve 
4330 ¡ª Mean average precision 
4331 ¡ª Intersection overunion 
4332 G.4.7 Attention or saliency Maps 
4333 Attention or saliency maps can improve the explainablity of AI components by providing explanations/ visual 4334 aids why the AI component made certain prediction, which features led the AI component to take certain 4335 decisions [71] or which regions of the input are more important for prediction of AI component [72]. 
4336 The explanations provided by the attention maps can be reviewed by the stakeholders for identifying the 4337 problems in the AI component architecture or development process (e.g. training set improvement) such that 4338 they can enhance or introduce other architectural or development measures. 
4339 NOTE The properties of the explanation such as completeness and correctness are heavily dependent on aspects 4340 such asthequality and thequantity of theinputsconsidered duringthetraining process..
4341 Using the visualization provided by attention and saliency maps can improve the robustness [73] and reduce 4342 the bias of the AI components by focusing more on and the most relevant features of the input space. AI 4343 components that can produce more interpretable saliency maps can be more robust to the adversarial inputs 4344 [73]. 
4345 G.4.8 Interpretable latent features 
4346 Learned or identified latent features in an AI model can be utilized as evidence to ensure that specific 4347 requirements are fulfilled. The methods Supervised concept extraction [74], Unsupervised concept mining 4348 [75] and; Concept bottleneck architectures [76] can be used to obtain evidence for learned features, which can 4349 be utilized so that an expert can derive possible failures that a learned AI model has, guiding training or update 4350 strategies. 
4351 G.4.9 Augmentation of Data 
4352 Data augmentation techniques can improve the performance and robustness of machine learning models by 4353 increasing the amount and diversity of training data, reducing sensitivity to noise, and improving 4354 generalization to unseen data. Data augmentation is useful in a variety of scenarios especially when datasets 4355 are small or imbalanced, the model is complex and prone to overfitting.. Data augmentation can be applied at 4356 different stages during the design phase (train, dev-test) or after evaluating in-use performance.. There are 4357 additional considerations that inform the need for data augmentation, these include (but are not limited to) 4358 an assessment of structural coverage (see G.4.9.1) as well as test data coverage (see G.4.9.2)..
4359 G.4.9.1 Structural coverage of an AI component 
4360 Some methods for model coverages are proposed and are based on Structural coverage metrics at the software 4361 unit level in ISO 26262. In these methods, augmentation or filtering of input data based on structural Coverage 4362 of AI component is used: 
4363 1) to ensure comprehensiveness of testing,.
4364 2) to identify an undesired behaviour of the AI model, and 
4365 3) to obtain methodical evidence towards robustness 
4366 Examples of coverage methods are: 4367 ¡ª Neuron Coverage: assessing if all neurons are activated during testing [77]; 
4368 ¡ª Sign-Sign Coverage: confirming that a change of sign in one or more AI component inputs has the desired 4369 change of sign in one more AI component outputs [78]; 
4370 ¡ª Sign-Value Coverage: confirming that a change of sign in one or more AI component inputs has the desired 4371 change of value in one more AI component outputs; 
4372 ¡ª Value-Value Coverage: confirming that a change of value in one or more AI component; inputs has the 4373 desired change of value in one more AI component outputs; 
4374 ¡ª Value-Sign Coverage: confirming that a change of value in one or more AI component inputs has the 
4375 desired change of sign in one more AI component outputs. 4376 NOTE As these methods are white or open box methods, they can help improve the overall trustworthiness in the 4377 AI components by building confidence due to some level of transparency and possibly explainability. However, to be 4378 adopted as state-of the art, these methods need to be developed further to have more solid and evidence-based 4379 relationship with the AI properties. 
4380 G.4.9.2 Data coverage techniques for test data augmentation 
4381 Data coverage techniques based on dataset characteristics[65] can be used to inform data augmentation. A 4382 suitable method is used to determine the appropriate augmentation method(s)..
4383 EXAMPLE Dataaugmentationcanbeinformedviadatacoveragetechniquessuch as:.4384 1) equivalence partitioning: partitions a set of test cases into groups with each group representing a different scenario. 4385 Theobjectiveis to test each partitionwithanequivalent set of test cases..
4386 2) centroid positioning: this is a technique for selecting test cases that are representative of the mean of the data 
4387 distributionand representsan¡°onaverage¡± performanceof themodel. 4388 3) boundary conditioning: this technique selects test case at the boundaries/edge of the distribution representing the 4389 inputdomain..
4390 4) pair-wiseboundaryconditioning:inthismethodpairsof boundarytest casesareselected to.coverallcombinations 4391 ofedgecases..
4392 The method helps identifying the potential bias in the data (e.g. by using equivalence partitioning). By 4393 augmenting the dataset based on the defined techniques this method can also help identify issues in the AI 4394 model related to accuracy and thereby supporting predictability improvement. This method can also help 4395 identify the feature space relevance for prediction by understanding sensitive dataset properties there by 4396 supporting transparency and interpretability. 








4397 G.5 Monitoring and AI system modification 
4398 G.5.1 Dynamic Environment Monitoring 
4399 The AI system and AI components are designed by making certain assumptions about the environment that 4400 they need to operate in. During the design time their behaviour is verified and validated to ensure that they 4401 operate correctly when all the assumption are met. However when certain assumptions become invalid it is 4402 likely that the correct functioning of AI system / AI components cannot be guaranteed. Dynamic environment 4403 monitoring involves monitoring assumptions made during design at runtime such that if they become invalid, 4404 appropriate actions can be taken to ensure continued AI safety [57]. 
4405 EXAMPLE The example assumption can include the relative speed of other traffic participants, weather conditions, 4406 lighting conditions, road conditions (e.g., construction zone). 
4407 The capabilities for realising this measure can include specific AI components for monitoring the input space 4408 and operating environment.. Such components can be non-AI based components or less complex AI 4409 components as they will be acting as a safety measure. Where necessary to detect the violation of the 4410 assumptions, appropriate threshold for the attributes of the operating environment can be considered. 
4411 G.5.2 AI Model Modification 
4412 AI models can become stale or lose performance over time due to a variety of reasons, such as from 4413 distributional shift.. These include designtime issuessuch asinherentlyinadequate predictive powerof the 4414 model, data and/or algorithmicbiasanddeployment (or inuse) issuessuchasshiftindistributionsbetween 4415 training and deployment and the presence of outliers or out-of-distribution (OOD) examples...
4416 To overcome such problems AI models might need to be updated/re-trained.. However, retraining or 4417 continual learning of AI models can be challenging and issues such as catastrophic forgetting[79][80], 4418 catastrophic remembering [81] etc. need to be addressed where necessary..
4419 G.5.2.1 Criteria for Retraining 
4420 The quality of the safety related performance of an AI element is defined by its input space and the 4421 functionalities (behavioural, prediction, etc.) required to fulfil the intended mission within the operational 4422 domain (OD) i.e., the environment where the system is deployed. 
4423 Partial or full retraining is performed when there is evidence that the AI system can no longer safely fulfil its 4424 mission. These inadequaciesof the AI systemmight result from: 
4425 ¡ª A mismatch between the AI systems¡¯ input space and its OD..
4426 ¡ª Shifts in distributions between training and deployment (see section Clause G.3), for example the level of 4427 traffic density increased over time, there are new agents to interact with, there are new road signs etc. 
4428 ¡ª A domain shift where the AI system is deployed in an OD that is not within a reasonable generalization 4429 distance from the input space used for development.. For example, AV driving policies developed for 4430 country A are deployed in country B. 
4431 ¡ª A mismatch between the learnt competencies of an AI system and the competencies required in 4432 deployment. This can result from: 
4433 a) The proven inability or insufficient ability of the AI system to manage situations within its input space, for 4434 example incidents, near-misses or misdiagnoses are reported. 
4435 b) A change in the regulations or in the relevant safety standards covering the input space so that expected 4436 performance levels are modified, for example the tolerated rate of false negative is reduced. 
4437 NOTE Partial or full retraining cannot be dictated by the type of discrepancy or the amplitude of the discrepancy but 4438 by the confidence in either approach to guarantee that the discrepancy is addressed and no safety-critical regression 4439 results from the retraining. 
4440 G.5.2.2 Targeted and controlled model update 
4441 Successive iterations of developing and training an AI system can be required in order to achieve the AI safety 4442 requirements.Retraining with additional data might completely change the behaviour of model, and thus the 4443 controllabilityof the outcome islimited, i.e.,difficult toaddresstarget insufficienciesorprone tointroduce 4444 undesirable regression. 
4445 For traditional program code, we have clear modularization and thus can analyse which functions are likely 4446 to be affected when changes are made, and which tests are to be executed again. In the case of ML models, all 4447 the tests are executed again as change in the outcome might not be able to systematically predicted. However, 4448 in some cases, differential testing can be used to understand the differences between the original and re-4449 trained models. For example, the inputs where the current model fails but the previous version succeeded, are 4450 examined. 
4451 There have been the followingapproachesto addressthisproblembytargeted andcontrolled model update:.
4452 ¡ª Additional optimisation objectives (e.g., in the loss function) can be used to penalise regressions, i.e., 4453 failures on inputs for which a previous version of the model succeeded.The balance between improvement 4454 and regression can be investigated as a hyperparameter such as weights between the original fitness and 4455 the penalty. 
4456 ¡ª Model repair techniques identify neuronsor parameters suspicious/responsible for critical errors or 4457 significant successes. Then, important parameters for successful behaviour in the past version can be 4458 frozen or focus on re-optimization of the undesirable behaviour in the current version. This way of model 4459 update can be done directly by focused optimization or indirectly by data augmentation that stimulate the 4460 relevant neurons..
4461 NOTE Retraining can be implemented in a manner that avoids model regression. Regression of model performance 4462 canbeestablished based onthemodel¡¯straining history [79]. 





4463 G.6 Alignment of intention 
4464 AI Alignment addresses the concern of AI system's behaviours in achieving some human-designed objectives 4465 being not aligned with human-level expectations with respect to values and objectives. Complex AI systems 4466 might seek to aggressively achieve an objective with disregard for other factors that can result in harmful, 4467 unsafe or unethical behaviours. Typically misalignment is a result of a mismatch between human-defined 4468 objectives and values and the behaviour the AI system exhibits in achieving those objectives. As an example, 4469 a shortest-time requirement on a self-driving car is by itself an incomplete requirement and might lead to the 4470 AI system acting in a hazardous manner to achieve this objective. The same requirement subject to constraints 4471 that prevent violation of safety or other values can be designed to achieve full alignment. For example, 4472 Qualitative methods include Value specification, Adversarial Testing, or Ethical Framework and Quantitative 4473 measures could include, Reward Modeling, Robustness testing, etc. 
4474 G.7 Considerations related to the target execution environment 
4475 This clause relates to requirement 10.3.11 regarding the demonstration of AI safety within the target 4476 execution environment.Classes of evidence that support this claim include targeted safety analyses and 4477 assumptions on the execution integrity of the target platform. 
4478 G.7.1 Optimization of parameters and optimization of architectural entities of AI components 
4479 Optimizations of parameters and model architectures can be applied to meet the constraints of the target 4480 execution environment. The optimizations can alter the size and the complexity of the AI components such 4481 that lesser spatial and temporal resources are required. The basis datatype used for the target execution 4482 environment can be smaller in size, precision and accuracy than the one that is used during training. Also, 4483 certain neurons, their weights, their filters, or channel can be pruned. In addition to fulfil the target constraints, 4484 pruning can lead to improve the computation efficiency of AI components. 
4485 Pruning can lead to better generalization by preventing overfitting. The reduced network dimension can 4486 contribute towards the improving the interpretability using visualization methods, analysability of AI 4487 component architecture. 
4488 G.7.2 Knowledge distillation also known as teacher-student model 
4489 Optimized models can be derived from larger or more complex models to meet target execution environment 4490 constraints. Hence, optimized AI-models are trained to: 
4491 ¡ª follow larger model behaviour; 
4492 ¡ª minimize differences between the outputs of both models. 
4493 Knowledge distillation can be applied to improve generalization and accuracy of optimized AI-models 4494 compared to the larger AI-model they are derived from. Since optimized AI-models are usually lower in 4495 complexity and size the risk related to interpretability will also be reduced. 
4496 G.7.3 Analysis for differences 
4497 There are many factors that can be different between the development environment and target execution 4498 environment such as: 
4499 ¡ª Hardware (e.g., GPU, accelerators) and Hardware Resources (e.g., memory, storage); 
4500 ¡ª Supported data size; 
4501 ¡ª Software Dependencies (e.g., libraries, operating system); 
4502 ¡ª Tools (e.g., compiler). 
4503 Such an argument is based on thorough analysis of all differences. However, in case detailed analysis of AI-4504 model differences is not feasible (e.g., due to the high complexity of AI-models) specific acceptance tests can 4505 be run on both development and target execution environment. Test results can be compared to identify any 4506 potential increase of risk and build or support the argument. 
4507 4508 4509 4510  Annex H Typical performance metrics for machine learning  
4511 4512  In this annex, some of the widely-adopted performance metrics for machine learning is described. Here, these metrics are categorised into metrics for regression and metrics for classification.  
4513  a)  Metrics for regression  

. ISO 2024 ¨CAll rights reserved 
4514 4515 
4516 4517 
4518 4519 4520 
4521 4522 4523 
4524 
4525 
4526 
4527 4528 4529 4530 
4531 4532 
4533 
4534 
4535 
4536 4537 4538 4539 
4540 4541 
4542 
4543 
¡ª Mean Squared Error (MSE): The average of squared difference between the ground truth values and the predicted values from the model. 
^ 
1¡Æ..
...... = ..=1(.........)2, Formula H¨C1 
.. ^ 
where ..is ground truth, ..is the predicted output, and ..is the number of datums. Range: [0,¡Þ], with 0 being the best. MSE is a differentiable metric and can be well optimized. However, MSE penalizes small errors (by squaring 
the terms), leading essentially to an over-estimation of how bad the model is. 
¡ª Mean Absolute Error (MAE): The average of the absolute differences between the ground truth values and predicted values from the model. 
^ 
1¡Æ..
......= ..|,Formula H¨C2 
.. ..=1 |....... ^ 
where ..is ground truth, ..is the predicted output, and ..is the number of datums. 
Range: [0,¡Þ], with 0 being the best. 
MAE is similar to MSE but more robust towards outliers than MSE, as it does not exaggerate errors by squaring those terms. MAE indicates how far the predictions were from the ground truth. However, MAE does not indicate the direction of the error, i.e. whether the data were under-predicted or over-predicted. MAE is typically used to assess how close the predictions are to the ground truth on average. 
¡ª Mean Absolute Percentage Error (MAPE): Average absolute percentage difference between predicted values and actual values. 
1¡Æ.. |(.........)|
........= ..=1 , Formula H¨C3 
.. .... 
where ..is the number of fitted points,.... is the actual value, and .... is the predicted value. 
Range: [0,1], with 0 being the best. 
MAPE measures the average magnitude of error produced by a model, or how far off predictions are on average. It is often used as the loss function in regression problems and forecasting models due to the intuitive interpretation in terms of relative error for evaluation. MAPE is not suggested to be used when actual values can be at or close to zero. 
¡ª Root Mean Squared Error (RMSE): Square root of the average of the squared difference between the ground truth values and the predicted values from the model. 
^ 
¡Ì1¡Æ..
........=(....... Formula H¨C4
..=1 ..)2 .. ^ 
where ..is ground truth, ..is the predicted output, and ..is the number of datums. 
4544 
4545 4546 4547 
4548 4549 
4550 
4551 
4552 
4553 4554 4555 4556 4557 
4558 4559 
ISO/CD PAS 8800:2024(en) 
Range: [0,¡Þ], with 0 being the best. 
RMSE has an advantage over MSE with the handling of the penalization of smaller error by square rooting the error terms. RMSE is often used for large numbers (prediction or ground truth) for hyper-parameter tuning or batch training a ML model. RMSE focuses on penalizing large errors. 
¡ª R-Squared: a measure of the proportion of the variance of a dependent variable that is explained bythe regression model. 
^ 
¡Æ.. ..=1 (.........)2 
..2=1. Formula H¨C5 
¡Æ.. ..=1 (.........)2 
^ 
where ..is ground truth, ..is the predicted output, ..is the mean of dependent variables, 
and ..is the number of datums. 
Range: [.¡Þ,1], with 1 being the best. Negative score of R-Squared indicates that the regression model is erroneous. The lower the error in the regression analysis relative to total error, the higher the R-squared value will be. Any R-squared value greater than zero means that the regression analysis did better than just using a horizontal line through the mean value. In the rare cases that R-squared value is negative, the regression analysis need to be re-evaluated, especially if an intercept is forced. 
R-Squared is a relative metric used to compare the model with other models trained on the same dataset. R-Squared indicates the difference between samples in the dataset and the predictions made by the model. 
4560 b) Metrics for classification 

4575 
4576 4577 4578 
4579 4580 4581 4582 
4583 
4584 
4585 
4586 4587 4588 4589 
4590 4591 
4592 
4593 4594 4595 
4596 4597 4598 
4599 
4600 
4601 4602 4603 
4604 4605 Figure H¨C1 ¡ªTable of confusion matrix 
Confusion Matrix is an intuitive and descriptive metrics used to find the accuracy and correctness of a machine learning algorithm. It is mainly used where the output can contain two or more types of classes. 
¡ª Precision and Recall [82]: Precision for a label is defined as the number of true positives divided by the number of predicted positives. Recall for a label is defined as the number of the true positives divided by the total number of actual positives. 
....
..................= Formula H¨C7 
....+.... ....
............= Formula H¨C8 
....+.... 
Precision range: [0,1], with 1 being the best. 
Recall range: [0,1], with 1 being the best. 
In contrast to Accuracy, Precision and Recall are two important metrics for performance evaluation from different aspects for imbalanced dataset. Models inherently trade off Precision and Recall, and they are used differently based on specific use case requirements. Precision is often used when false negative is less emphasized, while Recall is often preferred for output-sensitive predictions. 
Mean Average Precision: The average of Average Precision (weighted mean of precisions at each threshold) of each class. 
..1
......= ¡Æ...... Formula H¨C9 
.. 
..=1 where ...... is the Average Precision of class , and is the number of classes. Range: [0,1], with 1 being the best. mAP incorporates the trade-off between Precision and Recall, and considers both False Positive and False Negative. mAP is a suitable metric for most detection applications. 
¡ª F1-score [82]: The harmonic mean of Precision and Recall 
..................¡Á............ 
..1...........=2Formula H¨C10 
..................+............ 
Range: [0,1]. 
F1-score, also known as f-score or f-measure,isused to measure a test¡¯saccuracy. It indicateshow precise the classifier is (i.e., how many instances it classifies correctly), as well as how robust it is (i.e., does not miss a significant number of instances). 
F1-score maintains a balance between Precision and Recall for the classifier. However, F1-score gives equal weight to precision and recall. 
4606  ¡ª 
AU-ROC (Area under Receiver Operating Characteristics Curve) [82]: an ROC curve is a graph  
4607  showing the performance of a classification model at all classification thresholds, with two parameters True  
4608  .... ....Positive Rate (TPR) and False Positive Rate (FPR), which are defined as ......= , and ......= . AU-....+........+....  
4609  ROC measures the entire two-dimensional area underneath the entire ROC curve as shown in Figure H¨C2.  

4610 

4611  Figure H¨C2 ¡ªROC curve and AU-ROC  
4612  Range: [0,1].  
4613  AU-ROC provides an aggregate measure of performance across all possible classification thresholds. It is  
4614  particularly useful if the importance of positive and negative classes is equal for evaluation purpose. One way  
4615  of interpreting AUC is as the probability that the model ranks a random positive example higher than a random  
4616  negative example. AU-ROC is scale-invariant, it measures how well the predictions are ranked rather than  
4617  their absolute values. AU-ROC is also classification-threshold-invariant, meaning that it measures the quality  
4618  of the model¡¯s prediction regardless of what classification threshold is selected.  
4619  NOTE 1  In real applications, using a single metric mentioned above could result in biases in system  
4620  performance and are not adequate to evaluate the system performance within a certain level of confidence. A  
4621  combination  of  multiple  metrics  from  different  angles  could  lead  to  more  reliable,  balanced  and  
4622  comprehensive conclusion of the system performance.  
4623  NOTE 2  An AI system does not render predictions/decisions on a general scope. Instead, the AI system  
4624  focuses on particular use cases. A specific use case demands careful consideration, choice, and adaptation of  
4625  metrics. Same score for the same metric does not necessarily indicate the same level of performance for a  
4626  different application.  
4627  
4628  

4630  [1]  ISO 21448:2022, Road vehicles ¡ª Safety of the intended functionality  
4631  [2]  ISO/SAE PAS 22736:2021, Taxonomy and definitions for terms related to driving automation systems  
4632  for on-road motor vehicles  
4633  [3]  ISO/IEC Guide 51:2014, Safety aspects ¡ª Guidelines for their inclusion in standards  
4634  [4]  ISO/IEC TR 5469, Artificial intelligence ¡ª Functional safety and AI systems  
4635  [6]  Goal Structuring Notation Community Standard, Version 3, The Assurance Case Working Group  
4636  (ACWG), SCSC-141C, https://scsc.uk/gsn?page=gsn%202standard  
4637  [7]  Claims Evidence Argument (CAE) Framework, https://claimsargumentsevidence.org/  
4638  [8]  Structured Assurance Case Metamodel (SACM), v2.2, The Object Management Group, 2021,  
4639  https://www.omg.org/spec/SACM.  
4640  [9]  ISO 26262-5:2018, Road vehicles ¡ª Functional safety ¡ª Part 5: Product development at the hardware  
4641  level  
4642  [11]  Hawkins, R., Kelly, T., Knight, J. and Graydon, P., 2011. A new approach to creating clear safety  
4643  arguments. In Advances in systems safety (pp. 3-23). Springer, London.  
4644  [12]  IEC 61508-4:2010, Functional safety of electrical/electronic/programmable electronic safety-related  
4645  systems -Part 4: Definitions and abbreviations (see <a  
4646  href="http://www.iec.ch/functionalsafety">Functional Safety and IEC 61508</a>)  
4647  [13]  Mood, A. M.; Graybill, F. A.; Boes, D. C. (1974). "Section 2.3". Introduction to the Theory of Statistics  
4648  (3rd ed.). McGraw-Hill. ISBN 0070428646  
4649  [14]  Czarnecki, Krzysztof, and Rick Salay. "Towards a framework to manage perceptual uncertainty for  
4650  safe automated driving." International Conference on Computer Safety, Reliability, and Security.  
4651  Springer, Cham, 2018.  
4652  [15]  Montavon, G., Binder, A., Lapuschkin, S., Samek, W., Mler, KR. (2019). Layer-Wise Relevance  
4653  Propagation: An Overview. In: Samek, W., Montavon, G., Vedaldi, A., Hansen, L., Mler, KR. (eds)  
4654  Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer  
4655  Science, vol 11700. Springer, Cham. https://doi.org/10.1007/978-3-030-28954-6_10  
4656  [16]  Delseny, Herv¨¦, et al. "White paper machine learning in certified systems." arXiv preprint  
4657  arXiv:2103.10529 (2021).  
4658  [17]  Willers, Oliver, et al. "Safety concerns and mitigation approaches regarding the use of deep learning in  
4659  safety-critical perception tasks." International Conference on Computer Safety, Reliability, and  
4660  Security. Springer, Cham, 2020.  
4661  [18]  Vapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory. Information Science and  
4662  Statistics. Springer-Verlag.  
4663  [19]  Hippenstiel R. D., Detection theory: Applications and Digital Signal Processing, 2002.  

. ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved . ISO 2024 ¨CAll rights reserved 
ISO/CD PAS 8800:2024(en)  
4664  [20]  Jie M. Zhang, Mark Harman, Lei Ma, Yang Liu, ¡°Machine Learning Testing: Survey, Landscapes and  
4665  Horizons¡±, IEEE Transactions on Software Engineering(.Volume: 48,.Issue: 1, 01 January 2022)  
4666  [21]  Wassim G. Najm, John D. Smith, Mikio Yanagisawa: Pre-Crash Scenario Typology for Crash Avoidance  
4667  Research. Proceeding of the 20th international technical conference on the enhanced safety of  
4668  vehicles 2007.  
4669  [22]  Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel Steininger, Fernandez  Gustavo Dominguez :  
4670  WildDash -Creating Hazard-Aware Benchmarks. ECCV 2018.  
4671  [23]  Zhou, Zhi Quan, and Liqun Sun. "Metamorphic testing of driverless cars." Communications of the ACM  
4672  62.3 (2019): 61-67.  
4673  [24]  Zhang, Mengshi, et al. "DeepRoad: GAN-based metamorphic testing and input validation framework  
4674  for autonomous driving systems." 2018 33rd IEEE/ACM International Conference on Automated  
4675  Software Engineering (ASE). IEEE, 2018.  
4676  [25]  Nie, Changhai, and Hareton Leung. "A survey of combinatorial testing." ACM Computing Surveys  
4677  (CSUR) 43.2 (2011): 1-29.  
4678  [26]  Cheng, Chih-Hong, Chung-Hao Huang, and Georg Nhrenberg. "nn-dependability-kit: Engineering  
4679  neural networks for safety-critical autonomous driving systems." 2019 IEEE/ACM International  
4680  Conference on Computer-Aided Design (ICCAD). IEEE, 2019.  
4681  [27]  Gladisch, Christoph, et al. "Leveraging combinatorial testing for safety-critical computer vision  
4682  datasets." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition  
4683  Workshops. 2020.  
4684  [28]  Szegedy, Christian, et al. "Intriguing properties of neural networks." arXiv preprint arXiv:1312.6199  
4685  (2013).  
4686  [29]  Kurakin, Alexey, Ian J. Goodfellow, and Samy Bengio. "Adversarial examples in the physical world."  
4687  Artificial intelligence safety and security. Chapman and Hall/CRC, 2018. 99-112.  
4688  [30]  Abbas, Houssam, and Georgios Fainekos. "Convergence proofs for simulated annealing falsification of  
4689  safety properties." 2012 50th Annual Allerton Conference on Communication, Control, and  
4690  Computing (Allerton). IEEE, 2012.  
4691  [31]  Deshmukh, Jyotirmoy, et al. "Testing cyber-physical systems through Bayesian optimization." ACM  
4692  Transactions on Embedded Computing Systems (TECS) 16.5s (2017): 1-18.  
4693  [32]  Pietrantuono, Roberto, and Stefano Russo. "Probabilistic sampling-based testing for accelerated  
4694  reliability assessment." 2018 IEEE International Conference on Software Quality, Reliability and  
4695  Security (QRS). IEEE, 2018.  
4696  [33]  Katz, Guy, et al. "Reluplex: An efficient SMT solver for verifying deep neural networks." International  
4697  conference on computer aided verification. Springer, Cham, 2017.  
4698  [34]  Gehr, Timon, et al. "Ai2: Safety and robustness certification of neural networks with abstract  
4699  interpretation." 2018 IEEE symposium on security and privacy (SP). IEEE, 2018.  
4700  [35]  Tran, Hoang-Dung, et al. "Verification of deep convolutional neural networks using imagestars."  
4701  International conference on computer aided verification. Springer, Cham, 2020.  

ISO/CD PAS 8800:2024(en)  
4702  [36]  Cheng, Chih-Hong, et al. "Towards safety verification of direct perception neural networks." 2020  
4703  Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2020.  
4704  [37]  Sinan Hasirlioglu, A Novel Method for Simulation-based Testing and Validation of Automotive  
4705  Surround Sensors under Adverse Weather Conditions, Doctoral Thesis, Institute for Pervasive  
4706  Computing, Johannes Kepler University Linz, 2020.  
4707  [38]  Hejase, Mohammad, et al. "A Validation Methodology for the Minimization of Unknown Unknowns in  
4708  Autonomous Vehicle Systems." 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2020.  
4709  [39]  Li, Changwen, et al. "ComOpT: Combination and Optimization for Testing Autonomous Driving  
4710  Systems." 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022.  
4711  [40]  ISO 3534-1:2006, Statistics ¡ª Vocabulary and symbols ¡ª Part 1: General statistical terms and terms  
4712  used in probability  
4713  [41]  Ali, Nazakat, Manzoor Hussain, and Jang-Eui Hong. 2020.¡°Analyzing safety of collaborative cyber- 
4714  physical systems considering variability.¡± IEEE Access (IEEE) 8: 162701¨C162713.  
4715  [42]  Kramer, Birte, Christian Neurohr, Matthias Ber, Eckard Be, Martin Fr.nzle, and Werner Damm.  
4716  2020.¡°Identification andquantification of hazardous scenariosforautomated driving.¡± Model-Based  
4717  Safety and Assessment: 7th International Symposium, IMBSA 2020, Lisbon, Portugal, September 14¨C  
4718  16, 2020, Proceedings 7. 163¨C178.  
4719  [43]  Adee, Ahmad, Roman Gansch, and Peter Liggesmeyer. 2021. ¡°Systematicmodelingapproach for  
4720  environmental perception limitations in automated driving.¡± 2021 17th European Dependable  
4721  Computing Conference (EDCC). 103¨C110.  
4722  [44]  Adee, Ahmad, Roman Gansch, Peter Liggesmeyer, Claudius Glaeser, and Florian Drews. 2021.  
4723  ¡°Discovery of perception performance limiting triggeringconditionsinautomated driving.¡± 2021 5th  
4724  International Conference on System Reliability and Safety (ICSRS). 248¨C257.  
4725  [45]  Berk, Mario, Olaf Schubert, Hans-Martin Kroll, Boris Buschardt, and Daniel Straub. 2020. ¡°Assessing  
4726  the safety of environment perception in automated driving vehicles.¡± SAE International journal of  
4727  transportation safety (JSTOR) 8: 49¨C74.  
4728  [46]  Salay, Rick, Matt Angus, and Krzysztof Czarnecki. 2019. ¡°Asafety analysis method for perceptual  
4729  componentsinautomated driving.¡± 2019 IEEE 30th International Symposiumon Software Reliability  
4730  Engineering (ISSRE). 24¨C34.  
4731  [47]  Qi, Yi, Yi Dong, Xingyu Zhao, and Xiaowei Huang. 2023. ¡°STPA for Learning-Enabled Systems: A Survey  
4732  and A New Method.¡± arXiv preprint arXiv:2302.10588.   
4733  [48]  Thomas, Stephen, and Katrina M. Groth. 2021. ¡°Toward a hybrid causal framework for autonomous  
4734  vehicle safety analysis.¡± Proceedingsof the Institution of Mechanical Engineers, Part O: Journal of Risk  
4735  and Reliability (SAGE Publications Sage UK: London, England) 1748006X211043310.  
4736  [49]  Qi, Yi, Philippa Ryan Conmy, Wei Huang, Xingyu Zhao, and Xiaowei Huang. 2022. ¡°A hierarchical  
4737  HAZOP-like safety analysis for learning-enabled systems.¡± arXiv preprint arXiv:2206.10216.   
4738  [50]  AIAG and VDA. "EXECUSION OF THE PROCESS FMEA (PFMEA)." In Failure Mode and Effects Analysis  
4739  -FMEA Handbook: Design FMEA, Process FMEA, Supplemental FMEA for Monitoring & System  
4740  Response, 79-124. Southfild, MI: Automotive Industry Action Group, 2019.  

ISO/CD PAS 8800:2024(en)  
4741  [51]  Simon, Burton. "A causal model of safety assurance for machine learning¡±  
4742  https://arxiv.org/abs/2201.05451  
4743  [52]  STPA Handbook Nancy G. LEVESON JOHN P. THOMAS MARCH 2018. 
4744  [53]  SAE J3187:2022, System Theoretic Process Analysis (STPA) Recommended Practices for Evaluations of  
4745  Automotive Related Safety-Critical Systems. 
4746  [54]  Simon, Burton, et al. "Safety, Complexity, and Automated Driving: Holistic Perspectives on Safety  
4747  Assurance.¡±Computer. pp. 22-32, 2021.  
4748  [55]  Mariusz Bojarski and others, the NVIDIA PilotNet Experiment, arXiv:2010.08776v1 [cs.CV] 17 Oct  
4749  2020  
4750  [56]  Vasu Singh, Mandar Pitale, Impact of Automotive System Safety Design on Machine Learning Based  
4751  Perception Systems, 2021 4th IEEE International Conference on Industrial Cyber-Physical Systems  
4752  [57]  Rob Ashmore and others, Assuring the Machine Learning Lifecycle: Desiderata, Methods, and  
4753  Challenges, arXiv:1905.04223v1  
4754  [58]  H. Xu, Z. Chen, W. Wu, Z. Jin, S. Kuo and M. Lyu, NV-DNN: Towards Fault-Tolerant DNN Systems with  
4755  N-Version Programming, 2019 49th Annual IEEE/IFIP International Conference on Dependable  
4756  Systems and Networks Workshops (DSN-W), Portland, OR, USA, 2019, pp. 44-47  
4757  [59]  Fumio Machida, N-version machine learning models for safety critical systems, The DSN Workshop on  
4758  Dependable and Secure Machine Learning (DSML) 2019  
4759  [60]  Timo Samann and others, Strategy to Increase the Safety of a DNN-based Perception for HAD Systems,  
4760  arXiv:2002.08935v1 [cs.CV] 20 Feb 2020  
4761  [61]  Raul Sena Ferreira and others, Benchmarking Safety Monitors for Image Classifiers with Machine  
4762  Learning, 26th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC 2021),  
4763  IEEE, Dec 2021, Perth, Australia.  
4764  [62]  Sina Mohseni, Mandar Pitale, JBS Yadawa, Zhangyang Wang; "Self-Supervised Learning for  
4765  Generalizable Out-of-Distribution Detection", AAAI -2020  
4766  [63]  Joaquin Q Candela, et.al, ¡°Dataset shift in machine learning¡±, TheMIT press 2009.  
4767  [64]  Aria Khademi, Michael Hopka, Devesh Upadhyay. 2023. "Model Monitoring and Robustness of In-Use  
4768  Machine Learning Models: Quantifying Data Distribution Shifts Using Population Stability Index"  
4769  arXiv preprint arXiv:2302.00775v1  
4770  [65]  Senthil Mani et. al.; "Coverage Testing of Deep Learning Models using Dataset Characterization", IBM  
4771  Research, arXiv:1911.07309v1 [cs.LG] 17 Nov 2019  
4772  [66]  Shimodaira H; ¡°Improving predictive inference under covariate shift by weighting the log-likelihood  
4773  function"; https://doi.org/10.1016/S0378-3758(00)00115-4  
4774  [67]  Ussama Zahid et. al; "FAT: Training Neural Networks for Reliable Inference Under Hardware Faults";  
4775  arXiv:2011.05873 [cs.LG]  
4776  [68]  Fuzhen Zhuang et. al.; A Comprehensive Survey on Transfer Learning; arXiv:1911.02685 [cs.LG]  

4777  [69]  Rishi Bommasani et. al,; "On the Opportunities and Risks of Foundation Models"; arXiv:2108.07258v3  
4778  [cs.LG] 12 Jul 2022  
4779  [70]  P van de Laar,  T Heskes, S Gielen; "Partial retraining: a new approach to input relevance  
4780  determination"; 1999 Feb;9(1):75-85. doi: 10.1142/s0129065799000071.  
4781  [71]  Mariusz Bojarski et. al.; "Explaining How a Deep Neural Network Trained with End-to-End Learning  
4782  Steers a Car"; NVIDIA Corporation; arXiv:1704.07911v1 [cs.CV] 25 Apr 2017  
4783  [72]  Vitali Petsiuk et. al.; "Black-box Explanation of Object Detectors via Saliency Maps";  
4784  arXiv:2006.03204v2 [cs.CV] 10 Jun 2021  
4785  [73]  Christian Etmann et. al.; "On the Connection Between Adversarial Robustness and Saliency Map  
4786  Interpretability"; arXiv:1905.04172v1 [stat.ML] 10 May 2019  
4787  [74]  Been Kim et al.¡°Interpretability beyond feature attribution: Quantitative testing with concept  
4788  activation vectors (tcav)¡±. In: International conference on machine learning. PMLR, 2018, pp. 2668¨C  
4789  2677.  
4790  [75]  Zhang, R., Madumal, P., Miller, T., Ehinger, K.A., Rubinstein, B.I.: Invertible concept-based explanations  
4791  for cnn models with non-negative concept activation vectors. In: Proc. AAAI Conf. Artificial  
4792  Intelligence. pp. 11682¨C11690 (2021)  
4793  [76]  Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like  
4794  that: Deep learning for interpretable image recognition. NeurIPS, 32, 2  
4795  [77]  Kexin Pei et. al.; "DeepXplore: Automated Whitebox Testing of Deep Learning Systems";  
4796  arXiv:1705.06640v4 [cs.LG] 24 Sep 2017  
4797  [78]  Youcheng Sun et. al.; "Testing Deep Neural Networks"; arXiv:1803.04792v4 [cs.LG] 15 Apr 2019  
4798  [79]  Shogo Tokui et. al.; "NEURECOVER: Regression-Controlled Repair of Deep Neural Networks with  
4799  Training History"; arXiv:2203.00191v2 [cs.LG] 4 Mar 2022  
4800  [80]  James Kirkpatrick et. al.; "Overcoming catastrophic forgetting in neural networks";  
4801  https://www.pnas.org/doi/full/10.1073/pnas.1611835114  
4802  [81]  Prakhar Kaushik et. al.; "Understanding Catastrophic Forgetting and Remembering in Continual  
4803  Learning with Optimal Relevance Mapping"; arXiv:2102.11343v1 [cs.LG] 22 Feb 2021  
4804  [82]  A. Tharwat, ¡°Classification assessment methods.¡± Applied Computing and Informatics, Volume 17,  
4805  Issue 1, 30, 2020.  
4806  [83]  C. Sammut and G. I. Webb, Encyclopedia of machine learning: Springer Science & Business Media,  
4807  2011  
4808  [84]  ISO/IEC TR 24027:2021, Information technology ¡ª Artificial intelligence (AI) ¡ª Bias in AI systems and  
4809  AI aided decision making  
4810  [85]  ISO/TR 4804:2020, Road vehicles ¡ª Safety and cybersecurity for automated driving systems ¡ª Design,  
4811  verification and validation  
4812  [86]  ISO 26262-8:2018, Road vehicles ¡ª Functional safety ¡ª Part 8: Supporting processes  

ISO/CD PAS 8800:2024(en)  
4813  [10]  ISO 26262-6:2018, Road vehicles ¡ª Functional safety ¡ª Part 6: Product development at the software  
4814  level  
4815  [87]  Kuhn, D. Richard, Raghu N. Kacker, and Yu Lei. Introduction to combinatorial testing. CRC press, 2013.  
4816  [88]  ISO/IEC 22989, Information technology ¡ª Artificial intelligence ¡ª Artificial intelligence concepts and  
4817  terminology  
4818  [89]  ISO 26262-2:2018, Road vehicles ¡ª Functional safety ¡ª Part 2: Management of functional safety  
4819  [90]  ZHANG, Q. AND ZHU, S.-C., "Visual Interpretability for Deep Learning: a Survey", 2018,  
4820  https://arxiv.org/abs/1802.00614  
4821  [91]  ISO/TR 4804:2020, Road vehicles ¡ª Safety and cybersecurity for automated driving systems ¡ª Design,  
4822  verification and validation  
4823  [92]  ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) ¡ª Assessment of the robustness of neural  
4824  networks ¡ª Part 1: Overview  
4825  [5]  ISO 26262-10:2018, Road vehicles ¡ª Functional safety ¡ª Part 10: Guidelines on ISO 26262  
4826  [93]  Sina Mohseni, Mandar Pitale, Vasu Singh, Zhangyang Wang: "Practical Solutions for Machine Learning  
4827  Safety in Autonomous Vehicles", Safe AI 2020  
4828  [94]  Practical Solutions for Machine Learning Safety in Autonomous Vehicles  
4829  [95]  MOLNAR, C., "A Guide for Making Black Box Models Explainable, 2019,  
4830  https://christophm.github.io/interpretable-ml-book/  
4831  [96]  Shai Shalev-Shwartz and Amnon Shashua. On the Sample Complexity of End-to-end Training vs.  
4832  Semantic Abstraction Training. arXiv:1604.06915, 2016,  
4833  https://doi.org/10.48550/arXiv.1604.06915  
4834  [97]  IEC 61508-4:2010, Functional safety of electrical/electronic/programmable electronic safety-related  
4835  systems -Part 4: Definitions and abbreviations (see <a  
4836  href="http://www.iec.ch/functionalsafety">Functional Safety and IEC 61508</a>)  
4837  [98]  Koopmann, Tjark, Christian Neurohr, Lina Putze, Lukas Westhofen, Roman Gansch, and Ahmad Adee.  
4838  2022.¡°Grasping Causality for the Explanation of Criticality for Automated Driving.¡± arXiv preprint  
4839  arXiv:2210.15375.   
4840  [99]  arXiv:2302.10588.  Salay, Rick, Matt Angus, and KrzysztofCzarnecki. 2019. ¡°A safety analysis method  
4841  for perceptual componentsinautomated driving.¡± 2019 IEEE 30th International Symposiumon  
4842  Software Reliability Engineering (ISSRE). 24¨C34.  
4843  [100]  IEC 61508-4:2010, Functional safety of electrical/electronic/programmable electronic safety-related  
4844  systems -Part 4: Definitions and abbreviations (see <a  
4845  href="http://www.iec.ch/functionalsafety">Functional Safety and IEC 61508</a>)  
4846  [101]  LAPUSCHKIN, S., W.LDCHEN, S., BINDER, A., MONTAVON, G., SAMEK, W., and M¨¹LLER, K.-R.,  
4847  "Unmasking Clever Hans predictors and assessing what machines really learn", 2019, In: Nature  
4848  Communications 1096 (2019), https://www.nature.com/articles/s41467-019-08987-4  
4849  [102]  Dong, Yi, Wei Huang, Vibhav Bharti, Victoria Cox, Alec Banks, Sen Wang, Xingyu Zhao, Sven Schewe,  
4850  and Xiaowei Huang. 2022. ¡°Reliability Assessment and Safety Arguments for Machine Learning  

4851  Componentsin System Assurance.¡± ACM Transactionson Embedded Computing Systems (ACM New  
4852  York, NY).  
4853  [103]  Acar Celik, Esra, Carmen Carlan, Asim Abdulkhaleq, Fridolin Bauer, Martin Schels, and Henrik J.  
4854  Putzer. 2022. ¡°Application of STPA for the Elicitation of Safety Requirements for a Machine Learning- 
4855  Based Perception Component in Automotive.¡± Computer Safety, Reliability, and Security: 41st  
4856  International Conference, SAFECOMP 2022, Munich, Germany, September 6¨C9, 2022, Proceedings.  
4857  319¨C332.  








